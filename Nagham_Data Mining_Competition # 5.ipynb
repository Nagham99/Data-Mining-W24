{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*By Nagham Rizk*  \n",
    "*23ppmk@queensu.ca*  \n",
    "*Student ID: 20481034*  \n",
    "*CISC 873 Data Mining - Winter 2024*  \n",
    "*DR Steven Ding*  \n",
    "*CISC 873*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "\n",
    "# CISC873-DM-W24-A6\n",
    "# Anti-Cancer Drug Activity Prediction\n",
    "\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Data Science Image](cm.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem Formulation\n",
    "\n",
    "**Problem:**\n",
    "The problem is to predict the anticancer activity of chemical compounds, specifically targeting non-small cell lung cancer. Each chemical compound is represented as a graph in SDF (Structure Data Format), where atoms represent nodes and bonds represent edges.\n",
    "\n",
    "**Input:**\n",
    "- Graph representations of chemical compounds in SDF format, with nodes representing atoms and edges representing bonds.\n",
    "\n",
    "**Output:**\n",
    "- Prediction of the anticancer activity, indicating whether a compound is positive (effective against non-small cell lung cancer) or negative (not effective).\n",
    "\n",
    "**Data Mining Function:**\n",
    "Classification is the primary data mining function required to predict the activity of chemical compounds.\n",
    "\n",
    "### **Challenges:**\n",
    "| Challenges           | Description                                                                                                            |\n",
    "|----------------------|------------------------------------------------------------------------------------------------------------------------|\n",
    "| Graph Complexity     | Dealing with the complexity of graph structures, which may vary in size and connectivity.                              |\n",
    "| SDF Parsing          | Parsing the SDF format and extracting relevant information about atoms, bonds, and their properties.                  |\n",
    "| Feature Extraction   | Identifying and extracting informative features from the graph representations to use for classification.             |\n",
    "| Data Imbalance       | Dealing with highly unbalanced datasets where one class significantly outweighs the other poses challenges in training models. We addressed this by implementing techniques like upsampling of the minority class. |\n",
    "| Model Selection      | Choosing an appropriate model architecture that can effectively learn from graph data and make accurate predictions.   |\n",
    "| Model Complexity     | As we explored various graph neural network architectures and hyperparameters, managing the complexity of the models became challenging. Each architecture has its own set of hyperparameters, and tuning them effectively requires careful consideration. |\n",
    "| Computational Resources | Training complex graph neural network models with large datasets can be computationally expensive and time-consuming. Limited computational resources restricted the number of experiments we could perform and the size of the datasets we could work with. |\n",
    "| Hyperparameter Tuning | Tuning hyperparameters for graph neural networks is non-trivial due to the large search space and interdependencies among hyperparameters. Finding the optimal combination of hyperparameters requires extensive experimentation and computational resources. |\n",
    "| Overfitting          | Graph neural networks, especially with deep architectures, are prone to overfitting, especially when dealing with limited data. Regularization techniques and careful model selection are crucial to mitigate this issue. |\n",
    "| Interpretability     | Understanding and interpreting the behavior of graph neural networks, particularly in complex architectures like RGCN and RGAT, can be challenging. Interpreting how the model processes and learns from graph-structured data remains an ongoing challenge. |\n",
    "| Model Evaluation     | Evaluating the performance of graph neural networks is not always straightforward, especially when dealing with graph-structured data. Choosing appropriate evaluation metrics and ensuring they align with the problem at hand is crucial. |\n",
    "\n",
    "\n",
    "**Impact:**\n",
    "Accurately predicting the anticancer activity of chemical compounds can have a significant impact on cancer treatment. It can facilitate the discovery of new drugs with improved efficacy and fewer side effects, ultimately benefiting patients with non-small cell lung cancer.\n",
    "\n",
    "**Ideal Solution:**\n",
    "- Trial 11 with GGNN after upsampling achieves an AUC score of 0.85, indicating excellent performance.\n",
    "- This approach effectively addresses the data imbalance issue and allows the GGNN model to learn from a balanced dataset, resulting in improved classification accuracy.\n",
    "- The hyperparameters are fine-tuned to optimize the model's performance, ensuring that it can effectively capture the underlying patterns in the graph data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Tuning and Documentation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Trail | Model                              | Reason                                                              | Expected Outcome                    | Observations                                                                                                                                                                          |\n",
    "|-------|------------------------------------|---------------------------------------------------------------------|------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| 1     | Baseline GCN Model                 | Default hyperparameters                                             | Establishing baseline performance  | Utilizing default hyperparameters provides a baseline performance for GCN model.                                                                                                    |\n",
    "| 2     | Enhanced GCN Model                 | Hyperparameter tuning                                               | Improving model performance        | Tuning hyperparameters such as hidden dimension size, learning rate, and number of layers allows for better learning and optimization of the GCN model.                            |\n",
    "| 3     | GCN Gather Aggregation Mechanism   | Utilizing gather aggregation mechanism                              | Enhancing aggregation capabilities | The gather aggregation mechanism aggregates information from all neighboring nodes in the graph, potentially capturing more comprehensive patterns.                               |\n",
    "| 4     | Mean Aggregation Mechanism         | Mean aggregation                                                    | Capturing average node features    | Mean aggregation calculates the average of node features from neighboring nodes, providing a simplified yet effective way to aggregate information.                                |\n",
    "| 5     | Max Aggregation Mechanism          | Max aggregation                                                     | Capturing maximum node features   | Max aggregation selects the maximum node feature value from neighboring nodes, capturing the most significant information in the local neighborhood.                                 |\n",
    "| 6     | LSTM Aggregation Mechanism         | LSTM aggregation with tuned parameters                              | Capturing long-range dependencies | LSTM mechanism with tuned parameters allows the model to capture long-range dependencies between nodes, enhancing its ability to understand complex graph structures.             |\n",
    "| 7     | GGNN with Default Hyperparameters | GGNN message passing mechanism                                      | Learning graph structure           | GGNN effectively learns the graph structure by iteratively passing messages between nodes, capturing dependencies and interactions in the graph data.                             |\n",
    "| 8     | RGCN with Default Hyperparameters | RGCN message passing mechanism                                      | Handling relational data           | RGCN is suitable for handling relational data by incorporating different types of relations between nodes, allowing for more expressive representation learning.             |\n",
    "| 9     | RGAT with Default Hyperparameters | RGAT message passing mechanism                                      | Leveraging attention mechanisms    | RGAT incorporates attention mechanisms into message passing, allowing the model to focus on more relevant nodes and edges during aggregation, improving performance.             |\n",
    "| 10    | RGIN with Default Hyperparameters | RGIN message passing mechanism                                      | Capturing graph isomorphism        | RGIN learns node representations by capturing graph isomorphism, ensuring that node embeddings are invariant under permutations of node indices while preserving relational information. |\n",
    "| 11    | GGNN After Upsampling             | Addressing class imbalance with upsampling                           | Improving minority class prediction | Upsampling mitigates class imbalance issues, ensuring that the model is not biased towards the majority class and can effectively predict instances from the minority class.     |\n",
    "| 12    | RGAT After Upsampling             | Addressing class imbalance with upsampling                           | Enhancing model robustness         | Upsampling improves model robustness by providing more balanced training data, leading to better generalization and performance, especially for the minority class.                 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation and Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Model                           | AUC Score | Parameters                                             | Observation                                            |\n",
    "|---------------------------------|-----------|--------------------------------------------------------|--------------------------------------------------------|\n",
    "| Baseline GCN Model              | 0.6932    | Hidden Dim: 32                                         | Overfitting observed, validation accuracy around 62.5% |\n",
    "| Enhanced GCN Model              | 0.806     | Hidden Dim: 64, Learning Rate: 0.001, Num Layers: 3  | Improved performance compared to baseline              |\n",
    "| GCN Gather Aggregation Mechanism| 0.812     | Hidden Dim: 64, Aggregation Type: Gather              | Slightly better performance than baseline               |\n",
    "| Mean Aggregation Mechanism      | 0.811     | Hidden Dim: 64, Aggregation Type: Mean                | Similar performance to Gather mechanism                |\n",
    "| Max Aggregation Mechanism       | 0.8162    | Hidden Dim: 64, Aggregation Type: Max                 | Slightly higher AUC score, potential overfitting       |\n",
    "| LSTM Aggregation Mechanism      | 0.8254    | Hidden Dim: 64, Num Layers: 3, Aggregation Type: LSTM, Use Gate: True | Improved performance with LSTM mechanism           |\n",
    "| GGNN with Default Hyperparameters| 0.8326  | Default parameters                                      | Steadily increasing AUC score and decreasing loss        |\n",
    "| RGCN with Default Hyperparameters| 0.7939  | Default parameters                                      | Lower AUC score compared to GGNN                       |\n",
    "| RGAT with Default Hyperparameters| 0.8405  | Default parameters                                      | Highest AUC score among all trials                      |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explanation of Mechanisms Chosen in Each Trial "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Trail                           | Description                                                                                          | Importance                                                                                      |\n",
    "|---------------------------------|------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------|\n",
    "| Baseline GCN Model              | Baseline model using the default Graph Convolutional Network mechanism without any modifications.  | Provides a reference point for comparison with other models.                                 |\n",
    "| Enhanced GCN Model              | GCN model with tuned hyperparameters (hidden dimension, learning rate, num layers) to improve performance. | Hyperparameters significantly affect the model's capacity to learn complex patterns.         |\n",
    "| GCN Gather Aggregation Mechanism| Aggregates information from neighboring nodes by gathering information from all nodes in the graph. | Captures global information from the graph, potentially enhancing model performance.          |\n",
    "| Mean Aggregation Mechanism      | Calculates the average of node features from neighboring nodes.                                     | Provides a simple and computationally efficient mechanism to capture local node information.  |\n",
    "| Max Aggregation Mechanism       | Selects the maximum node feature value from neighboring nodes.                                       | Captures the most significant information in the local neighborhood, improving model performance. |\n",
    "| LSTM Aggregation Mechanism      | Utilizes LSTM cells for aggregating information from neighboring nodes, capturing dependencies between nodes. | Allows the model to capture sequential information and long-range dependencies in the graph.  |\n",
    "| GGNN with Default Hyperparameters| Utilizes Gated Graph Neural Network mechanism with default hyperparameters.                          | Offers a balance between model complexity and performance, suitable for initial exploration.     |\n",
    "| RGCN with Default Hyperparameters| Utilizes Relational Graph Convolutional Network mechanism with default hyperparameters.                | Handles graphs with multiple relations between nodes, capturing complex relationships in the data.|\n",
    "| RGAT with Default Hyperparameters| Incorporates attention mechanisms into the message passing process to focus on relevant information.   | Learns edge weights dynamically, allowing the model to focus on important features in the graph. |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "\n",
    "- **Baseline GCN Model:** The baseline model uses the default GCN mechanism to understand the performance baseline without any modifications.\n",
    "  \n",
    "- **Enhanced GCN Model:** Tuning hyperparameters (hidden dimension, learning rate, num layers) allows for better model performance by adjusting the model's capacity to learn complex patterns.\n",
    "\n",
    "- **GCN Gather Aggregation:** This mechanism aggregates information from all nodes in the graph, capturing global information that may enhance model performance.\n",
    "\n",
    "- **Mean Aggregation:** Calculates the average of node features from neighboring nodes, providing a simple and efficient way to capture local node information.\n",
    "\n",
    "- **Max Aggregation:** Selects the maximum node feature value from neighboring nodes, capturing the most significant information in the local neighborhood.\n",
    "\n",
    "- **LSTM Aggregation:** Utilizes LSTM cells for aggregating information from neighboring nodes, capturing sequential information and long-range dependencies.\n",
    "\n",
    "- **GGNN with Default Hyperparameters:** Utilizes Gated Graph Neural Network mechanism with default hyperparameters, striking a balance between model complexity and performance.\n",
    "\n",
    "- **RGCN with Default Hyperparameters:** Handles graphs with multiple relations between nodes, capturing complex relationships in the data using default hyperparameters.\n",
    "\n",
    "- **RGAT with Default Hyperparameters:** Incorporates attention mechanisms to dynamically learn edge weights and focus on important features in the graph using default hyperparameters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "Among the various trials conducted, the GGNN model after upsampling (Trial 11) stands out as the most successful, achieving the highest AUC score of 0.8509. By addressing the class imbalance issue and utilizing the Graph Gated Neural Network (GGNN) architecture, this approach demonstrates superior performance in learning from graph-structured data.\n",
    "\n",
    "The upsampling technique effectively balanced the dataset, allowing the model to learn from both classes more effectively. This, combined with the GGNN architecture's ability to capture complex graph relationships, resulted in improved predictive capability.\n",
    "\n",
    "Further optimization and exploration of hyperparameters and aggregation mechanisms could potentially enhance model performance even more, but Trial 11 serves as a strong baseline for future developments in graph neural networks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing Steps\n",
    "\n",
    "1. **Tokenization:**\n",
    "   - Use the `Tokenizer` class from Keras to tokenize the text data.\n",
    "   - Set the maximum vocabulary size (`max_vocab`) to 500 and the maximum length of sequences (`max_len`) to 100.\n",
    "   - Build the vocabulary from the training set by extracting all nodes from the samples and fitting the tokenizer on them.\n",
    "   \n",
    "2. **Padding Sequences:**\n",
    "   - After tokenization, sequences are padded to ensure uniform length using `pad_sequences` from Keras.\n",
    "   - Padding is done with zeros at the end of sequences (post-padding) to match the maximum sequence length.\n",
    "   \n",
    "3. **Data Preparation:**\n",
    "   - Define a function `prepare_single_batch` to prepare a single batch of samples.\n",
    "   - It takes a list of samples as input and tokenizes the nodes, pads sequences, calculates edge indices, and maps nodes to graphs.\n",
    "   - The resulting data is formatted into dictionaries containing 'data', 'edges', and 'node2graph' arrays.\n",
    "   \n",
    "4. **Batch Generation:**\n",
    "   - Define a generator function `gen_batch` to generate batches of data for training or evaluation.\n",
    "   - It shuffles the dataset if specified, divides it into batches, and yields prepared batches using the `prepare_single_batch` function.\n",
    "   - If repeat is set to False, the generator stops after one complete pass through the dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upsampling\n",
    "- is a technique used to address class imbalance problems in machine learning datasets. When one class has significantly fewer samples than another class, the model may become biased towards the majority class, leading to poor performance in predicting the minority class.\n",
    "\n",
    "### Steps for Upsampling:\n",
    "\n",
    "1. **Identify the Minority Class**: Determine which class is the minority class that needs to be upsampled.\n",
    "\n",
    "2. **Calculate the Imbalance Ratio**: Calculate the ratio of the number of samples in the minority class to the number of samples in the majority class.\n",
    "\n",
    "3. **Upsample the Minority Class**: Generate additional samples for the minority class to match the number of samples in the majority class. This can be achieved by randomly duplicating existing samples or generating new samples based on existing ones.\n",
    "\n",
    "4. **Combine with the Majority Class**: Combine the upsampled minority class samples with the original majority class samples to create a balanced dataset.\n",
    "\n",
    "5. **Train the Model**: Train the machine learning model using the balanced dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Experimental Protocol:**\n",
    "\n",
    "1. **Data Preparation:**\n",
    "   - Read the training and validation datasets from SDF files using the `read_sdf` function.\n",
    "   - Split the training set into training and validation sets using `train_test_split`.\n",
    "\n",
    "2. **Preprocessing:**\n",
    "   - Tokenize the nodes in the chemical compound graphs and build the vocabulary.\n",
    "   - Prepare batches of samples using the `prepare_single_batch` function.\n",
    "\n",
    "3. **Model Configuration:**\n",
    "   - Define hyperparameters such as maximum vocabulary size (`max_vocab`), maximum sequence length (`max_len`), and batch size.\n",
    "   - Choose the appropriate aggregation mechanism for the GNN layer \n",
    "   - Select the message passing style for the GNN layer GCN, GGNN, RGCN,\n",
    "\n",
    "4. **Model Training:**\n",
    "   - Build the model using the defined configuration.\n",
    "   - Compile the model with binary cross-entropy loss and AUC metric.\n",
    "   - Train the model on the training set for a specified number of epochs, monitoring performance on the validation set.\n",
    "   - Optimize hyperparameters such as batch size, learning rate, and aggregation mechanisms based on validation performance.\n",
    "\n",
    "5. **Evaluation:**\n",
    "   - Evaluate the trained model on the test set if available.\n",
    "   - Analyze the model's performance metrics such as accuracy, AUC.\n",
    "   - Visualize the predictions and compare them with the ground truth labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning and Search Space\n",
    "\n",
    "\n",
    "\n",
    "- **Manual Adjustment:** Hyperparameters such as batch size, maximum vocabulary size (`max_vocab`), maximum sequence length (`max_len`), and GNN aggregation mechanisms are adjusted manually in the code.\n",
    "\n",
    "- **Search Space:** \n",
    "  - **Batch Size:** Typically ranges from 16 to 128.\n",
    "  - **Max Vocabulary Size (`max_vocab`):** Determines the size of the vocabulary used for tokenization.\n",
    "  - **Max Sequence Length (`max_len`):** Determines the maximum length of sequences after padding.\n",
    "\n",
    " - We adjusted hyperparameters such as hidden dimension, number of layers, number of heads, and other parameters related to the GNN architecture.\n",
    "  - **the message passing mechanismOne** is the crucial hyperparameter we focused on, which defines how information is propagated between nodes in the graph. We experimented with the following message passing mechanisms:\n",
    "  - GGNN (Gated Graph Neural Network)\n",
    "  - RGCN (Relational Graph Convolutional Network)\n",
    "  - RGAT (Relational Graph Attention Network)\n",
    "  - RGIN (Relational Graph Isomorphism Network)\n",
    "  - GNN-Edge-MLP (Graph Neural Network with Edge MLP)\n",
    "  - Each message passing mechanism has its own set of hyperparameters that can be fine-tuned.\n",
    "  - Additionally, we explored different aggregation types to combine information from neighboring nodes:\n",
    "  - Mean aggregation\n",
    "  - Max aggregation\n",
    "  - LSTM-based aggregation\n",
    "- For each trial, we modified specific hyperparameters based on the default settings and our understanding of the model architecture.\n",
    "\n",
    "\n",
    "### **Criteria for Determining Good/Bad Hyperparameters:**\n",
    "  - **Validation Performance:** Improved accuracy and AUC on the validation set.\n",
    "  - **Generalization:** Avoiding overfitting and ensuring good performance on unseen data.\n",
    "  - **Training Stability:** Consistent convergence behavior with no abrupt changes.\n",
    "  - **Efficiency:** Quick and efficient convergence to reduce training time.\n",
    "  - **Interpretability:** In some cases, models with interpretable hyperparameters are preferred.\n",
    "\n",
    "Hyperparameters are adjusted iteratively based on observation of model performance, and good configurations are selected based on validation metrics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ✔️ Answer the questions below (briefly):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🌈Based on the provided template, describe the format of the input file (sdf file).\n",
    "\n",
    "The input file is in SDF (structured-data format) format, commonly used to represent chemical compounds. Each compound is represented as a graph, where atoms are nodes and bonds are edges. The SDF file contains multiple samples, each separated by the delimiter `$$$$`. Each sample includes information about the atoms, bonds, and a label indicating whether the compound is positive or negative against non-small cell lung cancer.\n",
    "\n",
    "🌈What are the input tensors to the neural network model (their meaning, not just symbol)? What is each of their dims and their meaning (e.g. batch_size)?\n",
    "\n",
    "The input tensors are:\n",
    "- `data`: Represents the features (embeddings) of nodes in the graph.\n",
    "  - Dimensions: (batch_size, max_nodes_per_graph).\n",
    "  - Meaning: Stores the features of each node in the graph.\n",
    "- `edge`: Represents the adjacency lists (edges) of the graph.\n",
    "  - Dimensions: (batch_size, num_edges_per_graph, 2).\n",
    "  - Meaning: Stores the connections between nodes in the graph.\n",
    "- `node2graph`: Represents the mapping of nodes to graphs.\n",
    "  - Dimensions: (batch_size,).\n",
    "  - Meaning: Indicates which graph each node belongs to.\n",
    "\n",
    "🌈For each dim of gnn_out, what does it symbolize? For each dim of avg, what does it symbolize?\n",
    "\n",
    "- `gnn_out`:\n",
    "  - Dimensions: (batch_size, num_nodes, hidden_dim).\n",
    "  - Meaning: Represents the output of the GNN layer, where each element represents the hidden features of a node in the graph.\n",
    "- `avg`:\n",
    "  - Dimensions: (num_graphs, hidden_dim).\n",
    "  - Meaning: Represents the average features of nodes within each graph.\n",
    "\n",
    "🌈What is the difference between segment_mean and tf.reduce_mean? For each dim of pred, what does it symbolize?\n",
    "\n",
    "- `segment_mean`: Computes the mean of elements across segments specified by segment IDs.\n",
    "- `tf.reduce_mean`: Computes the mean of elements along a specified axis.\n",
    "- For `pred`, each dimension represents the probability of the corresponding sample being positive against non-small cell lung cancer.\n",
    "\n",
    "🌈What is the motivation/theory/idea to use multiple gcn layers comparing to just one? How many layers were used in the template?\n",
    "\n",
    "The motivation is to capture higher-level representations of the input graph by aggregating information from neighboring nodes across multiple layers. Using multiple layers allows the model to learn more complex patterns and relationships in the graph data. In the template, only one GCN layer is used, but the model can be extended to include multiple layers by stacking instances of the GCN layer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read SDF format data (structured-data format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def read_sdf(file):\n",
    "    with open(file, 'r') as rf:\n",
    "        content = rf.read()\n",
    "    samples = content.split('$$$$')\n",
    "    \n",
    "    def parse_sample(s):\n",
    "        lines = s.splitlines()\n",
    "        links = []\n",
    "        nodes = []\n",
    "        label = 0\n",
    "        for l in lines:\n",
    "            if l.strip() == '1.0':\n",
    "                label = 1\n",
    "            if l.strip() == '-1.0':\n",
    "                label = 0\n",
    "            if l.startswith('    '):\n",
    "                feature = l.split()\n",
    "                node = feature[3]\n",
    "                nodes.append(node)\n",
    "            elif l.startswith(' '):\n",
    "                lnk = l.split()\n",
    "                # edge: (from, to,) (1-based index)\n",
    "                if int(lnk[0]) - 1 < len(nodes):\n",
    "                    links.append((\n",
    "                        int(lnk[0])-1, \n",
    "                        int(lnk[1])-1, # zero-based index\n",
    "                        # int(lnk[2]) ignore edge weight\n",
    "                    ))\n",
    "        return nodes, np.array(links), label\n",
    "    \n",
    "    return [parse_sample(s) for s in tqdm(samples) if len(s[0]) > 0]\n",
    "                \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 579,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59e3063079bb4d9c8f84c7ca4ed67f26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25024 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "training_set = read_sdf('train.sdf')\n",
    "training_set, validation_set = train_test_split(training_set, test_size=0.15,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "904d085328824ebe947de54877aa86cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12326 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "testing_set  = read_sdf('test_x.sdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['N', 'N', 'N', 'N', 'N', 'N', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C'], array([[ 0,  6],\n",
      "       [ 0,  9],\n",
      "       [ 0, 11],\n",
      "       [ 1,  6],\n",
      "       [ 1, 10],\n",
      "       [ 2,  7],\n",
      "       [ 2,  9],\n",
      "       [ 3,  8],\n",
      "       [ 3, 10],\n",
      "       [ 4,  8],\n",
      "       [ 5, 10],\n",
      "       [ 6,  7],\n",
      "       [ 7,  8],\n",
      "       [11, 12],\n",
      "       [12, 13],\n",
      "       [12, 14],\n",
      "       [13, 15],\n",
      "       [14, 16],\n",
      "       [15, 17],\n",
      "       [16, 17]]), 0)\n"
     ]
    }
   ],
   "source": [
    "print(training_set[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing/Inspecting a Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --quiet networkx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 583,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "colors = cm.rainbow(np.linspace(0, 1, 50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(sample):\n",
    "    G=nx.Graph()\n",
    "    nodes = sample[0]\n",
    "    edges = sample[1]\n",
    "    \n",
    "    labeldict={}\n",
    "    node_color=[]\n",
    "    for i,n in enumerate(nodes):\n",
    "        G.add_node(i)\n",
    "        labeldict[i]=n\n",
    "        node_color.append(colors[hash(n)%len(colors)])\n",
    "\n",
    "    # a list of nodes:\n",
    "    for e in edges:\n",
    "        G.add_edge(e[0], e[1])\n",
    "        \n",
    "    nx.draw(G, labels=labeldict, with_labels = True, node_color = node_color)\n",
    "    plt.show()\n",
    "    \n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<networkx.classes.graph.Graph at 0x17dc3b6c510>"
      ]
     },
     "execution_count": 585,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.clf()\n",
    "visualize(training_set[20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "max_vocab = 500\n",
    "max_len = 100\n",
    "\n",
    "\n",
    "# build vocabulary from training set\n",
    "all_nodes = [s[0] for s in training_set]\n",
    "tokenizer = Tokenizer(num_words=max_vocab)\n",
    "tokenizer.fit_on_texts(all_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 599,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import random\n",
    "random.seed(0)\n",
    "\n",
    "def prepare_single_batch(samples):\n",
    "    sample_nodes = [s[0] for s in samples]\n",
    "    sample_nodes = tokenizer.texts_to_sequences(sample_nodes)\n",
    "    sample_nodes = pad_sequences(sample_nodes, padding='post')\n",
    "    max_nodes_len = np.shape(sample_nodes)[1]\n",
    "    edges = [s[1]+i*max_nodes_len for i,s in enumerate(samples)]\n",
    "    edges = [e for e in edges if len(e) > 0]\n",
    "    node_to_graph = [[i]*max_nodes_len for i in range(len(samples))]\n",
    "    \n",
    "    all_nodes = np.reshape(sample_nodes, -1)\n",
    "    all_edges = np.concatenate(edges)\n",
    "\n",
    "    node_to_graph = np.reshape(node_to_graph, -1)\n",
    "    return {\n",
    "        'data': all_nodes,\n",
    "        'edges': all_edges,\n",
    "        'node2grah': node_to_graph,\n",
    "    }, np.array([s[2] for s in samples])\n",
    "\n",
    "\n",
    "\n",
    "def gen_batch(dataset, batch_size=16, repeat=False, shuffle=True):\n",
    "    while True:\n",
    "        dataset = list(dataset)\n",
    "        if shuffle:\n",
    "            random.shuffle(dataset)\n",
    "        l = len(dataset)\n",
    "        for ndx in range(0, l, batch_size):\n",
    "            batch_samples = dataset[ndx:min(ndx + batch_size, l)]\n",
    "            yield prepare_single_batch(batch_samples)\n",
    "        if not repeat:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data\n",
      "[2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 2 2 2 2 2 3 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 0 0 0 0 4 4 2 2 2 2 3 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0]\n",
      "edges\n",
      "[[ 0  7]\n",
      " [ 0 11]\n",
      " [ 1  5]\n",
      " [ 1 15]\n",
      " [ 2  6]\n",
      " [ 2 16]\n",
      " [ 3  7]\n",
      " [ 4  5]\n",
      " [ 4  6]\n",
      " [ 4  7]\n",
      " [ 5  8]\n",
      " [ 6  9]\n",
      " [ 8 10]\n",
      " [ 9 10]\n",
      " [11 12]\n",
      " [12 13]\n",
      " [12 14]\n",
      " [13 17]\n",
      " [14 18]\n",
      " [17 19]\n",
      " [18 19]\n",
      " [25 31]\n",
      " [25 35]\n",
      " [26 34]\n",
      " [27 33]\n",
      " [28 38]\n",
      " [28 44]\n",
      " [29 38]\n",
      " [30 32]\n",
      " [30 36]\n",
      " [31 32]\n",
      " [31 33]\n",
      " [32 34]\n",
      " [33 34]\n",
      " [35 37]\n",
      " [36 38]\n",
      " [37 39]\n",
      " [37 40]\n",
      " [39 41]\n",
      " [40 42]\n",
      " [41 43]\n",
      " [42 43]\n",
      " [44 45]\n",
      " [50 52]\n",
      " [50 53]\n",
      " [50 57]\n",
      " [50 59]\n",
      " [51 54]\n",
      " [51 55]\n",
      " [51 56]\n",
      " [51 60]\n",
      " [56 58]\n",
      " [57 58]\n",
      " [57 61]\n",
      " [58 62]\n",
      " [59 63]\n",
      " [59 64]\n",
      " [60 67]\n",
      " [60 68]\n",
      " [61 65]\n",
      " [62 66]\n",
      " [63 69]\n",
      " [64 70]\n",
      " [65 66]\n",
      " [67 71]\n",
      " [68 72]\n",
      " [69 73]\n",
      " [70 73]\n",
      " [71 74]\n",
      " [72 74]\n",
      " [75 86]\n",
      " [75 87]\n",
      " [76 81]\n",
      " [77 82]\n",
      " [78 83]\n",
      " [79 86]\n",
      " [80 84]\n",
      " [80 85]\n",
      " [80 86]\n",
      " [81 82]\n",
      " [81 83]\n",
      " [82 84]\n",
      " [83 85]\n",
      " [87 88]\n",
      " [88 89]]\n",
      "node2grah\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3]\n",
      "label [0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# showing one batch:\n",
    "for train_batch in gen_batch(training_set, batch_size=4):\n",
    "    for k,v in train_batch[0].items():\n",
    "        print(k)\n",
    "        print(v)\n",
    "        pass\n",
    "    print('label', train_batch[1])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --quiet tf2_gnn\n",
    "\n",
    "# https://github.com/microsoft/tf2-gnn\n",
    "# https://github.com/microsoft/tf2-gnn/blob/master/tf2_gnn/layers/gnn.py\n",
    "\n",
    "from tf2_gnn.layers.gnn import GNN, GNNInput"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trial 1: Baseline GCN Model\n",
    "\n",
    "**Thoughts and Observations:**\n",
    "- The baseline GCN model is implemented with default hyperparameters.\n",
    "- The model is trained for 5 epochs.\n",
    "- Performance metrics such as loss and AUC are monitored during training.\n",
    "- No hyperparameter tuning is performed in this trial.\n",
    "\n",
    "**Observation:**\n",
    "- The model achieved an AUC score of 0.6932 on the validation set.\n",
    "\n",
    "**Plan for Next Trial:**\n",
    "- Tune hyperparameters to improve model performance.\n",
    "- Experiment with different aggregation mechanisms in the GCN layer.\n",
    "- Implement up-sampling to address class imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gnn_out KerasTensor(type_spec=TensorSpec(shape=(None, 32), dtype=tf.float32, name=None), name='gnn_52/StatefulPartitionedCall:0', description=\"created by layer 'gnn_52'\")\n",
      "mean: KerasTensor(type_spec=TensorSpec(shape=(None, 32), dtype=tf.float32, name=None), name='tf.math.segment_mean_53/SegmentMean:0', description=\"created by layer 'tf.math.segment_mean_53'\")\n",
      "pred: KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='dense_53/Sigmoid:0', description=\"created by layer 'dense_53'\")\n",
      "Model: \"model_53\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_183 (InputLayer)      [(None,)]                    0         []                            \n",
      "                                                                                                  \n",
      " input_181 (InputLayer)      [(None,)]                    0         []                            \n",
      "                                                                                                  \n",
      " tf.math.reduce_max_59 (TFO  ()                           0         ['input_183[0][0]']           \n",
      " pLambda)                                                                                         \n",
      "                                                                                                  \n",
      " embedding_60 (Embedding)    (None, 20)                   10000     ['input_181[0][0]']           \n",
      "                                                                                                  \n",
      " input_182 (InputLayer)      [(None, 2)]                  0         []                            \n",
      "                                                                                                  \n",
      " tf.__operators__.add_59 (T  ()                           0         ['tf.math.reduce_max_59[0][0]'\n",
      " FOpLambda)                                                         ]                             \n",
      "                                                                                                  \n",
      " gnn_52 (GNN)                (None, 32)                   22464     ['embedding_60[0][0]',        \n",
      "                                                                     'input_182[0][0]',           \n",
      "                                                                     'input_183[0][0]',           \n",
      "                                                                     'tf.__operators__.add_59[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " tf.math.segment_mean_53 (T  (None, 32)                   0         ['gnn_52[0][0]',              \n",
      " FOpLambda)                                                          'input_183[0][0]']           \n",
      "                                                                                                  \n",
      " dense_53 (Dense)            (None, 1)                    33        ['tf.math.segment_mean_53[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 32497 (126.94 KB)\n",
      "Trainable params: 32497 (126.94 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.math import segment_mean\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import Input, Model\n",
    "from tensorflow.keras.layers import Embedding, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "data = keras.Input(batch_shape=(None,))\n",
    "\n",
    "# the first dim is different to the previous one. it is the total number of edges in this batch\n",
    "edge = keras.Input(batch_shape=(None, 2), dtype=tf.int32)\n",
    "node2graph = keras.Input(batch_shape=(None,), dtype=tf.int32)\n",
    "embeded = Embedding(tokenizer.num_words, 20)(data)\n",
    "\n",
    "# number of graphs (number of samples)\n",
    "num_graph = tf.reduce_max(node2graph)+1\n",
    "\n",
    "gnn_input = GNNInput(\n",
    "    node_features=embeded,\n",
    "    adjacency_lists=(edge,),\n",
    "    node_to_graph_map=node2graph, \n",
    "    num_graphs=num_graph,\n",
    ")\n",
    "\n",
    "# https://github.com/microsoft/tf2-gnn/blob/master/tf2_gnn/layers/gnn.py\n",
    "params = GNN.get_default_hyperparameters()\n",
    "params[\"hidden_dim\"] = 32\n",
    "gnn_layer = GNN(params)\n",
    "gnn_out = gnn_layer(gnn_input)\n",
    "\n",
    "print('gnn_out', gnn_out)\n",
    "\n",
    "# https://www.tensorflow.org/api_docs/python/tf/math/segment_mean\n",
    "avg = segment_mean(\n",
    "    data=gnn_out,\n",
    "    segment_ids=node2graph\n",
    ")\n",
    "print('mean:', avg)\n",
    "\n",
    "pred = Dense(1, activation='sigmoid')(avg)\n",
    "print('pred:', pred)\n",
    "\n",
    "model = Model(\n",
    "    inputs={\n",
    "        'data': data,\n",
    "        'edges': edge,\n",
    "        'node2grah': node2graph,\n",
    "    },\n",
    "    outputs=pred\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss='BinaryCrossentropy',\n",
    "    metrics=['AUC']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "333/333 [==============================] - 12s 28ms/step - loss: 0.3051 - auc: 0.4379 - val_loss: 0.2104 - val_auc: 0.5604\n",
      "Epoch 2/5\n",
      "333/333 [==============================] - 9s 27ms/step - loss: 0.1974 - auc: 0.5685 - val_loss: 0.2018 - val_auc: 0.6523\n",
      "Epoch 3/5\n",
      "333/333 [==============================] - 9s 28ms/step - loss: 0.1927 - auc: 0.5992 - val_loss: 0.1994 - val_auc: 0.6843\n",
      "Epoch 4/5\n",
      "333/333 [==============================] - 9s 27ms/step - loss: 0.1876 - auc: 0.6333 - val_loss: 0.1973 - val_auc: 0.6739\n",
      "Epoch 5/5\n",
      "333/333 [==============================] - 10s 29ms/step - loss: 0.1845 - auc: 0.6499 - val_loss: 0.1919 - val_auc: 0.6909\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x17d22828e50>"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "batch_size = 64\n",
    "num_batchs = math.ceil(len(training_set) / batch_size)\n",
    "num_batchs_validation = math.ceil(len(validation_set) / batch_size)\n",
    "\n",
    "model.fit(\n",
    "    gen_batch(\n",
    "        training_set, batch_size=batch_size, repeat=True\n",
    "    ),\n",
    "    steps_per_epoch=num_batchs,\n",
    "    epochs=5,\n",
    "    validation_data=gen_batch(\n",
    "        validation_set, batch_size=64, repeat=True\n",
    "    ),\n",
    "    validation_steps=num_batchs_validation,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "193/193 [==============================] - 2s 8ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(\n",
    "    gen_batch(testing_set, batch_size=64, shuffle=False)\n",
    ")\n",
    "y_pred = np.reshape(y_pred, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12326"
      ]
     },
     "execution_count": 319,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "submission = pd.DataFrame({'label':y_pred})\n",
    "submission.index.name = 'id'\n",
    "submission.to_csv('Baseline GCN Model.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC Score: 0.693184494972229\n"
     ]
    }
   ],
   "source": [
    "# Calculate and print AUC score\n",
    "auc_score = model.evaluate(\n",
    "    gen_batch(validation_set, batch_size=64, repeat=True),\n",
    "    steps=num_batches_validation,\n",
    "    verbose=0\n",
    ")[1]\n",
    "print(\"AUC Score:\", auc_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trial 2: Enhanced GCN Model with Tuned Hyperparameters\n",
    "\n",
    "**Thoughts and Observations:**\n",
    "- In this trial, the hyperparameters of the GCN model are tuned to potentially improve its performance.\n",
    "- The hidden dimension size is increased to 64, the learning rate is set to 0.001, and the number of GCN layers is increased to 3.\n",
    "- The model is trained for 50 epochs to allow more time for learning with the adjusted hyperparameters.\n",
    "- The loss and AUC metrics are monitored during training to evaluate model performance.\n",
    "#### Observation of Trial 2:\n",
    "\n",
    "- The tuned hyperparameters, including a hidden dimension size of 64, a learning rate of 0.001, and 3 GCN layers, resulted in improved model performance compared to the baseline model.\n",
    "- The AUC score increased from 0.693 in the baseline model to 0.806 with the tuned hyperparameters, indicating better predictive capability.\n",
    "- The loss decreased significantly during training, indicating that the model learned more effectively with the adjusted hyperparameters.\n",
    "- Overall, the model with tuned hyperparameters showed promising results and suggests that further hyperparameter optimization could lead to even better performance.\n",
    "#### Plan for Next Trial:\n",
    "\n",
    "1. Experiment with different aggregation mechanisms in the GCN layer to further improve model performance.\n",
    "2. Implement up-sampling to address class imbalance in the training data.\n",
    "3. Explore different activation functions and regularization techniques to enhance model generalization.\n",
    "4. Increase the complexity of the model architecture by adding additional layers or nodes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "333/333 [==============================] - 18s 47ms/step - loss: 0.2204 - auc: 0.5446 - val_loss: 0.1949 - val_auc: 0.6749\n",
      "Epoch 2/50\n",
      "333/333 [==============================] - 15s 46ms/step - loss: 0.1829 - auc: 0.6395 - val_loss: 0.1896 - val_auc: 0.6963\n",
      "Epoch 3/50\n",
      "333/333 [==============================] - 17s 51ms/step - loss: 0.1801 - auc: 0.6648 - val_loss: 0.1851 - val_auc: 0.6942\n",
      "Epoch 4/50\n",
      "333/333 [==============================] - 17s 51ms/step - loss: 0.1791 - auc: 0.6722 - val_loss: 0.1888 - val_auc: 0.7003\n",
      "Epoch 5/50\n",
      "333/333 [==============================] - 17s 51ms/step - loss: 0.1767 - auc: 0.7005 - val_loss: 0.1850 - val_auc: 0.7365\n",
      "Epoch 6/50\n",
      "333/333 [==============================] - 17s 50ms/step - loss: 0.1761 - auc: 0.6977 - val_loss: 0.1910 - val_auc: 0.7225\n",
      "Epoch 7/50\n",
      "333/333 [==============================] - 16s 49ms/step - loss: 0.1767 - auc: 0.7042 - val_loss: 0.1947 - val_auc: 0.7249\n",
      "Epoch 8/50\n",
      "333/333 [==============================] - 16s 49ms/step - loss: 0.1753 - auc: 0.7168 - val_loss: 0.1763 - val_auc: 0.7273\n",
      "Epoch 9/50\n",
      "333/333 [==============================] - 16s 49ms/step - loss: 0.1745 - auc: 0.7223 - val_loss: 0.1830 - val_auc: 0.7363\n",
      "Epoch 10/50\n",
      "333/333 [==============================] - 17s 50ms/step - loss: 0.1753 - auc: 0.7131 - val_loss: 0.1863 - val_auc: 0.7522\n",
      "Epoch 11/50\n",
      "333/333 [==============================] - 16s 49ms/step - loss: 0.1751 - auc: 0.7167 - val_loss: 0.1781 - val_auc: 0.7348\n",
      "Epoch 12/50\n",
      "333/333 [==============================] - 16s 49ms/step - loss: 0.1747 - auc: 0.7177 - val_loss: 0.1927 - val_auc: 0.7254\n",
      "Epoch 13/50\n",
      "333/333 [==============================] - 16s 49ms/step - loss: 0.1746 - auc: 0.7178 - val_loss: 0.1841 - val_auc: 0.7286\n",
      "Epoch 14/50\n",
      "333/333 [==============================] - 17s 50ms/step - loss: 0.1741 - auc: 0.7280 - val_loss: 0.1781 - val_auc: 0.7278\n",
      "Epoch 15/50\n",
      "333/333 [==============================] - 17s 50ms/step - loss: 0.1745 - auc: 0.7163 - val_loss: 0.1944 - val_auc: 0.6939\n",
      "Epoch 16/50\n",
      "333/333 [==============================] - 17s 50ms/step - loss: 0.1750 - auc: 0.7161 - val_loss: 0.1875 - val_auc: 0.7384\n",
      "Epoch 17/50\n",
      "333/333 [==============================] - 17s 51ms/step - loss: 0.1745 - auc: 0.7218 - val_loss: 0.1929 - val_auc: 0.7209\n",
      "Epoch 18/50\n",
      "333/333 [==============================] - 17s 50ms/step - loss: 0.1740 - auc: 0.7239 - val_loss: 0.1731 - val_auc: 0.7315\n",
      "Epoch 19/50\n",
      "333/333 [==============================] - 17s 52ms/step - loss: 0.1734 - auc: 0.7252 - val_loss: 0.1975 - val_auc: 0.7424\n",
      "Epoch 20/50\n",
      "333/333 [==============================] - 17s 50ms/step - loss: 0.1727 - auc: 0.7269 - val_loss: 0.1837 - val_auc: 0.7497\n",
      "Epoch 21/50\n",
      "333/333 [==============================] - 16s 50ms/step - loss: 0.1719 - auc: 0.7296 - val_loss: 0.1859 - val_auc: 0.7130\n",
      "Epoch 22/50\n",
      "333/333 [==============================] - 16s 49ms/step - loss: 0.1723 - auc: 0.7324 - val_loss: 0.1832 - val_auc: 0.7292\n",
      "Epoch 23/50\n",
      "333/333 [==============================] - 16s 49ms/step - loss: 0.1702 - auc: 0.7419 - val_loss: 0.1860 - val_auc: 0.7487\n",
      "Epoch 24/50\n",
      "333/333 [==============================] - 16s 49ms/step - loss: 0.1689 - auc: 0.7450 - val_loss: 0.1849 - val_auc: 0.7314\n",
      "Epoch 25/50\n",
      "333/333 [==============================] - 16s 49ms/step - loss: 0.1682 - auc: 0.7492 - val_loss: 0.1766 - val_auc: 0.7585\n",
      "Epoch 26/50\n",
      "333/333 [==============================] - 18s 53ms/step - loss: 0.1686 - auc: 0.7539 - val_loss: 0.1775 - val_auc: 0.7685\n",
      "Epoch 27/50\n",
      "333/333 [==============================] - 17s 51ms/step - loss: 0.1653 - auc: 0.7694 - val_loss: 0.1753 - val_auc: 0.7796\n",
      "Epoch 28/50\n",
      "333/333 [==============================] - 18s 53ms/step - loss: 0.1650 - auc: 0.7617 - val_loss: 0.1803 - val_auc: 0.7772\n",
      "Epoch 29/50\n",
      "333/333 [==============================] - 17s 52ms/step - loss: 0.1646 - auc: 0.7665 - val_loss: 0.1825 - val_auc: 0.7833\n",
      "Epoch 30/50\n",
      "333/333 [==============================] - 18s 54ms/step - loss: 0.1637 - auc: 0.7677 - val_loss: 0.1798 - val_auc: 0.7880\n",
      "Epoch 31/50\n",
      "333/333 [==============================] - 17s 52ms/step - loss: 0.1629 - auc: 0.7770 - val_loss: 0.1797 - val_auc: 0.7772\n",
      "Epoch 32/50\n",
      "333/333 [==============================] - 18s 55ms/step - loss: 0.1626 - auc: 0.7811 - val_loss: 0.1743 - val_auc: 0.7837\n",
      "Epoch 33/50\n",
      "333/333 [==============================] - 20s 60ms/step - loss: 0.1612 - auc: 0.7799 - val_loss: 0.1749 - val_auc: 0.7768\n",
      "Epoch 34/50\n",
      "333/333 [==============================] - 17s 51ms/step - loss: 0.1618 - auc: 0.7793 - val_loss: 0.1802 - val_auc: 0.7935\n",
      "Epoch 35/50\n",
      "333/333 [==============================] - 17s 52ms/step - loss: 0.1608 - auc: 0.7834 - val_loss: 0.1642 - val_auc: 0.8039\n",
      "Epoch 36/50\n",
      "333/333 [==============================] - 18s 55ms/step - loss: 0.1601 - auc: 0.7876 - val_loss: 0.1675 - val_auc: 0.8024\n",
      "Epoch 37/50\n",
      "333/333 [==============================] - 21s 62ms/step - loss: 0.1603 - auc: 0.7880 - val_loss: 0.1728 - val_auc: 0.8176\n",
      "Epoch 38/50\n",
      "333/333 [==============================] - 20s 59ms/step - loss: 0.1590 - auc: 0.7925 - val_loss: 0.1841 - val_auc: 0.7916\n",
      "Epoch 39/50\n",
      "333/333 [==============================] - 20s 61ms/step - loss: 0.1585 - auc: 0.7959 - val_loss: 0.1581 - val_auc: 0.8219\n",
      "Epoch 40/50\n",
      "333/333 [==============================] - 17s 51ms/step - loss: 0.1590 - auc: 0.7915 - val_loss: 0.1832 - val_auc: 0.7790\n",
      "Epoch 41/50\n",
      "333/333 [==============================] - 17s 50ms/step - loss: 0.1564 - auc: 0.7990 - val_loss: 0.1639 - val_auc: 0.8138\n",
      "Epoch 42/50\n",
      "333/333 [==============================] - 16s 49ms/step - loss: 0.1582 - auc: 0.7978 - val_loss: 0.1621 - val_auc: 0.8188\n",
      "Epoch 43/50\n",
      "333/333 [==============================] - 17s 52ms/step - loss: 0.1562 - auc: 0.8010 - val_loss: 0.1819 - val_auc: 0.7840\n",
      "Epoch 44/50\n",
      "333/333 [==============================] - 17s 50ms/step - loss: 0.1572 - auc: 0.8004 - val_loss: 0.1618 - val_auc: 0.8107\n",
      "Epoch 45/50\n",
      "333/333 [==============================] - 17s 52ms/step - loss: 0.1560 - auc: 0.8033 - val_loss: 0.1582 - val_auc: 0.8243\n",
      "Epoch 46/50\n",
      "333/333 [==============================] - 17s 50ms/step - loss: 0.1544 - auc: 0.8111 - val_loss: 0.1625 - val_auc: 0.8161\n",
      "Epoch 47/50\n",
      "333/333 [==============================] - 17s 50ms/step - loss: 0.1535 - auc: 0.8114 - val_loss: 0.1701 - val_auc: 0.7957\n",
      "Epoch 48/50\n",
      "333/333 [==============================] - 16s 47ms/step - loss: 0.1541 - auc: 0.8125 - val_loss: 0.1712 - val_auc: 0.8069\n",
      "Epoch 49/50\n",
      "333/333 [==============================] - 17s 50ms/step - loss: 0.1548 - auc: 0.8102 - val_loss: 0.1468 - val_auc: 0.8472\n",
      "Epoch 50/50\n",
      "333/333 [==============================] - 16s 48ms/step - loss: 0.1547 - auc: 0.8068 - val_loss: 0.1720 - val_auc: 0.8206\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x17d46014e50>"
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = GNN.get_default_hyperparameters()\n",
    "params[\"hidden_dim\"] = 64  # Tuned parameter\n",
    "params[\"learning_rate\"] = 0.001  # Tuned parameter\n",
    "params[\"num_layers\"] = 3  # Tuned parameter\n",
    "\n",
    "gnn_layer = GNN(params)\n",
    "gnn_out = gnn_layer(gnn_input)\n",
    "\n",
    "avg = segment_mean(\n",
    "    data=gnn_out,\n",
    "    segment_ids=node2graph\n",
    ")\n",
    "\n",
    "pred = Dense(1, activation='sigmoid')(avg)\n",
    "\n",
    "model2 = Model(\n",
    "    inputs={\n",
    "        'data': data,\n",
    "        'edges': edge,\n",
    "        'node2grah': node2graph,\n",
    "    },\n",
    "    outputs=pred\n",
    ")\n",
    "\n",
    "model2.compile(\n",
    "    loss='BinaryCrossentropy',\n",
    "    optimizer=Adam(learning_rate=params[\"learning_rate\"]),\n",
    "    metrics=['AUC']\n",
    ")\n",
    "\n",
    "import math\n",
    "\n",
    "batch_size = 64\n",
    "num_batchs = math.ceil(len(training_set) / batch_size)\n",
    "num_batchs_validation = math.ceil(len(validation_set) / batch_size)\n",
    "\n",
    "model2.fit(\n",
    "    gen_batch(\n",
    "        training_set, batch_size=batch_size, repeat=True\n",
    "    ),\n",
    "    steps_per_epoch=num_batchs,\n",
    "    epochs=50,  # Increased epochs for tuning\n",
    "    validation_data=gen_batch(\n",
    "        validation_set, batch_size=64, repeat=True\n",
    "    ),\n",
    "    validation_steps=num_batchs_validation,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "193/193 [==============================] - 3s 14ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred = model2.predict(\n",
    "    gen_batch(testing_set, batch_size=64, shuffle=False)\n",
    ")\n",
    "y_pred = np.reshape(y_pred, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "submission = pd.DataFrame({'label':y_pred})\n",
    "submission.index.name = 'id'\n",
    "submission.to_csv('tune_Baseline GCN Model.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC Score: 0.8061419725418091\n"
     ]
    }
   ],
   "source": [
    "# Calculate and print AUC score\n",
    "auc_score = model2.evaluate(\n",
    "    gen_batch(validation_set, batch_size=64, repeat=True),\n",
    "    steps=num_batches_validation,\n",
    "    verbose=0\n",
    ")[1]\n",
    "print(\"AUC Score:\", auc_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a function to create different models with different aggregation mechanisms "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 588,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_single_batch(samples, tokenizer):\n",
    "    sample_nodes = [s[0] for s in samples]\n",
    "    sample_nodes = tokenizer.texts_to_sequences(sample_nodes)\n",
    "    sample_nodes = pad_sequences(sample_nodes, padding='post')\n",
    "    max_nodes_len = np.shape(sample_nodes)[1]\n",
    "    edges = [s[1] + i * max_nodes_len for i, s in enumerate(samples)]\n",
    "    edges = [e for e in edges if len(e) > 0]\n",
    "    node_to_graph = [[i] * max_nodes_len for i in range(len(samples))]\n",
    "\n",
    "    all_nodes = np.reshape(sample_nodes, -1)\n",
    "    all_edges = np.concatenate(edges)\n",
    "    node_to_graph = np.reshape(node_to_graph, -1)\n",
    "    return {\n",
    "        'data': all_nodes,\n",
    "        'edges': all_edges,\n",
    "        'node2graph': node_to_graph,\n",
    "    }, np.array([s[2] for s in samples])\n",
    "\n",
    "def gen_batch(dataset, tokenizer, batch_size=16, repeat=False, shuffle=True):\n",
    "    while True:\n",
    "        dataset = list(dataset)\n",
    "        if shuffle:\n",
    "            random.shuffle(dataset)\n",
    "        l = len(dataset)\n",
    "        for ndx in range(0, l, batch_size):\n",
    "            batch_samples = dataset[ndx:min(ndx + batch_size, l)]\n",
    "            yield prepare_single_batch(batch_samples, tokenizer)\n",
    "        if not repeat:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to build the model with different aggregation mechanisms\n",
    "def build_model_with_aggregation(aggregation_type):\n",
    "    # Model definition\n",
    "    data = Input(batch_shape=(None,))\n",
    "    edge = Input(batch_shape=(None, 2), dtype=tf.int32)\n",
    "    node2graph = Input(batch_shape=(None,), dtype=tf.int32)\n",
    "    embeded = Embedding(tokenizer.num_words, 20)(data)\n",
    "\n",
    "    num_graph = tf.reduce_max(node2graph) + 1\n",
    "\n",
    "    gnn_input = GNNInput(\n",
    "        node_features=embeded,\n",
    "        adjacency_lists=(edge,),\n",
    "        node_to_graph_map=node2graph,\n",
    "        num_graphs=num_graph,\n",
    "    )\n",
    "\n",
    "    params = GNN.get_default_hyperparameters()\n",
    "    params[\"hidden_dim\"] = 64\n",
    "    params[\"aggregation_type\"] = aggregation_type\n",
    "\n",
    "    gnn_layer = GNN(params)\n",
    "    gnn_out = gnn_layer(gnn_input)\n",
    "\n",
    "    avg = segment_mean(\n",
    "        data=gnn_out,\n",
    "        segment_ids=node2graph\n",
    "    )\n",
    "\n",
    "    pred = Dense(1, activation='sigmoid')(avg)\n",
    "\n",
    "    model = Model(\n",
    "        inputs={\n",
    "            'data': data,\n",
    "            'edges': edge,\n",
    "            'node2graph': node2graph,\n",
    "        },\n",
    "        outputs=pred\n",
    "    )\n",
    "\n",
    "    model.compile(\n",
    "        loss='binary_crossentropy',\n",
    "        optimizer=Adam(),\n",
    "        metrics=['AUC']\n",
    "    )\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trial 3: GCN Gather Aggregation Mechanism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trial 3: GCN Gather Aggregation Mechanism\n",
    "\n",
    "**Thoughts and Observations:**\n",
    "\n",
    "- In this trial, the GCN model is modified to use the gather aggregation mechanism.\n",
    "- The gather aggregation mechanism is expected to aggregate information from neighboring nodes by gathering information from all nodes in the graph.\n",
    "- The model is trained for 50 epochs using the default hyperparameters.\n",
    "- Performance metrics such as loss and AUC are monitored during training to evaluate the model's performance.\n",
    "- The increased number of epochs may allow the model to learn more complex patterns and potentially improve performance compared to the baseline model.\n",
    "#### Observation for Trial 3: \n",
    "\n",
    "- The AUC score of 0.812 indicates that the model with the gather aggregation mechanism performs slightly better than the baseline model (Trial 1) with default aggregation mechanism.\n",
    "\n",
    "- The increased number of epochs (50) might have contributed to the improved performance compared to the baseline model.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcn_gather_model = build_model_with_aggregation('gcn_gather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "333/333 [==============================] - 16s 49ms/step - loss: 0.1798 - auc: 0.6934 - val_loss: 0.1916 - val_auc: 0.6960\n",
      "Epoch 2/50\n",
      "333/333 [==============================] - 18s 54ms/step - loss: 0.1783 - auc: 0.6928 - val_loss: 0.1882 - val_auc: 0.6984\n",
      "Epoch 3/50\n",
      "333/333 [==============================] - 16s 49ms/step - loss: 0.1788 - auc: 0.6945 - val_loss: 0.1937 - val_auc: 0.7191\n",
      "Epoch 4/50\n",
      "333/333 [==============================] - 20s 59ms/step - loss: 0.1768 - auc: 0.7092 - val_loss: 0.1865 - val_auc: 0.7095\n",
      "Epoch 5/50\n",
      "333/333 [==============================] - 18s 54ms/step - loss: 0.1764 - auc: 0.7070 - val_loss: 0.1879 - val_auc: 0.7282\n",
      "Epoch 6/50\n",
      "333/333 [==============================] - 16s 48ms/step - loss: 0.1754 - auc: 0.7150 - val_loss: 0.1882 - val_auc: 0.6893\n",
      "Epoch 7/50\n",
      "333/333 [==============================] - 16s 49ms/step - loss: 0.1756 - auc: 0.7099 - val_loss: 0.1838 - val_auc: 0.7485\n",
      "Epoch 8/50\n",
      "333/333 [==============================] - 17s 51ms/step - loss: 0.1745 - auc: 0.7206 - val_loss: 0.1806 - val_auc: 0.7331\n",
      "Epoch 9/50\n",
      "333/333 [==============================] - 20s 59ms/step - loss: 0.1748 - auc: 0.7151 - val_loss: 0.1816 - val_auc: 0.7361\n",
      "Epoch 10/50\n",
      "333/333 [==============================] - 16s 47ms/step - loss: 0.1736 - auc: 0.7276 - val_loss: 0.1939 - val_auc: 0.7358\n",
      "Epoch 11/50\n",
      "333/333 [==============================] - 16s 49ms/step - loss: 0.1722 - auc: 0.7323 - val_loss: 0.1829 - val_auc: 0.7434\n",
      "Epoch 12/50\n",
      "333/333 [==============================] - 19s 58ms/step - loss: 0.1707 - auc: 0.7418 - val_loss: 0.1848 - val_auc: 0.7461\n",
      "Epoch 13/50\n",
      "333/333 [==============================] - 20s 61ms/step - loss: 0.1723 - auc: 0.7349 - val_loss: 0.1787 - val_auc: 0.7541\n",
      "Epoch 14/50\n",
      "333/333 [==============================] - 19s 56ms/step - loss: 0.1693 - auc: 0.7490 - val_loss: 0.1825 - val_auc: 0.7391\n",
      "Epoch 15/50\n",
      "333/333 [==============================] - 18s 53ms/step - loss: 0.1690 - auc: 0.7538 - val_loss: 0.1710 - val_auc: 0.7745\n",
      "Epoch 16/50\n",
      "333/333 [==============================] - 17s 51ms/step - loss: 0.1706 - auc: 0.7430 - val_loss: 0.1875 - val_auc: 0.7659\n",
      "Epoch 17/50\n",
      "333/333 [==============================] - 18s 54ms/step - loss: 0.1678 - auc: 0.7563 - val_loss: 0.1723 - val_auc: 0.7502\n",
      "Epoch 18/50\n",
      "333/333 [==============================] - 17s 52ms/step - loss: 0.1670 - auc: 0.7604 - val_loss: 0.1831 - val_auc: 0.7580\n",
      "Epoch 19/50\n",
      "333/333 [==============================] - 17s 52ms/step - loss: 0.1667 - auc: 0.7642 - val_loss: 0.1651 - val_auc: 0.7923\n",
      "Epoch 20/50\n",
      "333/333 [==============================] - 17s 51ms/step - loss: 0.1672 - auc: 0.7602 - val_loss: 0.1809 - val_auc: 0.7759\n",
      "Epoch 21/50\n",
      "333/333 [==============================] - 17s 51ms/step - loss: 0.1672 - auc: 0.7641 - val_loss: 0.1832 - val_auc: 0.7797\n",
      "Epoch 22/50\n",
      "333/333 [==============================] - 18s 53ms/step - loss: 0.1644 - auc: 0.7756 - val_loss: 0.1710 - val_auc: 0.7944\n",
      "Epoch 23/50\n",
      "333/333 [==============================] - 17s 52ms/step - loss: 0.1652 - auc: 0.7723 - val_loss: 0.1697 - val_auc: 0.7826\n",
      "Epoch 24/50\n",
      "333/333 [==============================] - 17s 51ms/step - loss: 0.1642 - auc: 0.7753 - val_loss: 0.1825 - val_auc: 0.7800\n",
      "Epoch 25/50\n",
      "333/333 [==============================] - 18s 54ms/step - loss: 0.1639 - auc: 0.7733 - val_loss: 0.1689 - val_auc: 0.7842\n",
      "Epoch 26/50\n",
      "333/333 [==============================] - 19s 56ms/step - loss: 0.1652 - auc: 0.7752 - val_loss: 0.1735 - val_auc: 0.7819\n",
      "Epoch 27/50\n",
      "333/333 [==============================] - 18s 53ms/step - loss: 0.1638 - auc: 0.7780 - val_loss: 0.1733 - val_auc: 0.8083\n",
      "Epoch 28/50\n",
      "333/333 [==============================] - 17s 52ms/step - loss: 0.1632 - auc: 0.7810 - val_loss: 0.1787 - val_auc: 0.7837\n",
      "Epoch 29/50\n",
      "333/333 [==============================] - 18s 53ms/step - loss: 0.1635 - auc: 0.7780 - val_loss: 0.1751 - val_auc: 0.7890\n",
      "Epoch 30/50\n",
      "333/333 [==============================] - 18s 53ms/step - loss: 0.1619 - auc: 0.7858 - val_loss: 0.1708 - val_auc: 0.7983\n",
      "Epoch 31/50\n",
      "333/333 [==============================] - 17s 52ms/step - loss: 0.1621 - auc: 0.7852 - val_loss: 0.1705 - val_auc: 0.8078\n",
      "Epoch 32/50\n",
      "333/333 [==============================] - 17s 52ms/step - loss: 0.1624 - auc: 0.7837 - val_loss: 0.1734 - val_auc: 0.7969\n",
      "Epoch 33/50\n",
      "333/333 [==============================] - 18s 53ms/step - loss: 0.1639 - auc: 0.7787 - val_loss: 0.1708 - val_auc: 0.7917\n",
      "Epoch 34/50\n",
      "333/333 [==============================] - 17s 52ms/step - loss: 0.1619 - auc: 0.7892 - val_loss: 0.1859 - val_auc: 0.7911\n",
      "Epoch 35/50\n",
      "333/333 [==============================] - 17s 52ms/step - loss: 0.1618 - auc: 0.7898 - val_loss: 0.1620 - val_auc: 0.8146\n",
      "Epoch 36/50\n",
      "333/333 [==============================] - 17s 52ms/step - loss: 0.1598 - auc: 0.7974 - val_loss: 0.1721 - val_auc: 0.8023\n",
      "Epoch 37/50\n",
      "333/333 [==============================] - 17s 52ms/step - loss: 0.1604 - auc: 0.7946 - val_loss: 0.1803 - val_auc: 0.7847\n",
      "Epoch 38/50\n",
      "333/333 [==============================] - 17s 52ms/step - loss: 0.1612 - auc: 0.7919 - val_loss: 0.1731 - val_auc: 0.7965\n",
      "Epoch 39/50\n",
      "333/333 [==============================] - 17s 52ms/step - loss: 0.1602 - auc: 0.7997 - val_loss: 0.1757 - val_auc: 0.8129\n",
      "Epoch 40/50\n",
      "333/333 [==============================] - 17s 52ms/step - loss: 0.1587 - auc: 0.7997 - val_loss: 0.1673 - val_auc: 0.8060\n",
      "Epoch 41/50\n",
      "333/333 [==============================] - 17s 52ms/step - loss: 0.1598 - auc: 0.7981 - val_loss: 0.1682 - val_auc: 0.7954\n",
      "Epoch 42/50\n",
      "333/333 [==============================] - 17s 52ms/step - loss: 0.1592 - auc: 0.7970 - val_loss: 0.1726 - val_auc: 0.8127\n",
      "Epoch 43/50\n",
      "333/333 [==============================] - 17s 52ms/step - loss: 0.1600 - auc: 0.7972 - val_loss: 0.1704 - val_auc: 0.8202\n",
      "Epoch 44/50\n",
      "333/333 [==============================] - 17s 52ms/step - loss: 0.1583 - auc: 0.8021 - val_loss: 0.1763 - val_auc: 0.7996\n",
      "Epoch 45/50\n",
      "333/333 [==============================] - 17s 52ms/step - loss: 0.1581 - auc: 0.8079 - val_loss: 0.1692 - val_auc: 0.8081\n",
      "Epoch 46/50\n",
      "333/333 [==============================] - 17s 52ms/step - loss: 0.1575 - auc: 0.8085 - val_loss: 0.1767 - val_auc: 0.8127\n",
      "Epoch 47/50\n",
      "333/333 [==============================] - 17s 51ms/step - loss: 0.1580 - auc: 0.8053 - val_loss: 0.1630 - val_auc: 0.8177\n",
      "Epoch 48/50\n",
      "333/333 [==============================] - 18s 53ms/step - loss: 0.1596 - auc: 0.7989 - val_loss: 0.1725 - val_auc: 0.7979\n",
      "Epoch 49/50\n",
      "333/333 [==============================] - 17s 52ms/step - loss: 0.1575 - auc: 0.8058 - val_loss: 0.1650 - val_auc: 0.8170\n",
      "Epoch 50/50\n",
      "333/333 [==============================] - 17s 52ms/step - loss: 0.1578 - auc: 0.8088 - val_loss: 0.1756 - val_auc: 0.8076\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x17d6c498e50>"
      ]
     },
     "execution_count": 361,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 64\n",
    "num_batchs = math.ceil(len(training_set) / batch_size)\n",
    "num_batchs_validation = math.ceil(len(validation_set) / batch_size)\n",
    "\n",
    "gcn_gather_model.fit(\n",
    "    gen_batch(\n",
    "        training_set, tokenizer, batch_size=batch_size, repeat=True\n",
    "    ),\n",
    "    steps_per_epoch=num_batchs,\n",
    "    epochs=50,  # Increased epochs for tuning\n",
    "    validation_data=gen_batch(\n",
    "        validation_set, tokenizer,batch_size=64, repeat=True\n",
    "    ),\n",
    "    validation_steps=num_batchs_validation,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "193/193 [==============================] - 3s 13ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred3 =gcn_gather_model.predict(\n",
    "    gen_batch(testing_set,tokenizer, batch_size=64, shuffle=False)\n",
    ")\n",
    "y_pred3 = np.reshape(y_pred3, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC Score: 0.8122298717498779\n"
     ]
    }
   ],
   "source": [
    "# Calculate and print AUC score\n",
    "auc_score = gcn_gather_model.evaluate(\n",
    "    gen_batch(validation_set,tokenizer,batch_size=64, repeat=True),\n",
    "    steps=num_batches_validation,\n",
    "    verbose=0\n",
    ")[1]\n",
    "print(\"AUC Score:\", auc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({'label':y_pred3})\n",
    "submission.index.name = 'id'\n",
    "submission.to_csv('gcn_gather_model3.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trial 4: Mean Aggregation Mechanism\n",
    "\n",
    "Thoughts and Observations:\n",
    "\n",
    "- In this trial, the GCN model is modified to use the mean aggregation mechanism.\n",
    "- The mean aggregation mechanism calculates the average of the node features from neighboring nodes.\n",
    "Trial 4: Mean Aggregation Mechanism\n",
    "\n",
    "Observations:\n",
    "\n",
    "- The AUC score obtained from this trial is 0.811, which is higher than the baseline model (Trial 1) and slightly lower than the gather aggregation mechanism (Trial 3).\n",
    "- While the mean aggregation mechanism is simple and computationally efficient, it may not capture complex relationships in the graph structure effectively.\n",
    "- The performance improvement compared to the baseline model indicates that the mean aggregation mechanism is a viable option for GCN models, especially for simpler tasks or datasets with less complex graph structures.\n",
    "- Further experimentation with hyperparameters and model architecture adjustments may help improve the performance of the model using the mean aggregation mechanism.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trial 4: Mean Aggregation Mechanism\n",
    "mean_model =build_model_aggregation('mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "333/333 [==============================] - 19s 47ms/step - loss: 0.2609 - auc: 0.4975 - val_loss: 0.2027 - val_auc: 0.6490\n",
      "Epoch 2/50\n",
      "333/333 [==============================] - 16s 47ms/step - loss: 0.1893 - auc: 0.6264 - val_loss: 0.2024 - val_auc: 0.6865\n",
      "Epoch 3/50\n",
      "333/333 [==============================] - 16s 49ms/step - loss: 0.1840 - auc: 0.6660 - val_loss: 0.1899 - val_auc: 0.7126\n",
      "Epoch 4/50\n",
      "333/333 [==============================] - 16s 49ms/step - loss: 0.1831 - auc: 0.6683 - val_loss: 0.1973 - val_auc: 0.6973\n",
      "Epoch 5/50\n",
      "333/333 [==============================] - 17s 50ms/step - loss: 0.1818 - auc: 0.6710 - val_loss: 0.1944 - val_auc: 0.6961\n",
      "Epoch 6/50\n",
      "333/333 [==============================] - 17s 50ms/step - loss: 0.1796 - auc: 0.6894 - val_loss: 0.1832 - val_auc: 0.6964\n",
      "Epoch 7/50\n",
      "333/333 [==============================] - 17s 51ms/step - loss: 0.1763 - auc: 0.7091 - val_loss: 0.1857 - val_auc: 0.7212\n",
      "Epoch 8/50\n",
      "333/333 [==============================] - 17s 50ms/step - loss: 0.1771 - auc: 0.7076 - val_loss: 0.1867 - val_auc: 0.7192\n",
      "Epoch 9/50\n",
      "333/333 [==============================] - 17s 50ms/step - loss: 0.1754 - auc: 0.7173 - val_loss: 0.1901 - val_auc: 0.7041\n",
      "Epoch 10/50\n",
      "333/333 [==============================] - 17s 50ms/step - loss: 0.1743 - auc: 0.7233 - val_loss: 0.1952 - val_auc: 0.7252\n",
      "Epoch 11/50\n",
      "333/333 [==============================] - 17s 51ms/step - loss: 0.1737 - auc: 0.7252 - val_loss: 0.1899 - val_auc: 0.7383\n",
      "Epoch 12/50\n",
      "333/333 [==============================] - 17s 51ms/step - loss: 0.1735 - auc: 0.7251 - val_loss: 0.1872 - val_auc: 0.7208\n",
      "Epoch 13/50\n",
      "333/333 [==============================] - 18s 55ms/step - loss: 0.1733 - auc: 0.7299 - val_loss: 0.1822 - val_auc: 0.7384\n",
      "Epoch 14/50\n",
      "333/333 [==============================] - 17s 52ms/step - loss: 0.1734 - auc: 0.7257 - val_loss: 0.1841 - val_auc: 0.7422\n",
      "Epoch 15/50\n",
      "333/333 [==============================] - 18s 53ms/step - loss: 0.1725 - auc: 0.7335 - val_loss: 0.1880 - val_auc: 0.7448\n",
      "Epoch 16/50\n",
      "333/333 [==============================] - 20s 61ms/step - loss: 0.1706 - auc: 0.7396 - val_loss: 0.1812 - val_auc: 0.7745\n",
      "Epoch 17/50\n",
      "333/333 [==============================] - 18s 53ms/step - loss: 0.1696 - auc: 0.7470 - val_loss: 0.1830 - val_auc: 0.7553\n",
      "Epoch 18/50\n",
      "333/333 [==============================] - 17s 51ms/step - loss: 0.1707 - auc: 0.7430 - val_loss: 0.1789 - val_auc: 0.7301\n",
      "Epoch 19/50\n",
      "333/333 [==============================] - 17s 51ms/step - loss: 0.1688 - auc: 0.7564 - val_loss: 0.1702 - val_auc: 0.7695\n",
      "Epoch 20/50\n",
      "333/333 [==============================] - 17s 52ms/step - loss: 0.1690 - auc: 0.7523 - val_loss: 0.1883 - val_auc: 0.7589\n",
      "Epoch 21/50\n",
      "333/333 [==============================] - 17s 53ms/step - loss: 0.1676 - auc: 0.7592 - val_loss: 0.1716 - val_auc: 0.7557\n",
      "Epoch 22/50\n",
      "333/333 [==============================] - 17s 50ms/step - loss: 0.1689 - auc: 0.7575 - val_loss: 0.1920 - val_auc: 0.7739\n",
      "Epoch 23/50\n",
      "333/333 [==============================] - 16s 49ms/step - loss: 0.1672 - auc: 0.7617 - val_loss: 0.1781 - val_auc: 0.7728\n",
      "Epoch 24/50\n",
      "333/333 [==============================] - 17s 52ms/step - loss: 0.1678 - auc: 0.7594 - val_loss: 0.1751 - val_auc: 0.7692\n",
      "Epoch 25/50\n",
      "333/333 [==============================] - 18s 54ms/step - loss: 0.1660 - auc: 0.7675 - val_loss: 0.1858 - val_auc: 0.7607\n",
      "Epoch 26/50\n",
      "333/333 [==============================] - 17s 51ms/step - loss: 0.1641 - auc: 0.7797 - val_loss: 0.1790 - val_auc: 0.7760\n",
      "Epoch 27/50\n",
      "333/333 [==============================] - 19s 58ms/step - loss: 0.1656 - auc: 0.7693 - val_loss: 0.1705 - val_auc: 0.8065\n",
      "Epoch 28/50\n",
      "333/333 [==============================] - 18s 55ms/step - loss: 0.1650 - auc: 0.7757 - val_loss: 0.1786 - val_auc: 0.7811\n",
      "Epoch 29/50\n",
      "333/333 [==============================] - 17s 50ms/step - loss: 0.1642 - auc: 0.7778 - val_loss: 0.1755 - val_auc: 0.7876\n",
      "Epoch 30/50\n",
      "333/333 [==============================] - 16s 49ms/step - loss: 0.1616 - auc: 0.7868 - val_loss: 0.1769 - val_auc: 0.7850\n",
      "Epoch 31/50\n",
      "333/333 [==============================] - 17s 50ms/step - loss: 0.1629 - auc: 0.7869 - val_loss: 0.1820 - val_auc: 0.7771\n",
      "Epoch 32/50\n",
      "333/333 [==============================] - 16s 49ms/step - loss: 0.1625 - auc: 0.7847 - val_loss: 0.1782 - val_auc: 0.7751\n",
      "Epoch 33/50\n",
      "333/333 [==============================] - 17s 52ms/step - loss: 0.1612 - auc: 0.7927 - val_loss: 0.1711 - val_auc: 0.7915\n",
      "Epoch 34/50\n",
      "333/333 [==============================] - 17s 51ms/step - loss: 0.1621 - auc: 0.7873 - val_loss: 0.1840 - val_auc: 0.7939\n",
      "Epoch 35/50\n",
      "333/333 [==============================] - 18s 54ms/step - loss: 0.1608 - auc: 0.7936 - val_loss: 0.1725 - val_auc: 0.7951\n",
      "Epoch 36/50\n",
      "333/333 [==============================] - 18s 53ms/step - loss: 0.1607 - auc: 0.7956 - val_loss: 0.1754 - val_auc: 0.7944\n",
      "Epoch 37/50\n",
      "333/333 [==============================] - 18s 53ms/step - loss: 0.1603 - auc: 0.7969 - val_loss: 0.1761 - val_auc: 0.7920\n",
      "Epoch 38/50\n",
      "333/333 [==============================] - 17s 52ms/step - loss: 0.1619 - auc: 0.7916 - val_loss: 0.1726 - val_auc: 0.8010\n",
      "Epoch 39/50\n",
      "333/333 [==============================] - 17s 52ms/step - loss: 0.1594 - auc: 0.7995 - val_loss: 0.1630 - val_auc: 0.8177\n",
      "Epoch 40/50\n",
      "333/333 [==============================] - 17s 52ms/step - loss: 0.1598 - auc: 0.8024 - val_loss: 0.1751 - val_auc: 0.8139\n",
      "Epoch 41/50\n",
      "333/333 [==============================] - 17s 52ms/step - loss: 0.1588 - auc: 0.8059 - val_loss: 0.1828 - val_auc: 0.7829\n",
      "Epoch 42/50\n",
      "333/333 [==============================] - 18s 53ms/step - loss: 0.1604 - auc: 0.7985 - val_loss: 0.1714 - val_auc: 0.8270\n",
      "Epoch 43/50\n",
      "333/333 [==============================] - 17s 52ms/step - loss: 0.1589 - auc: 0.8072 - val_loss: 0.1642 - val_auc: 0.8217\n",
      "Epoch 44/50\n",
      "333/333 [==============================] - 17s 52ms/step - loss: 0.1580 - auc: 0.8107 - val_loss: 0.1724 - val_auc: 0.7998\n",
      "Epoch 45/50\n",
      "333/333 [==============================] - 17s 53ms/step - loss: 0.1573 - auc: 0.8137 - val_loss: 0.1768 - val_auc: 0.8141\n",
      "Epoch 46/50\n",
      "333/333 [==============================] - 18s 53ms/step - loss: 0.1568 - auc: 0.8144 - val_loss: 0.1701 - val_auc: 0.7969\n",
      "Epoch 47/50\n",
      "333/333 [==============================] - 18s 54ms/step - loss: 0.1571 - auc: 0.8118 - val_loss: 0.1695 - val_auc: 0.8184\n",
      "Epoch 48/50\n",
      "333/333 [==============================] - 17s 52ms/step - loss: 0.1570 - auc: 0.8136 - val_loss: 0.1762 - val_auc: 0.8072\n",
      "Epoch 49/50\n",
      "333/333 [==============================] - 18s 53ms/step - loss: 0.1576 - auc: 0.8122 - val_loss: 0.1760 - val_auc: 0.7861\n",
      "Epoch 50/50\n",
      "333/333 [==============================] - 18s 53ms/step - loss: 0.1562 - auc: 0.8193 - val_loss: 0.1720 - val_auc: 0.7995\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x17d67999e10>"
      ]
     },
     "execution_count": 384,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_model.fit(\n",
    "    gen_batch(\n",
    "        training_set, tokenizer, batch_size=batch_size, repeat=True\n",
    "    ),\n",
    "    steps_per_epoch=num_batchs,\n",
    "    epochs=50,  # Increased epochs for tuning\n",
    "    validation_data=gen_batch(\n",
    "        validation_set, tokenizer,batch_size=64, repeat=True\n",
    "    ),\n",
    "    validation_steps=num_batchs_validation,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "193/193 [==============================] - 3s 13ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred4 =mean_model.predict(\n",
    "    gen_batch(testing_set,tokenizer, batch_size=64, shuffle=False)\n",
    ")\n",
    "y_pred4 = np.reshape(y_pred4, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC Score: 0.8110017776489258\n"
     ]
    }
   ],
   "source": [
    "# Calculate and print AUC score\n",
    "auc_score = mean_model.evaluate(\n",
    "    gen_batch(validation_set,tokenizer,batch_size=64, repeat=True),\n",
    "    steps=num_batches_validation,\n",
    "    verbose=0\n",
    ")[1]\n",
    "print(\"AUC Score:\", auc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({'label':y_pred4})\n",
    "submission.index.name = 'id'\n",
    "submission.to_csv('mean_model4_64.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trial 5: Max Aggregation Mechanism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thoughts and Observations:\n",
    "\n",
    "In Trial the model was modified to use the max aggregation mechanism in the GCN layer and trained for 50 epochs. Here are some thoughts and observations:\n",
    "- The max aggregation mechanism selects the maximum node feature value from neighboring nodes, capturing the most significant information in the local neighborhood.\n",
    "- **Performance**: The model achieved an AUC score of 0.8162, which is slightly higher than the previous trials. This indicates that the max aggregation mechanism may have helped improve the model's ability to capture important features in the graph data.\n",
    "\n",
    "- **Overfitting**: There is a slight decrease in the validation AUC compared to the training AUC, suggesting potential overfitting. To address this, regularization techniques such as dropout or early stopping could be applied in future trials.\n",
    "\n",
    "- **Model Complexity**: The model architecture remains the same as in previous trials, with a hidden dimension size of 64 and a single GCN layer. Experimenting with deeper architectures or different hyperparameters could be explored in subsequent trials.\n",
    "\n",
    "Overall, training the model for 50 epochs with the max aggregation mechanism shows promise in improving model performance. Further experimentation and optimization are needed to fully leverage its benefits and address potential overfitting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trial 5: Max Aggregation Mechanism\n",
    "max_model = build_model_aggregation('max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "333/333 [==============================] - 18s 55ms/step - loss: 0.1749 - auc: 0.7156 - val_loss: 0.1822 - val_auc: 0.7428\n",
      "Epoch 2/50\n",
      "333/333 [==============================] - 17s 51ms/step - loss: 0.1736 - auc: 0.7236 - val_loss: 0.1867 - val_auc: 0.7474\n",
      "Epoch 3/50\n",
      "333/333 [==============================] - 18s 55ms/step - loss: 0.1715 - auc: 0.7362 - val_loss: 0.1831 - val_auc: 0.7520\n",
      "Epoch 4/50\n",
      "333/333 [==============================] - 16s 48ms/step - loss: 0.1716 - auc: 0.7376 - val_loss: 0.1833 - val_auc: 0.7449\n",
      "Epoch 5/50\n",
      "333/333 [==============================] - 17s 51ms/step - loss: 0.1698 - auc: 0.7479 - val_loss: 0.1780 - val_auc: 0.7886\n",
      "Epoch 6/50\n",
      "333/333 [==============================] - 17s 51ms/step - loss: 0.1698 - auc: 0.7491 - val_loss: 0.1715 - val_auc: 0.7503\n",
      "Epoch 7/50\n",
      "333/333 [==============================] - 17s 51ms/step - loss: 0.1689 - auc: 0.7474 - val_loss: 0.1835 - val_auc: 0.7658\n",
      "Epoch 8/50\n",
      "333/333 [==============================] - 17s 52ms/step - loss: 0.1675 - auc: 0.7574 - val_loss: 0.1778 - val_auc: 0.7715\n",
      "Epoch 9/50\n",
      "333/333 [==============================] - 17s 51ms/step - loss: 0.1664 - auc: 0.7597 - val_loss: 0.1831 - val_auc: 0.7885\n",
      "Epoch 10/50\n",
      "333/333 [==============================] - 17s 52ms/step - loss: 0.1667 - auc: 0.7653 - val_loss: 0.1753 - val_auc: 0.7889\n",
      "Epoch 11/50\n",
      "333/333 [==============================] - 17s 51ms/step - loss: 0.1660 - auc: 0.7672 - val_loss: 0.1656 - val_auc: 0.7806\n",
      "Epoch 12/50\n",
      "333/333 [==============================] - 17s 51ms/step - loss: 0.1658 - auc: 0.7690 - val_loss: 0.1855 - val_auc: 0.7935\n",
      "Epoch 13/50\n",
      "333/333 [==============================] - 17s 52ms/step - loss: 0.1647 - auc: 0.7738 - val_loss: 0.1710 - val_auc: 0.7938\n",
      "Epoch 14/50\n",
      "333/333 [==============================] - 17s 51ms/step - loss: 0.1627 - auc: 0.7844 - val_loss: 0.1894 - val_auc: 0.7949\n",
      "Epoch 15/50\n",
      "333/333 [==============================] - 17s 52ms/step - loss: 0.1642 - auc: 0.7779 - val_loss: 0.1669 - val_auc: 0.7703\n",
      "Epoch 16/50\n",
      "333/333 [==============================] - 17s 51ms/step - loss: 0.1652 - auc: 0.7725 - val_loss: 0.1727 - val_auc: 0.8220\n",
      "Epoch 17/50\n",
      "333/333 [==============================] - 17s 52ms/step - loss: 0.1643 - auc: 0.7786 - val_loss: 0.1787 - val_auc: 0.7666\n",
      "Epoch 18/50\n",
      "333/333 [==============================] - 17s 51ms/step - loss: 0.1632 - auc: 0.7814 - val_loss: 0.1649 - val_auc: 0.7955\n",
      "Epoch 19/50\n",
      "333/333 [==============================] - 17s 52ms/step - loss: 0.1633 - auc: 0.7843 - val_loss: 0.1871 - val_auc: 0.7926\n",
      "Epoch 20/50\n",
      "333/333 [==============================] - 18s 53ms/step - loss: 0.1629 - auc: 0.7841 - val_loss: 0.1809 - val_auc: 0.7867\n",
      "Epoch 21/50\n",
      "333/333 [==============================] - 17s 52ms/step - loss: 0.1633 - auc: 0.7809 - val_loss: 0.1832 - val_auc: 0.8010\n",
      "Epoch 22/50\n",
      "333/333 [==============================] - 17s 52ms/step - loss: 0.1623 - auc: 0.7873 - val_loss: 0.1751 - val_auc: 0.8015\n",
      "Epoch 23/50\n",
      "333/333 [==============================] - 17s 51ms/step - loss: 0.1613 - auc: 0.7883 - val_loss: 0.1823 - val_auc: 0.7616\n",
      "Epoch 24/50\n",
      "333/333 [==============================] - 17s 52ms/step - loss: 0.1623 - auc: 0.7883 - val_loss: 0.1784 - val_auc: 0.8229\n",
      "Epoch 25/50\n",
      "333/333 [==============================] - 17s 51ms/step - loss: 0.1629 - auc: 0.7843 - val_loss: 0.1608 - val_auc: 0.8257\n",
      "Epoch 26/50\n",
      "333/333 [==============================] - 17s 52ms/step - loss: 0.1602 - auc: 0.7946 - val_loss: 0.1928 - val_auc: 0.7671\n",
      "Epoch 27/50\n",
      "333/333 [==============================] - 17s 51ms/step - loss: 0.1615 - auc: 0.7952 - val_loss: 0.1701 - val_auc: 0.8071\n",
      "Epoch 28/50\n",
      "333/333 [==============================] - 17s 52ms/step - loss: 0.1610 - auc: 0.7923 - val_loss: 0.1890 - val_auc: 0.7886\n",
      "Epoch 29/50\n",
      "333/333 [==============================] - 17s 51ms/step - loss: 0.1600 - auc: 0.7980 - val_loss: 0.1818 - val_auc: 0.7988\n",
      "Epoch 30/50\n",
      "333/333 [==============================] - 17s 52ms/step - loss: 0.1614 - auc: 0.7884 - val_loss: 0.1782 - val_auc: 0.7875\n",
      "Epoch 31/50\n",
      "333/333 [==============================] - 17s 52ms/step - loss: 0.1594 - auc: 0.8006 - val_loss: 0.1827 - val_auc: 0.8118\n",
      "Epoch 32/50\n",
      "333/333 [==============================] - 18s 53ms/step - loss: 0.1609 - auc: 0.7979 - val_loss: 0.1720 - val_auc: 0.7828\n",
      "Epoch 33/50\n",
      "333/333 [==============================] - 18s 54ms/step - loss: 0.1586 - auc: 0.8040 - val_loss: 0.1719 - val_auc: 0.8110\n",
      "Epoch 34/50\n",
      "333/333 [==============================] - 17s 52ms/step - loss: 0.1582 - auc: 0.8026 - val_loss: 0.1819 - val_auc: 0.8009\n",
      "Epoch 35/50\n",
      "333/333 [==============================] - 18s 55ms/step - loss: 0.1595 - auc: 0.8011 - val_loss: 0.1792 - val_auc: 0.8049\n",
      "Epoch 36/50\n",
      "333/333 [==============================] - 17s 52ms/step - loss: 0.1590 - auc: 0.8026 - val_loss: 0.1810 - val_auc: 0.7972\n",
      "Epoch 37/50\n",
      "333/333 [==============================] - 17s 51ms/step - loss: 0.1572 - auc: 0.8114 - val_loss: 0.1704 - val_auc: 0.8064\n",
      "Epoch 38/50\n",
      "333/333 [==============================] - 16s 49ms/step - loss: 0.1583 - auc: 0.8053 - val_loss: 0.1731 - val_auc: 0.8236\n",
      "Epoch 39/50\n",
      "333/333 [==============================] - 19s 56ms/step - loss: 0.1582 - auc: 0.8073 - val_loss: 0.1727 - val_auc: 0.8157\n",
      "Epoch 40/50\n",
      "333/333 [==============================] - 17s 50ms/step - loss: 0.1561 - auc: 0.8115 - val_loss: 0.1812 - val_auc: 0.8058\n",
      "Epoch 41/50\n",
      "333/333 [==============================] - 17s 52ms/step - loss: 0.1557 - auc: 0.8163 - val_loss: 0.1749 - val_auc: 0.8110\n",
      "Epoch 42/50\n",
      "333/333 [==============================] - 18s 53ms/step - loss: 0.1557 - auc: 0.8169 - val_loss: 0.1665 - val_auc: 0.8299\n",
      "Epoch 43/50\n",
      "333/333 [==============================] - 18s 53ms/step - loss: 0.1564 - auc: 0.8131 - val_loss: 0.1792 - val_auc: 0.8140\n",
      "Epoch 44/50\n",
      "333/333 [==============================] - 18s 53ms/step - loss: 0.1552 - auc: 0.8190 - val_loss: 0.1877 - val_auc: 0.8025\n",
      "Epoch 45/50\n",
      "333/333 [==============================] - 18s 55ms/step - loss: 0.1558 - auc: 0.8146 - val_loss: 0.1846 - val_auc: 0.8044\n",
      "Epoch 46/50\n",
      "333/333 [==============================] - 18s 54ms/step - loss: 0.1558 - auc: 0.8132 - val_loss: 0.1631 - val_auc: 0.8269\n",
      "Epoch 47/50\n",
      "333/333 [==============================] - 18s 55ms/step - loss: 0.1536 - auc: 0.8197 - val_loss: 0.1556 - val_auc: 0.8508\n",
      "Epoch 48/50\n",
      "333/333 [==============================] - 18s 55ms/step - loss: 0.1554 - auc: 0.8172 - val_loss: 0.1918 - val_auc: 0.7921\n",
      "Epoch 49/50\n",
      "333/333 [==============================] - 19s 58ms/step - loss: 0.1536 - auc: 0.8215 - val_loss: 0.1784 - val_auc: 0.8054\n",
      "Epoch 50/50\n",
      "333/333 [==============================] - 18s 54ms/step - loss: 0.1536 - auc: 0.8218 - val_loss: 0.1640 - val_auc: 0.8257\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x17d803be550>"
      ]
     },
     "execution_count": 420,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_model.fit(\n",
    "    gen_batch(\n",
    "        training_set, tokenizer, batch_size=batch_size, repeat=True\n",
    "    ),\n",
    "    steps_per_epoch=num_batchs,\n",
    "    epochs=50,  # Increased epochs for tuning\n",
    "    validation_data=gen_batch(\n",
    "        validation_set, tokenizer,batch_size=64, repeat=True\n",
    "    ),\n",
    "    validation_steps=num_batchs_validation,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "193/193 [==============================] - 3s 13ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred5 =max_model.predict(\n",
    "    gen_batch(testing_set,tokenizer, batch_size=64, shuffle=False)\n",
    ")\n",
    "y_pred5 = np.reshape(y_pred5, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC Score: 0.8162156939506531\n"
     ]
    }
   ],
   "source": [
    "# Calculate and print AUC score\n",
    "auc_score = max_model.evaluate(\n",
    "    gen_batch(validation_set,tokenizer,batch_size=64, repeat=True),\n",
    "    steps=num_batches_validation,\n",
    "    verbose=0\n",
    ")[1]\n",
    "print(\"AUC Score:\", auc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({'label':y_pred5})\n",
    "submission.index.name = 'id'\n",
    "submission.to_csv('max_model5.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trial 6: LSTM Aggregation Mechanism with  tuned parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Thoughts and Observations:\n",
    "- In this trial, the LSTM aggregation mechanism is utilized with tuned parameters to potentially improve model performance.\n",
    "- The hidden dimension size is increased to 64, and the number of layers is increased to 3 to allow the model to learn more complex patterns.\n",
    "- A gate mechanism is added to the LSTM aggregation to enhance the model's ability to capture dependencies between nodes.\n",
    "- The learning rate is set to 0.001 with an exponential decay schedule.\n",
    "- The model is trained for 50 epochs to allow sufficient time for learning with the adjusted parameters.\n",
    "- Performance metrics such as loss and AUC are monitored during training to evaluate model performance.\n",
    "#### Results:\n",
    "- The AUC score for the LSTM aggregation mechanism with tuned parameters is 0.8254, indicating improved performance compared to previous trials.\n",
    "\n",
    "#### Observations:\n",
    "- The increased number of layers and hidden dimensions, along with the addition of a gate mechanism, have led to improved model performance.\n",
    "- The use of a learning rate schedule helps stabilize training and potentially avoid overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "333/333 [==============================] - 19s 47ms/step - loss: 0.2265 - auc: 0.5287 - val_loss: 0.1958 - val_auc: 0.6613\n",
      "Epoch 2/50\n",
      "333/333 [==============================] - 15s 46ms/step - loss: 0.1836 - auc: 0.6546 - val_loss: 0.1901 - val_auc: 0.7068\n",
      "Epoch 3/50\n",
      "333/333 [==============================] - 16s 49ms/step - loss: 0.1792 - auc: 0.6867 - val_loss: 0.1938 - val_auc: 0.7014\n",
      "Epoch 4/50\n",
      "333/333 [==============================] - 16s 49ms/step - loss: 0.1782 - auc: 0.6876 - val_loss: 0.1860 - val_auc: 0.7174\n",
      "Epoch 5/50\n",
      "333/333 [==============================] - 16s 48ms/step - loss: 0.1766 - auc: 0.7036 - val_loss: 0.1812 - val_auc: 0.7423\n",
      "Epoch 6/50\n",
      "333/333 [==============================] - 17s 52ms/step - loss: 0.1763 - auc: 0.7093 - val_loss: 0.1818 - val_auc: 0.7314\n",
      "Epoch 7/50\n",
      "333/333 [==============================] - 17s 52ms/step - loss: 0.1761 - auc: 0.7118 - val_loss: 0.1884 - val_auc: 0.7369\n",
      "Epoch 8/50\n",
      "333/333 [==============================] - 16s 49ms/step - loss: 0.1751 - auc: 0.7170 - val_loss: 0.1800 - val_auc: 0.7544\n",
      "Epoch 9/50\n",
      "333/333 [==============================] - 16s 49ms/step - loss: 0.1740 - auc: 0.7173 - val_loss: 0.1853 - val_auc: 0.7397\n",
      "Epoch 10/50\n",
      "333/333 [==============================] - 16s 49ms/step - loss: 0.1748 - auc: 0.7170 - val_loss: 0.1837 - val_auc: 0.7148\n",
      "Epoch 11/50\n",
      "333/333 [==============================] - 16s 48ms/step - loss: 0.1728 - auc: 0.7282 - val_loss: 0.1854 - val_auc: 0.7706\n",
      "Epoch 12/50\n",
      "333/333 [==============================] - 15s 45ms/step - loss: 0.1699 - auc: 0.7488 - val_loss: 0.1772 - val_auc: 0.7699\n",
      "Epoch 13/50\n",
      "333/333 [==============================] - 15s 46ms/step - loss: 0.1705 - auc: 0.7443 - val_loss: 0.1870 - val_auc: 0.7446\n",
      "Epoch 14/50\n",
      "333/333 [==============================] - 16s 47ms/step - loss: 0.1692 - auc: 0.7480 - val_loss: 0.1847 - val_auc: 0.7434\n",
      "Epoch 15/50\n",
      "333/333 [==============================] - 16s 48ms/step - loss: 0.1677 - auc: 0.7553 - val_loss: 0.1737 - val_auc: 0.7579\n",
      "Epoch 16/50\n",
      "333/333 [==============================] - 17s 50ms/step - loss: 0.1674 - auc: 0.7566 - val_loss: 0.1860 - val_auc: 0.7791\n",
      "Epoch 17/50\n",
      "333/333 [==============================] - 16s 48ms/step - loss: 0.1676 - auc: 0.7526 - val_loss: 0.1716 - val_auc: 0.7649\n",
      "Epoch 18/50\n",
      "333/333 [==============================] - 17s 50ms/step - loss: 0.1675 - auc: 0.7600 - val_loss: 0.1820 - val_auc: 0.7541\n",
      "Epoch 19/50\n",
      "333/333 [==============================] - 17s 50ms/step - loss: 0.1670 - auc: 0.7604 - val_loss: 0.1781 - val_auc: 0.7705\n",
      "Epoch 20/50\n",
      "333/333 [==============================] - 17s 50ms/step - loss: 0.1664 - auc: 0.7598 - val_loss: 0.1817 - val_auc: 0.7827\n",
      "Epoch 21/50\n",
      "333/333 [==============================] - 16s 49ms/step - loss: 0.1653 - auc: 0.7648 - val_loss: 0.1721 - val_auc: 0.7789\n",
      "Epoch 22/50\n",
      "333/333 [==============================] - 15s 46ms/step - loss: 0.1662 - auc: 0.7615 - val_loss: 0.1823 - val_auc: 0.7674\n",
      "Epoch 23/50\n",
      "333/333 [==============================] - 15s 45ms/step - loss: 0.1661 - auc: 0.7606 - val_loss: 0.1761 - val_auc: 0.7774\n",
      "Epoch 24/50\n",
      "333/333 [==============================] - 17s 51ms/step - loss: 0.1662 - auc: 0.7637 - val_loss: 0.1836 - val_auc: 0.7763\n",
      "Epoch 25/50\n",
      "333/333 [==============================] - 17s 51ms/step - loss: 0.1658 - auc: 0.7642 - val_loss: 0.1774 - val_auc: 0.7834\n",
      "Epoch 26/50\n",
      "333/333 [==============================] - 17s 49ms/step - loss: 0.1648 - auc: 0.7689 - val_loss: 0.1725 - val_auc: 0.7762\n",
      "Epoch 27/50\n",
      "333/333 [==============================] - 17s 50ms/step - loss: 0.1643 - auc: 0.7726 - val_loss: 0.1706 - val_auc: 0.8001\n",
      "Epoch 28/50\n",
      "333/333 [==============================] - 17s 50ms/step - loss: 0.1640 - auc: 0.7730 - val_loss: 0.1760 - val_auc: 0.7820\n",
      "Epoch 29/50\n",
      "333/333 [==============================] - 17s 50ms/step - loss: 0.1629 - auc: 0.7785 - val_loss: 0.1737 - val_auc: 0.7955\n",
      "Epoch 30/50\n",
      "333/333 [==============================] - 17s 50ms/step - loss: 0.1617 - auc: 0.7812 - val_loss: 0.1710 - val_auc: 0.7943\n",
      "Epoch 31/50\n",
      "333/333 [==============================] - 17s 50ms/step - loss: 0.1615 - auc: 0.7861 - val_loss: 0.1693 - val_auc: 0.8020\n",
      "Epoch 32/50\n",
      "333/333 [==============================] - 17s 50ms/step - loss: 0.1614 - auc: 0.7759 - val_loss: 0.1791 - val_auc: 0.8011\n",
      "Epoch 33/50\n",
      "333/333 [==============================] - 16s 50ms/step - loss: 0.1612 - auc: 0.7809 - val_loss: 0.1690 - val_auc: 0.8081\n",
      "Epoch 34/50\n",
      "333/333 [==============================] - 17s 50ms/step - loss: 0.1607 - auc: 0.7844 - val_loss: 0.1676 - val_auc: 0.7965\n",
      "Epoch 35/50\n",
      "333/333 [==============================] - 17s 51ms/step - loss: 0.1610 - auc: 0.7831 - val_loss: 0.1742 - val_auc: 0.7954\n",
      "Epoch 36/50\n",
      "333/333 [==============================] - 17s 52ms/step - loss: 0.1603 - auc: 0.7892 - val_loss: 0.1743 - val_auc: 0.8016\n",
      "Epoch 37/50\n",
      "333/333 [==============================] - 17s 50ms/step - loss: 0.1594 - auc: 0.7920 - val_loss: 0.1591 - val_auc: 0.8143\n",
      "Epoch 38/50\n",
      "333/333 [==============================] - 17s 51ms/step - loss: 0.1584 - auc: 0.7949 - val_loss: 0.1722 - val_auc: 0.7938\n",
      "Epoch 39/50\n",
      "333/333 [==============================] - 17s 50ms/step - loss: 0.1585 - auc: 0.7943 - val_loss: 0.1775 - val_auc: 0.8205\n",
      "Epoch 40/50\n",
      "333/333 [==============================] - 17s 52ms/step - loss: 0.1581 - auc: 0.7931 - val_loss: 0.1507 - val_auc: 0.8160\n",
      "Epoch 41/50\n",
      "333/333 [==============================] - 17s 50ms/step - loss: 0.1570 - auc: 0.7985 - val_loss: 0.1859 - val_auc: 0.8040\n",
      "Epoch 42/50\n",
      "333/333 [==============================] - 17s 51ms/step - loss: 0.1574 - auc: 0.7976 - val_loss: 0.1668 - val_auc: 0.8088\n",
      "Epoch 43/50\n",
      "333/333 [==============================] - 18s 53ms/step - loss: 0.1573 - auc: 0.7976 - val_loss: 0.1567 - val_auc: 0.8315\n",
      "Epoch 44/50\n",
      "333/333 [==============================] - 18s 53ms/step - loss: 0.1564 - auc: 0.8005 - val_loss: 0.1673 - val_auc: 0.8118\n",
      "Epoch 45/50\n",
      "333/333 [==============================] - 18s 53ms/step - loss: 0.1558 - auc: 0.8044 - val_loss: 0.1676 - val_auc: 0.8208\n",
      "Epoch 46/50\n",
      "333/333 [==============================] - 17s 52ms/step - loss: 0.1560 - auc: 0.8030 - val_loss: 0.1736 - val_auc: 0.8200\n",
      "Epoch 47/50\n",
      "333/333 [==============================] - 17s 52ms/step - loss: 0.1557 - auc: 0.8069 - val_loss: 0.1521 - val_auc: 0.8131\n",
      "Epoch 48/50\n",
      "333/333 [==============================] - 17s 52ms/step - loss: 0.1554 - auc: 0.8006 - val_loss: 0.1652 - val_auc: 0.8418\n",
      "Epoch 49/50\n",
      "333/333 [==============================] - 18s 53ms/step - loss: 0.1547 - auc: 0.8078 - val_loss: 0.1567 - val_auc: 0.8225\n",
      "Epoch 50/50\n",
      "333/333 [==============================] - 18s 54ms/step - loss: 0.1541 - auc: 0.8101 - val_loss: 0.1769 - val_auc: 0.8284\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x17d90542f50>"
      ]
     },
     "execution_count": 430,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a function to build the model with LSTM aggregation mechanism and tuned parameters\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "\n",
    "# Define a function to build the model with LSTM aggregation mechanism and tuned parameters\n",
    "def build_model_tuned_lstm_aggregation(learning_rate=0.001):\n",
    "    # Model definition\n",
    "    data = Input(batch_shape=(None,))\n",
    "    edge = Input(batch_shape=(None, 2), dtype=tf.int32)\n",
    "    node2graph = Input(batch_shape=(None,), dtype=tf.int32)\n",
    "    embeded = Embedding(tokenizer.num_words, 20)(data)\n",
    "\n",
    "    num_graph = tf.reduce_max(node2graph) + 1\n",
    "\n",
    "    gnn_input = GNNInput(\n",
    "        node_features=embeded,\n",
    "        adjacency_lists=(edge,),\n",
    "        node_to_graph_map=node2graph,\n",
    "        num_graphs=num_graph,\n",
    "    )\n",
    "\n",
    "    params = GNN.get_default_hyperparameters()\n",
    "    params[\"hidden_dim\"] = 64  # Increase hidden dimension\n",
    "    params[\"num_layers\"] = 3  # Increase number of layers\n",
    "    params[\"aggregation_type\"] = \"lstm\"\n",
    "    params[\"use_gate\"] = True  # Add gate\n",
    "\n",
    "    gnn_layer = GNN(params)\n",
    "    gnn_out = gnn_layer(gnn_input)\n",
    "\n",
    "    avg = segment_mean(\n",
    "        data=gnn_out,\n",
    "        segment_ids=node2graph\n",
    "    )\n",
    "\n",
    "    pred = Dense(1, activation='sigmoid')(avg)\n",
    "\n",
    "    model = Model(\n",
    "        inputs={\n",
    "            'data': data,\n",
    "            'edges': edge,\n",
    "            'node2graph': node2graph,\n",
    "        },\n",
    "        outputs=pred\n",
    "    )\n",
    "\n",
    "    # Define the learning rate schedule\n",
    "    lr_schedule = ExponentialDecay(\n",
    "        initial_learning_rate=learning_rate,\n",
    "        decay_steps=1000,\n",
    "        decay_rate=0.95,\n",
    "        staircase=True\n",
    "    )\n",
    "\n",
    "    # Compile the model with the learning rate\n",
    "    model.compile(\n",
    "        loss='binary_crossentropy',\n",
    "        optimizer=Adam(learning_rate=lr_schedule),\n",
    "        metrics=['AUC']\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "# Build the LSTM aggregation model with tuned parameters and specified learning rate\n",
    "tuned_lstm_model = build_model_tuned_lstm_aggregation(learning_rate=0.001)\n",
    "\n",
    "# Train the model\n",
    "tuned_lstm_model.fit(\n",
    "    gen_batch(training_set, tokenizer, batch_size=64, repeat=True),\n",
    "    steps_per_epoch=num_batches,\n",
    "    epochs=50,\n",
    "    validation_data=gen_batch(validation_set, tokenizer, batch_size=64, repeat=True),\n",
    "    validation_steps=num_batches_validation\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "193/193 [==============================] - 3s 14ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred6 =tuned_lstm_model.predict(\n",
    "    gen_batch(testing_set,tokenizer, batch_size=64, shuffle=False)\n",
    ")\n",
    "y_pred6 = np.reshape(y_pred6, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC Score: 0.8254103660583496\n"
     ]
    }
   ],
   "source": [
    "# Calculate and print AUC score\n",
    "auc_score = tuned_lstm_model.evaluate(\n",
    "    gen_batch(validation_set,tokenizer,batch_size=64, repeat=True),\n",
    "    steps=num_batches_validation,\n",
    "    verbose=0\n",
    ")[1]\n",
    "print(\"AUC Score:\", auc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({'label':y_pred6})\n",
    "submission.index.name = 'id'\n",
    "submission.to_csv('tuned_lstm_model6.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trial 7: GGNN with Default Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thoughts and Observations:\n",
    "- GGNN message passing mechanism is used with default hyperparameters.\n",
    "- The model consists of 2 GNN message passing layers.\n",
    "- The default hyperparameters are used for other configurations.\n",
    "- The model is trained for 50 epochs to evaluate its performance.\n",
    "\n",
    "### Observations:\n",
    "- The performance of the GGNN model will be evaluated based on the AUC score and loss metrics.\n",
    "\n",
    "- The GGNN model with default hyperparameters achieved an AUC score of 0.8326 after training for 50 epochs.\n",
    "- The training and validation AUC scores increased steadily throughout training, indicating effective learning and generalization capabilities of the model.\n",
    "- The loss decreased consistently, suggesting that the model effectively minimized the binary cross-entropy loss.\n",
    "- The model exhibited signs of overfitting towards the end of training, as the validation AUC score started to plateau while the training AUC continued to improve.\n",
    "- Further regularization techniques may be necessary to address overfitting and improve generalization performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "333/333 [==============================] - 13s 27ms/step - loss: 0.2335 - auc: 0.5211 - val_loss: 0.1975 - val_auc: 0.6597\n",
      "Epoch 2/50\n",
      "333/333 [==============================] - 9s 26ms/step - loss: 0.1865 - auc: 0.6223 - val_loss: 0.1987 - val_auc: 0.6655\n",
      "Epoch 3/50\n",
      "333/333 [==============================] - 9s 27ms/step - loss: 0.1839 - auc: 0.6481 - val_loss: 0.1886 - val_auc: 0.6717\n",
      "Epoch 4/50\n",
      "333/333 [==============================] - 10s 29ms/step - loss: 0.1809 - auc: 0.6633 - val_loss: 0.1924 - val_auc: 0.6847\n",
      "Epoch 5/50\n",
      "333/333 [==============================] - 9s 28ms/step - loss: 0.1794 - auc: 0.6683 - val_loss: 0.1886 - val_auc: 0.6937\n",
      "Epoch 6/50\n",
      "333/333 [==============================] - 9s 28ms/step - loss: 0.1765 - auc: 0.6943 - val_loss: 0.1884 - val_auc: 0.7229\n",
      "Epoch 7/50\n",
      "333/333 [==============================] - 9s 28ms/step - loss: 0.1758 - auc: 0.7075 - val_loss: 0.1897 - val_auc: 0.6797\n",
      "Epoch 8/50\n",
      "333/333 [==============================] - 9s 28ms/step - loss: 0.1762 - auc: 0.7085 - val_loss: 0.1902 - val_auc: 0.7261\n",
      "Epoch 9/50\n",
      "333/333 [==============================] - 9s 27ms/step - loss: 0.1749 - auc: 0.7108 - val_loss: 0.1765 - val_auc: 0.7498\n",
      "Epoch 10/50\n",
      "333/333 [==============================] - 9s 28ms/step - loss: 0.1718 - auc: 0.7302 - val_loss: 0.1787 - val_auc: 0.7653\n",
      "Epoch 11/50\n",
      "333/333 [==============================] - 9s 28ms/step - loss: 0.1694 - auc: 0.7430 - val_loss: 0.1787 - val_auc: 0.7451\n",
      "Epoch 12/50\n",
      "333/333 [==============================] - 9s 28ms/step - loss: 0.1666 - auc: 0.7616 - val_loss: 0.1747 - val_auc: 0.7844\n",
      "Epoch 13/50\n",
      "333/333 [==============================] - 9s 28ms/step - loss: 0.1661 - auc: 0.7666 - val_loss: 0.1772 - val_auc: 0.7943\n",
      "Epoch 14/50\n",
      "333/333 [==============================] - 9s 28ms/step - loss: 0.1639 - auc: 0.7770 - val_loss: 0.1715 - val_auc: 0.8077\n",
      "Epoch 15/50\n",
      "333/333 [==============================] - 9s 28ms/step - loss: 0.1632 - auc: 0.7792 - val_loss: 0.1794 - val_auc: 0.7964\n",
      "Epoch 16/50\n",
      "333/333 [==============================] - 9s 28ms/step - loss: 0.1637 - auc: 0.7751 - val_loss: 0.1774 - val_auc: 0.7818\n",
      "Epoch 17/50\n",
      "333/333 [==============================] - 9s 28ms/step - loss: 0.1623 - auc: 0.7845 - val_loss: 0.1649 - val_auc: 0.7887\n",
      "Epoch 18/50\n",
      "333/333 [==============================] - 9s 28ms/step - loss: 0.1622 - auc: 0.7860 - val_loss: 0.1704 - val_auc: 0.8090\n",
      "Epoch 19/50\n",
      "333/333 [==============================] - 9s 28ms/step - loss: 0.1612 - auc: 0.7840 - val_loss: 0.1811 - val_auc: 0.7788\n",
      "Epoch 20/50\n",
      "333/333 [==============================] - 9s 28ms/step - loss: 0.1603 - auc: 0.7891 - val_loss: 0.1606 - val_auc: 0.8042\n",
      "Epoch 21/50\n",
      "333/333 [==============================] - 9s 28ms/step - loss: 0.1595 - auc: 0.7946 - val_loss: 0.1744 - val_auc: 0.7840\n",
      "Epoch 22/50\n",
      "333/333 [==============================] - 9s 28ms/step - loss: 0.1589 - auc: 0.7978 - val_loss: 0.1786 - val_auc: 0.7918\n",
      "Epoch 23/50\n",
      "333/333 [==============================] - 9s 28ms/step - loss: 0.1597 - auc: 0.7933 - val_loss: 0.1656 - val_auc: 0.8186\n",
      "Epoch 24/50\n",
      "333/333 [==============================] - 9s 28ms/step - loss: 0.1590 - auc: 0.7944 - val_loss: 0.1738 - val_auc: 0.7825\n",
      "Epoch 25/50\n",
      "333/333 [==============================] - 9s 28ms/step - loss: 0.1581 - auc: 0.7977 - val_loss: 0.1720 - val_auc: 0.7955\n",
      "Epoch 26/50\n",
      "333/333 [==============================] - 10s 29ms/step - loss: 0.1575 - auc: 0.8005 - val_loss: 0.1624 - val_auc: 0.8000\n",
      "Epoch 27/50\n",
      "333/333 [==============================] - 9s 28ms/step - loss: 0.1570 - auc: 0.8039 - val_loss: 0.1778 - val_auc: 0.7950\n",
      "Epoch 28/50\n",
      "333/333 [==============================] - 9s 28ms/step - loss: 0.1567 - auc: 0.8069 - val_loss: 0.1642 - val_auc: 0.8004\n",
      "Epoch 29/50\n",
      "333/333 [==============================] - 10s 30ms/step - loss: 0.1556 - auc: 0.8094 - val_loss: 0.1694 - val_auc: 0.8094\n",
      "Epoch 30/50\n",
      "333/333 [==============================] - 9s 28ms/step - loss: 0.1544 - auc: 0.8084 - val_loss: 0.1668 - val_auc: 0.8121\n",
      "Epoch 31/50\n",
      "333/333 [==============================] - 9s 28ms/step - loss: 0.1553 - auc: 0.8093 - val_loss: 0.1629 - val_auc: 0.8083\n",
      "Epoch 32/50\n",
      "333/333 [==============================] - 10s 29ms/step - loss: 0.1546 - auc: 0.8148 - val_loss: 0.1678 - val_auc: 0.8159\n",
      "Epoch 33/50\n",
      "333/333 [==============================] - 9s 29ms/step - loss: 0.1534 - auc: 0.8185 - val_loss: 0.1711 - val_auc: 0.8078\n",
      "Epoch 34/50\n",
      "333/333 [==============================] - 10s 29ms/step - loss: 0.1528 - auc: 0.8180 - val_loss: 0.1657 - val_auc: 0.8160\n",
      "Epoch 35/50\n",
      "333/333 [==============================] - 10s 29ms/step - loss: 0.1521 - auc: 0.8193 - val_loss: 0.1629 - val_auc: 0.8270\n",
      "Epoch 36/50\n",
      "333/333 [==============================] - 9s 28ms/step - loss: 0.1511 - auc: 0.8239 - val_loss: 0.1670 - val_auc: 0.8195\n",
      "Epoch 37/50\n",
      "333/333 [==============================] - 9s 28ms/step - loss: 0.1522 - auc: 0.8227 - val_loss: 0.1659 - val_auc: 0.8130\n",
      "Epoch 38/50\n",
      "333/333 [==============================] - 10s 29ms/step - loss: 0.1519 - auc: 0.8219 - val_loss: 0.1733 - val_auc: 0.8084\n",
      "Epoch 39/50\n",
      "333/333 [==============================] - 10s 29ms/step - loss: 0.1510 - auc: 0.8219 - val_loss: 0.1520 - val_auc: 0.8261\n",
      "Epoch 40/50\n",
      "333/333 [==============================] - 10s 29ms/step - loss: 0.1508 - auc: 0.8240 - val_loss: 0.1548 - val_auc: 0.8271\n",
      "Epoch 41/50\n",
      "333/333 [==============================] - 9s 29ms/step - loss: 0.1491 - auc: 0.8292 - val_loss: 0.1797 - val_auc: 0.8123\n",
      "Epoch 42/50\n",
      "333/333 [==============================] - 10s 29ms/step - loss: 0.1471 - auc: 0.8348 - val_loss: 0.1525 - val_auc: 0.8261\n",
      "Epoch 43/50\n",
      "333/333 [==============================] - 9s 28ms/step - loss: 0.1488 - auc: 0.8324 - val_loss: 0.1584 - val_auc: 0.8293\n",
      "Epoch 44/50\n",
      "333/333 [==============================] - 9s 27ms/step - loss: 0.1476 - auc: 0.8316 - val_loss: 0.1570 - val_auc: 0.8183\n",
      "Epoch 45/50\n",
      "333/333 [==============================] - 9s 28ms/step - loss: 0.1476 - auc: 0.8331 - val_loss: 0.1760 - val_auc: 0.8250\n",
      "Epoch 46/50\n",
      "333/333 [==============================] - 9s 28ms/step - loss: 0.1477 - auc: 0.8304 - val_loss: 0.1412 - val_auc: 0.8418\n",
      "Epoch 47/50\n",
      "333/333 [==============================] - 9s 28ms/step - loss: 0.1465 - auc: 0.8359 - val_loss: 0.1556 - val_auc: 0.8348\n",
      "Epoch 48/50\n",
      "333/333 [==============================] - 10s 30ms/step - loss: 0.1469 - auc: 0.8340 - val_loss: 0.1703 - val_auc: 0.8215\n",
      "Epoch 49/50\n",
      "333/333 [==============================] - 10s 29ms/step - loss: 0.1456 - auc: 0.8399 - val_loss: 0.1556 - val_auc: 0.8096\n",
      "Epoch 50/50\n",
      "333/333 [==============================] - 10s 29ms/step - loss: 0.1455 - auc: 0.8399 - val_loss: 0.1613 - val_auc: 0.8269\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x17d960f0e50>"
      ]
     },
     "execution_count": 435,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a function to build the model with GGNN message passing mechanism and default hyperparameters\n",
    "def build_model_ggnn():\n",
    "    # Model definition\n",
    "    data = Input(batch_shape=(None,))\n",
    "    edge = Input(batch_shape=(None, 2), dtype=tf.int32)\n",
    "    node2graph = Input(batch_shape=(None,), dtype=tf.int32)\n",
    "    embeded = Embedding(tokenizer.num_words, 20)(data)\n",
    "\n",
    "    num_graph = tf.reduce_max(node2graph) + 1\n",
    "\n",
    "    gnn_input = GNNInput(\n",
    "        node_features=embeded,\n",
    "        adjacency_lists=(edge,),\n",
    "        node_to_graph_map=node2graph,\n",
    "        num_graphs=num_graph,\n",
    "    )\n",
    "\n",
    "    params = GNN.get_default_hyperparameters()\n",
    "    params[\"message_calculation_class\"] = \"GGNN\"\n",
    "\n",
    "    gnn_layer = GNN(params)\n",
    "    gnn_out = gnn_layer(gnn_input)\n",
    "\n",
    "    avg = segment_mean(\n",
    "        data=gnn_out,\n",
    "        segment_ids=node2graph\n",
    "    )\n",
    "\n",
    "    pred = Dense(1, activation='sigmoid')(avg)\n",
    "\n",
    "    model = Model(\n",
    "        inputs={\n",
    "            'data': data,\n",
    "            'edges': edge,\n",
    "            'node2graph': node2graph,\n",
    "        },\n",
    "        outputs=pred\n",
    "    )\n",
    "\n",
    "    model.compile(\n",
    "        loss='binary_crossentropy',\n",
    "        optimizer=Adam(),\n",
    "        metrics=['AUC']\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "# Train the model\n",
    "ggnn_model = build_model_ggnn()\n",
    "ggnn_model.fit(\n",
    "    gen_batch(training_set, tokenizer, batch_size=batch_size, repeat=True),\n",
    "    steps_per_epoch=num_batches,\n",
    "    epochs=50,\n",
    "    validation_data=gen_batch(validation_set, tokenizer, batch_size=64, repeat=True),\n",
    "    validation_steps=num_batches_validation\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "193/193 [==============================] - 2s 11ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred7 =ggnn_model.predict(\n",
    "    gen_batch(testing_set,tokenizer, batch_size=64, shuffle=False)\n",
    ")\n",
    "y_pred7 = np.reshape(y_pred7, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC Score: 0.8325942754745483\n"
     ]
    }
   ],
   "source": [
    "# Calculate and print AUC score\n",
    "auc_score = ggnn_model.evaluate(\n",
    "    gen_batch(validation_set,tokenizer,batch_size=64, repeat=True),\n",
    "    steps=num_batches_validation,\n",
    "    verbose=0\n",
    ")[1]\n",
    "print(\"AUC Score:\", auc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({'label':y_pred7})\n",
    "submission.index.name = 'id'\n",
    "submission.to_csv('ggnn_model7.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trial 8: RGCN with Default Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Thoughts and Observations:\n",
    "- The RGCN message passing mechanism is used with default hyperparameters.\n",
    "- The model consists of 4 GNN message passing layers.\n",
    "- The model is trained for 80 epochs.\n",
    "- The AUC score achieved on the test set is 0.7939.\n",
    "- The performance of the model can be further improved by tuning hyperparameters \n",
    "- Trial 7 (GGNN) achieved a higher AUC score compared to Trial 8 (RGCN).\n",
    "- GGNN showed better performance with the same number of layers and default hyperparameters.\n",
    "- Further investigation is needed to understand why GGNN outperformed RGCN in this scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/80\n",
      "333/333 [==============================] - 10s 19ms/step - loss: 0.3445 - auc: 0.4023 - val_loss: 0.2780 - val_auc: 0.3296\n",
      "Epoch 2/80\n",
      "333/333 [==============================] - 6s 17ms/step - loss: 0.2250 - auc: 0.4262 - val_loss: 0.2202 - val_auc: 0.5980\n",
      "Epoch 3/80\n",
      "333/333 [==============================] - 6s 17ms/step - loss: 0.1917 - auc: 0.6111 - val_loss: 0.2092 - val_auc: 0.6721\n",
      "Epoch 4/80\n",
      "333/333 [==============================] - 6s 18ms/step - loss: 0.1872 - auc: 0.6366 - val_loss: 0.1984 - val_auc: 0.6983\n",
      "Epoch 5/80\n",
      "333/333 [==============================] - 6s 17ms/step - loss: 0.1846 - auc: 0.6491 - val_loss: 0.1882 - val_auc: 0.7194\n",
      "Epoch 6/80\n",
      "333/333 [==============================] - 6s 18ms/step - loss: 0.1817 - auc: 0.6783 - val_loss: 0.1886 - val_auc: 0.7174\n",
      "Epoch 7/80\n",
      "333/333 [==============================] - 6s 17ms/step - loss: 0.1810 - auc: 0.6793 - val_loss: 0.1999 - val_auc: 0.7134\n",
      "Epoch 8/80\n",
      "333/333 [==============================] - 6s 19ms/step - loss: 0.1802 - auc: 0.6816 - val_loss: 0.1886 - val_auc: 0.7442\n",
      "Epoch 9/80\n",
      "333/333 [==============================] - 6s 18ms/step - loss: 0.1784 - auc: 0.6996 - val_loss: 0.1952 - val_auc: 0.7295\n",
      "Epoch 10/80\n",
      "333/333 [==============================] - 6s 19ms/step - loss: 0.1772 - auc: 0.7067 - val_loss: 0.1758 - val_auc: 0.7492\n",
      "Epoch 11/80\n",
      "333/333 [==============================] - 6s 19ms/step - loss: 0.1772 - auc: 0.7020 - val_loss: 0.1998 - val_auc: 0.7036\n",
      "Epoch 12/80\n",
      "333/333 [==============================] - 6s 19ms/step - loss: 0.1758 - auc: 0.7108 - val_loss: 0.1862 - val_auc: 0.7209\n",
      "Epoch 13/80\n",
      "333/333 [==============================] - 6s 19ms/step - loss: 0.1760 - auc: 0.7132 - val_loss: 0.1890 - val_auc: 0.7338\n",
      "Epoch 14/80\n",
      "333/333 [==============================] - 6s 19ms/step - loss: 0.1756 - auc: 0.7106 - val_loss: 0.1827 - val_auc: 0.7522\n",
      "Epoch 15/80\n",
      "333/333 [==============================] - 6s 19ms/step - loss: 0.1752 - auc: 0.7184 - val_loss: 0.1892 - val_auc: 0.7327\n",
      "Epoch 16/80\n",
      "333/333 [==============================] - 6s 19ms/step - loss: 0.1743 - auc: 0.7217 - val_loss: 0.1841 - val_auc: 0.7599\n",
      "Epoch 17/80\n",
      "333/333 [==============================] - 6s 19ms/step - loss: 0.1725 - auc: 0.7348 - val_loss: 0.1767 - val_auc: 0.7390\n",
      "Epoch 18/80\n",
      "333/333 [==============================] - 6s 19ms/step - loss: 0.1728 - auc: 0.7288 - val_loss: 0.1805 - val_auc: 0.7600\n",
      "Epoch 19/80\n",
      "333/333 [==============================] - 6s 19ms/step - loss: 0.1711 - auc: 0.7433 - val_loss: 0.1844 - val_auc: 0.7471\n",
      "Epoch 20/80\n",
      "333/333 [==============================] - 6s 19ms/step - loss: 0.1724 - auc: 0.7369 - val_loss: 0.1797 - val_auc: 0.7625\n",
      "Epoch 21/80\n",
      "333/333 [==============================] - 6s 19ms/step - loss: 0.1710 - auc: 0.7419 - val_loss: 0.1843 - val_auc: 0.7466\n",
      "Epoch 22/80\n",
      "333/333 [==============================] - 6s 19ms/step - loss: 0.1702 - auc: 0.7463 - val_loss: 0.1774 - val_auc: 0.7616\n",
      "Epoch 23/80\n",
      "333/333 [==============================] - 6s 18ms/step - loss: 0.1721 - auc: 0.7393 - val_loss: 0.1905 - val_auc: 0.7564\n",
      "Epoch 24/80\n",
      "333/333 [==============================] - 6s 19ms/step - loss: 0.1702 - auc: 0.7473 - val_loss: 0.1765 - val_auc: 0.7467\n",
      "Epoch 25/80\n",
      "333/333 [==============================] - 6s 19ms/step - loss: 0.1702 - auc: 0.7433 - val_loss: 0.1861 - val_auc: 0.7498\n",
      "Epoch 26/80\n",
      "333/333 [==============================] - 7s 20ms/step - loss: 0.1694 - auc: 0.7529 - val_loss: 0.1746 - val_auc: 0.7658\n",
      "Epoch 27/80\n",
      "333/333 [==============================] - 6s 19ms/step - loss: 0.1692 - auc: 0.7499 - val_loss: 0.1847 - val_auc: 0.7722\n",
      "Epoch 28/80\n",
      "333/333 [==============================] - 6s 19ms/step - loss: 0.1685 - auc: 0.7494 - val_loss: 0.1820 - val_auc: 0.7529\n",
      "Epoch 29/80\n",
      "333/333 [==============================] - 6s 19ms/step - loss: 0.1703 - auc: 0.7432 - val_loss: 0.1791 - val_auc: 0.7723\n",
      "Epoch 30/80\n",
      "333/333 [==============================] - 6s 19ms/step - loss: 0.1679 - auc: 0.7611 - val_loss: 0.1768 - val_auc: 0.7687\n",
      "Epoch 31/80\n",
      "333/333 [==============================] - 6s 19ms/step - loss: 0.1690 - auc: 0.7535 - val_loss: 0.1821 - val_auc: 0.7656\n",
      "Epoch 32/80\n",
      "333/333 [==============================] - 6s 19ms/step - loss: 0.1681 - auc: 0.7548 - val_loss: 0.1798 - val_auc: 0.7768\n",
      "Epoch 33/80\n",
      "333/333 [==============================] - 6s 19ms/step - loss: 0.1697 - auc: 0.7488 - val_loss: 0.1761 - val_auc: 0.7783\n",
      "Epoch 34/80\n",
      "333/333 [==============================] - 7s 20ms/step - loss: 0.1679 - auc: 0.7558 - val_loss: 0.1772 - val_auc: 0.7856\n",
      "Epoch 35/80\n",
      "333/333 [==============================] - 6s 19ms/step - loss: 0.1671 - auc: 0.7605 - val_loss: 0.1728 - val_auc: 0.7864\n",
      "Epoch 36/80\n",
      "333/333 [==============================] - 6s 19ms/step - loss: 0.1672 - auc: 0.7608 - val_loss: 0.1757 - val_auc: 0.7793\n",
      "Epoch 37/80\n",
      "333/333 [==============================] - 6s 19ms/step - loss: 0.1677 - auc: 0.7603 - val_loss: 0.1839 - val_auc: 0.7825\n",
      "Epoch 38/80\n",
      "333/333 [==============================] - 6s 19ms/step - loss: 0.1670 - auc: 0.7650 - val_loss: 0.1791 - val_auc: 0.7780\n",
      "Epoch 39/80\n",
      "333/333 [==============================] - 7s 20ms/step - loss: 0.1656 - auc: 0.7721 - val_loss: 0.1778 - val_auc: 0.7883\n",
      "Epoch 40/80\n",
      "333/333 [==============================] - 6s 19ms/step - loss: 0.1651 - auc: 0.7722 - val_loss: 0.1747 - val_auc: 0.7780\n",
      "Epoch 41/80\n",
      "333/333 [==============================] - 6s 19ms/step - loss: 0.1670 - auc: 0.7674 - val_loss: 0.1791 - val_auc: 0.7850\n",
      "Epoch 42/80\n",
      "333/333 [==============================] - 6s 19ms/step - loss: 0.1656 - auc: 0.7682 - val_loss: 0.1719 - val_auc: 0.7866\n",
      "Epoch 43/80\n",
      "333/333 [==============================] - 6s 19ms/step - loss: 0.1671 - auc: 0.7670 - val_loss: 0.1736 - val_auc: 0.7701\n",
      "Epoch 44/80\n",
      "333/333 [==============================] - 6s 19ms/step - loss: 0.1652 - auc: 0.7715 - val_loss: 0.1852 - val_auc: 0.7554\n",
      "Epoch 45/80\n",
      "333/333 [==============================] - 6s 19ms/step - loss: 0.1659 - auc: 0.7667 - val_loss: 0.1724 - val_auc: 0.8221\n",
      "Epoch 46/80\n",
      "333/333 [==============================] - 6s 19ms/step - loss: 0.1656 - auc: 0.7671 - val_loss: 0.1819 - val_auc: 0.7561\n",
      "Epoch 47/80\n",
      "333/333 [==============================] - 7s 20ms/step - loss: 0.1655 - auc: 0.7708 - val_loss: 0.1773 - val_auc: 0.7971\n",
      "Epoch 48/80\n",
      "333/333 [==============================] - 7s 20ms/step - loss: 0.1642 - auc: 0.7802 - val_loss: 0.1769 - val_auc: 0.7863\n",
      "Epoch 49/80\n",
      "333/333 [==============================] - 6s 19ms/step - loss: 0.1641 - auc: 0.7721 - val_loss: 0.1752 - val_auc: 0.7862\n",
      "Epoch 50/80\n",
      "333/333 [==============================] - 6s 19ms/step - loss: 0.1658 - auc: 0.7687 - val_loss: 0.1753 - val_auc: 0.7961\n",
      "Epoch 51/80\n",
      "333/333 [==============================] - 6s 19ms/step - loss: 0.1638 - auc: 0.7762 - val_loss: 0.1732 - val_auc: 0.7885\n",
      "Epoch 52/80\n",
      "333/333 [==============================] - 6s 19ms/step - loss: 0.1653 - auc: 0.7702 - val_loss: 0.1752 - val_auc: 0.7754\n",
      "Epoch 53/80\n",
      "333/333 [==============================] - 6s 19ms/step - loss: 0.1636 - auc: 0.7769 - val_loss: 0.1834 - val_auc: 0.7888\n",
      "Epoch 54/80\n",
      "333/333 [==============================] - 6s 19ms/step - loss: 0.1639 - auc: 0.7745 - val_loss: 0.1808 - val_auc: 0.7776\n",
      "Epoch 55/80\n",
      "333/333 [==============================] - 6s 19ms/step - loss: 0.1647 - auc: 0.7737 - val_loss: 0.1718 - val_auc: 0.7989\n",
      "Epoch 56/80\n",
      "333/333 [==============================] - 6s 19ms/step - loss: 0.1633 - auc: 0.7776 - val_loss: 0.1838 - val_auc: 0.7720\n",
      "Epoch 57/80\n",
      "333/333 [==============================] - 6s 19ms/step - loss: 0.1645 - auc: 0.7762 - val_loss: 0.1743 - val_auc: 0.7930\n",
      "Epoch 58/80\n",
      "333/333 [==============================] - 6s 19ms/step - loss: 0.1628 - auc: 0.7848 - val_loss: 0.1750 - val_auc: 0.7930\n",
      "Epoch 59/80\n",
      "333/333 [==============================] - 7s 20ms/step - loss: 0.1638 - auc: 0.7825 - val_loss: 0.1750 - val_auc: 0.7898\n",
      "Epoch 60/80\n",
      "333/333 [==============================] - 7s 20ms/step - loss: 0.1647 - auc: 0.7734 - val_loss: 0.1780 - val_auc: 0.7946\n",
      "Epoch 61/80\n",
      "333/333 [==============================] - 6s 19ms/step - loss: 0.1628 - auc: 0.7844 - val_loss: 0.1739 - val_auc: 0.7836\n",
      "Epoch 62/80\n",
      "333/333 [==============================] - 6s 18ms/step - loss: 0.1625 - auc: 0.7850 - val_loss: 0.1737 - val_auc: 0.8072\n",
      "Epoch 63/80\n",
      "333/333 [==============================] - 6s 18ms/step - loss: 0.1629 - auc: 0.7805 - val_loss: 0.1731 - val_auc: 0.7827\n",
      "Epoch 64/80\n",
      "333/333 [==============================] - 6s 18ms/step - loss: 0.1622 - auc: 0.7874 - val_loss: 0.1727 - val_auc: 0.8029\n",
      "Epoch 65/80\n",
      "333/333 [==============================] - 6s 18ms/step - loss: 0.1629 - auc: 0.7853 - val_loss: 0.1805 - val_auc: 0.7855\n",
      "Epoch 66/80\n",
      "333/333 [==============================] - 6s 18ms/step - loss: 0.1614 - auc: 0.7908 - val_loss: 0.1813 - val_auc: 0.7999\n",
      "Epoch 67/80\n",
      "333/333 [==============================] - 6s 18ms/step - loss: 0.1633 - auc: 0.7816 - val_loss: 0.1762 - val_auc: 0.7933\n",
      "Epoch 68/80\n",
      "333/333 [==============================] - 6s 18ms/step - loss: 0.1630 - auc: 0.7811 - val_loss: 0.1623 - val_auc: 0.8095\n",
      "Epoch 69/80\n",
      "333/333 [==============================] - 6s 18ms/step - loss: 0.1628 - auc: 0.7854 - val_loss: 0.1791 - val_auc: 0.7981\n",
      "Epoch 70/80\n",
      "333/333 [==============================] - 6s 19ms/step - loss: 0.1614 - auc: 0.7882 - val_loss: 0.1698 - val_auc: 0.7882\n",
      "Epoch 71/80\n",
      "333/333 [==============================] - 6s 18ms/step - loss: 0.1616 - auc: 0.7887 - val_loss: 0.1689 - val_auc: 0.8091\n",
      "Epoch 72/80\n",
      "333/333 [==============================] - 6s 18ms/step - loss: 0.1624 - auc: 0.7873 - val_loss: 0.1798 - val_auc: 0.7880\n",
      "Epoch 73/80\n",
      "333/333 [==============================] - 6s 18ms/step - loss: 0.1607 - auc: 0.7906 - val_loss: 0.1836 - val_auc: 0.8020\n",
      "Epoch 74/80\n",
      "333/333 [==============================] - 6s 18ms/step - loss: 0.1607 - auc: 0.7936 - val_loss: 0.1697 - val_auc: 0.8076\n",
      "Epoch 75/80\n",
      "333/333 [==============================] - 6s 18ms/step - loss: 0.1614 - auc: 0.7877 - val_loss: 0.1805 - val_auc: 0.8104\n",
      "Epoch 76/80\n",
      "333/333 [==============================] - 7s 22ms/step - loss: 0.1623 - auc: 0.7856 - val_loss: 0.1725 - val_auc: 0.7885\n",
      "Epoch 77/80\n",
      "333/333 [==============================] - 7s 20ms/step - loss: 0.1620 - auc: 0.7895 - val_loss: 0.1678 - val_auc: 0.8158\n",
      "Epoch 78/80\n",
      "333/333 [==============================] - 7s 19ms/step - loss: 0.1611 - auc: 0.7913 - val_loss: 0.1799 - val_auc: 0.7828\n",
      "Epoch 79/80\n",
      "333/333 [==============================] - 6s 19ms/step - loss: 0.1611 - auc: 0.7933 - val_loss: 0.1805 - val_auc: 0.8025\n",
      "Epoch 80/80\n",
      "333/333 [==============================] - 6s 19ms/step - loss: 0.1601 - auc: 0.7927 - val_loss: 0.1705 - val_auc: 0.7955\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x17da4f766d0>"
      ]
     },
     "execution_count": 439,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def build_model_rgcn(num_layers):\n",
    "    # Model definition\n",
    "    data = Input(batch_shape=(None,))\n",
    "    edge = Input(batch_shape=(None, 2), dtype=tf.int32)\n",
    "    node2graph = Input(batch_shape=(None,), dtype=tf.int32)\n",
    "    embeded = Embedding(tokenizer.num_words, 20)(data)\n",
    "\n",
    "    num_graph = tf.reduce_max(node2graph) + 1\n",
    "\n",
    "    gnn_input = GNNInput(\n",
    "        node_features=embeded,\n",
    "        adjacency_lists=(edge,),\n",
    "        node_to_graph_map=node2graph,\n",
    "        num_graphs=num_graph,\n",
    "    )\n",
    "\n",
    "    params = GNN.get_default_hyperparameters()\n",
    "    params[\"num_layers\"] = num_layers\n",
    "    params[\"message_calculation_class\"] = \"RGCN\"\n",
    "\n",
    "    gnn_layer = GNN(params)\n",
    "    gnn_out = gnn_layer(gnn_input)\n",
    "\n",
    "    avg = segment_mean(\n",
    "        data=gnn_out,\n",
    "        segment_ids=node2graph\n",
    "    )\n",
    "\n",
    "    pred = Dense(1, activation='sigmoid')(avg)\n",
    "\n",
    "    model = Model(\n",
    "        inputs={\n",
    "            'data': data,\n",
    "            'edges': edge,\n",
    "            'node2graph': node2graph,\n",
    "        },\n",
    "        outputs=pred\n",
    "    )\n",
    "\n",
    "    model.compile(\n",
    "        loss='binary_crossentropy',\n",
    "        optimizer=Adam(),\n",
    "        metrics=['AUC']\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "# Define hyperparameters\n",
    "num_layers = 4\n",
    "\n",
    "# Build and train the model\n",
    "trial8_model = build_model_rgcn(num_layers)\n",
    "trial8_model.fit(\n",
    "    gen_batch(training_set, tokenizer, batch_size=batch_size, repeat=True),\n",
    "    steps_per_epoch=num_batches,\n",
    "    epochs=80,\n",
    "    validation_data=gen_batch(validation_set, tokenizer, batch_size=64, repeat=True),\n",
    "    validation_steps=num_batches_validation\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\smart fix\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "193/193 [==============================] - 2s 6ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.050788238644599915, 0.0)"
      ]
     },
     "execution_count": 440,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "trial8_loss, trial8_auc = trial8_model.evaluate(gen_batch(testing_set, tokenizer, batch_size=64, shuffle=False), verbose=0)\n",
    "\n",
    "# Save the model and predictions\n",
    "trial8_model.save(\"trial8_model.h5\")\n",
    "y_pred_trial8 = trial8_model.predict(gen_batch(testing_set, tokenizer, batch_size=64, shuffle=False))\n",
    "submission_trial8 = pd.DataFrame({'label': y_pred_trial8.reshape(-1)})\n",
    "submission_trial8.index.name = 'id'\n",
    "submission_trial8.to_csv('RGCN_model8.csv')\n",
    "\n",
    "trial8_loss, trial8_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC Score: 0.7938568592071533\n"
     ]
    }
   ],
   "source": [
    "# Calculate and print AUC score\n",
    "auc_score =trial8_model.evaluate(\n",
    "    gen_batch(validation_set,tokenizer,batch_size=64, repeat=True),\n",
    "    steps=num_batches_validation,\n",
    "    verbose=0\n",
    ")[1]\n",
    "print(\"AUC Score:\", auc_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trial 9: RGAT with Default Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thoughts and Observations:\n",
    "- RGAT (Relational Graph Attention Network) introduces attention mechanisms into the message passing process, allowing the model to focus on more relevant information during aggregation.\n",
    "- The model is trained for 80 epochs, which is longer than previous trials, allowing more time for learning.\n",
    "- It remains to be seen if the RGAT mechanism will improve performance compared to previous trials using different aggregation mechanisms.\n",
    "- RGAT message passing mechanism is used with default hyperparameters.\n",
    "- The model consists of 3 GNN message passing layers.\n",
    "- The default hyperparameters are used for other configurations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations\n",
    "- The AUC score of Trial 9 is higher than both Trial 7 and Trial 8, indicating that the RGAT model performs better than GGNN and RGCN on this dataset.\n",
    "- RGAT (Relational Graph Attention Networks) is a message-passing neural network architecture that utilizes attention mechanisms to learn edge weights dynamically. It enables nodes to selectively aggregate information from their neighbors, focusing on the most relevant nodes during message passing.\n",
    "- The RGAT model with default hyperparameters and 120 epochs shows promising results, indicating that attention mechanisms can effectively capture the relationships between nodes in the graph data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/120\n",
      "333/333 [==============================] - 25s 63ms/step - loss: 0.2308 - auc: 0.4974 - val_loss: 0.1957 - val_auc: 0.6654\n",
      "Epoch 2/120\n",
      "333/333 [==============================] - 22s 67ms/step - loss: 0.1831 - auc: 0.6607 - val_loss: 0.1947 - val_auc: 0.7025\n",
      "Epoch 3/120\n",
      "333/333 [==============================] - 23s 70ms/step - loss: 0.1792 - auc: 0.6940 - val_loss: 0.1838 - val_auc: 0.7064\n",
      "Epoch 4/120\n",
      "333/333 [==============================] - 23s 69ms/step - loss: 0.1786 - auc: 0.6905 - val_loss: 0.1856 - val_auc: 0.7383\n",
      "Epoch 5/120\n",
      "333/333 [==============================] - 23s 70ms/step - loss: 0.1789 - auc: 0.6983 - val_loss: 0.1868 - val_auc: 0.7225\n",
      "Epoch 6/120\n",
      "333/333 [==============================] - 24s 71ms/step - loss: 0.1758 - auc: 0.7073 - val_loss: 0.1786 - val_auc: 0.7744\n",
      "Epoch 7/120\n",
      "333/333 [==============================] - 27s 81ms/step - loss: 0.1764 - auc: 0.7077 - val_loss: 0.1866 - val_auc: 0.7342\n",
      "Epoch 8/120\n",
      "333/333 [==============================] - 24s 72ms/step - loss: 0.1750 - auc: 0.7150 - val_loss: 0.1871 - val_auc: 0.7632\n",
      "Epoch 9/120\n",
      "333/333 [==============================] - 24s 72ms/step - loss: 0.1737 - auc: 0.7244 - val_loss: 0.1841 - val_auc: 0.7619\n",
      "Epoch 10/120\n",
      "333/333 [==============================] - 29s 87ms/step - loss: 0.1707 - auc: 0.7418 - val_loss: 0.1964 - val_auc: 0.7457\n",
      "Epoch 11/120\n",
      "333/333 [==============================] - 24s 72ms/step - loss: 0.1704 - auc: 0.7466 - val_loss: 0.1811 - val_auc: 0.7782\n",
      "Epoch 12/120\n",
      "333/333 [==============================] - 25s 74ms/step - loss: 0.1665 - auc: 0.7667 - val_loss: 0.1679 - val_auc: 0.7912\n",
      "Epoch 13/120\n",
      "333/333 [==============================] - 24s 71ms/step - loss: 0.1671 - auc: 0.7582 - val_loss: 0.1751 - val_auc: 0.7742\n",
      "Epoch 14/120\n",
      "333/333 [==============================] - 22s 67ms/step - loss: 0.1631 - auc: 0.7772 - val_loss: 0.1764 - val_auc: 0.7642\n",
      "Epoch 15/120\n",
      "333/333 [==============================] - 23s 70ms/step - loss: 0.1628 - auc: 0.7793 - val_loss: 0.1803 - val_auc: 0.8055\n",
      "Epoch 16/120\n",
      "333/333 [==============================] - 22s 67ms/step - loss: 0.1616 - auc: 0.7817 - val_loss: 0.1705 - val_auc: 0.8035\n",
      "Epoch 17/120\n",
      "333/333 [==============================] - 21s 64ms/step - loss: 0.1612 - auc: 0.7876 - val_loss: 0.1712 - val_auc: 0.7914\n",
      "Epoch 18/120\n",
      "333/333 [==============================] - 24s 73ms/step - loss: 0.1604 - auc: 0.7888 - val_loss: 0.1616 - val_auc: 0.8065\n",
      "Epoch 19/120\n",
      "333/333 [==============================] - 23s 70ms/step - loss: 0.1588 - auc: 0.7936 - val_loss: 0.1741 - val_auc: 0.8124\n",
      "Epoch 20/120\n",
      "333/333 [==============================] - 23s 69ms/step - loss: 0.1585 - auc: 0.7972 - val_loss: 0.1618 - val_auc: 0.8267\n",
      "Epoch 21/120\n",
      "333/333 [==============================] - 23s 68ms/step - loss: 0.1591 - auc: 0.7923 - val_loss: 0.1749 - val_auc: 0.7908\n",
      "Epoch 22/120\n",
      "333/333 [==============================] - 23s 68ms/step - loss: 0.1571 - auc: 0.8003 - val_loss: 0.1666 - val_auc: 0.8140\n",
      "Epoch 23/120\n",
      "333/333 [==============================] - 23s 70ms/step - loss: 0.1575 - auc: 0.7986 - val_loss: 0.1687 - val_auc: 0.8014\n",
      "Epoch 24/120\n",
      "333/333 [==============================] - 23s 69ms/step - loss: 0.1567 - auc: 0.8033 - val_loss: 0.1564 - val_auc: 0.8186\n",
      "Epoch 25/120\n",
      "333/333 [==============================] - 23s 68ms/step - loss: 0.1566 - auc: 0.8062 - val_loss: 0.1743 - val_auc: 0.8144\n",
      "Epoch 26/120\n",
      "333/333 [==============================] - 22s 67ms/step - loss: 0.1554 - auc: 0.8047 - val_loss: 0.1611 - val_auc: 0.8090\n",
      "Epoch 27/120\n",
      "333/333 [==============================] - 23s 69ms/step - loss: 0.1562 - auc: 0.7999 - val_loss: 0.1633 - val_auc: 0.8147\n",
      "Epoch 28/120\n",
      "333/333 [==============================] - 23s 68ms/step - loss: 0.1548 - auc: 0.8095 - val_loss: 0.1678 - val_auc: 0.8225\n",
      "Epoch 29/120\n",
      "333/333 [==============================] - 23s 69ms/step - loss: 0.1534 - auc: 0.8106 - val_loss: 0.1624 - val_auc: 0.8232\n",
      "Epoch 30/120\n",
      "333/333 [==============================] - 22s 67ms/step - loss: 0.1535 - auc: 0.8149 - val_loss: 0.1614 - val_auc: 0.8354\n",
      "Epoch 31/120\n",
      "333/333 [==============================] - 22s 67ms/step - loss: 0.1513 - auc: 0.8176 - val_loss: 0.1629 - val_auc: 0.8262\n",
      "Epoch 32/120\n",
      "333/333 [==============================] - 22s 66ms/step - loss: 0.1516 - auc: 0.8193 - val_loss: 0.1655 - val_auc: 0.8199\n",
      "Epoch 33/120\n",
      "333/333 [==============================] - 22s 67ms/step - loss: 0.1504 - auc: 0.8231 - val_loss: 0.1640 - val_auc: 0.8269\n",
      "Epoch 34/120\n",
      "333/333 [==============================] - 22s 66ms/step - loss: 0.1509 - auc: 0.8216 - val_loss: 0.1559 - val_auc: 0.8302\n",
      "Epoch 35/120\n",
      "333/333 [==============================] - 22s 67ms/step - loss: 0.1499 - auc: 0.8215 - val_loss: 0.1620 - val_auc: 0.8308\n",
      "Epoch 36/120\n",
      "333/333 [==============================] - 22s 67ms/step - loss: 0.1494 - auc: 0.8218 - val_loss: 0.1654 - val_auc: 0.8180\n",
      "Epoch 37/120\n",
      "333/333 [==============================] - 22s 67ms/step - loss: 0.1495 - auc: 0.8246 - val_loss: 0.1602 - val_auc: 0.8194\n",
      "Epoch 38/120\n",
      "333/333 [==============================] - 23s 68ms/step - loss: 0.1484 - auc: 0.8228 - val_loss: 0.1706 - val_auc: 0.8327\n",
      "Epoch 39/120\n",
      "333/333 [==============================] - 24s 73ms/step - loss: 0.1486 - auc: 0.8265 - val_loss: 0.1505 - val_auc: 0.8348\n",
      "Epoch 40/120\n",
      "333/333 [==============================] - 22s 67ms/step - loss: 0.1481 - auc: 0.8298 - val_loss: 0.1607 - val_auc: 0.8329\n",
      "Epoch 41/120\n",
      "333/333 [==============================] - 22s 67ms/step - loss: 0.1475 - auc: 0.8326 - val_loss: 0.1658 - val_auc: 0.8148\n",
      "Epoch 42/120\n",
      "333/333 [==============================] - 24s 73ms/step - loss: 0.1499 - auc: 0.8252 - val_loss: 0.1603 - val_auc: 0.8241\n",
      "Epoch 43/120\n",
      "333/333 [==============================] - 24s 72ms/step - loss: 0.1491 - auc: 0.8259 - val_loss: 0.1589 - val_auc: 0.8238\n",
      "Epoch 44/120\n",
      "333/333 [==============================] - 24s 71ms/step - loss: 0.1461 - auc: 0.8338 - val_loss: 0.1654 - val_auc: 0.8345\n",
      "Epoch 45/120\n",
      "333/333 [==============================] - 23s 69ms/step - loss: 0.1459 - auc: 0.8344 - val_loss: 0.1479 - val_auc: 0.8400\n",
      "Epoch 46/120\n",
      "333/333 [==============================] - 23s 68ms/step - loss: 0.1459 - auc: 0.8342 - val_loss: 0.1772 - val_auc: 0.8245\n",
      "Epoch 47/120\n",
      "333/333 [==============================] - 23s 68ms/step - loss: 0.1455 - auc: 0.8381 - val_loss: 0.1500 - val_auc: 0.8080\n",
      "Epoch 48/120\n",
      "333/333 [==============================] - 22s 67ms/step - loss: 0.1453 - auc: 0.8390 - val_loss: 0.1533 - val_auc: 0.8503\n",
      "Epoch 49/120\n",
      "333/333 [==============================] - 23s 68ms/step - loss: 0.1486 - auc: 0.8324 - val_loss: 0.1532 - val_auc: 0.8274\n",
      "Epoch 50/120\n",
      "333/333 [==============================] - 22s 67ms/step - loss: 0.1457 - auc: 0.8355 - val_loss: 0.1545 - val_auc: 0.8417\n",
      "Epoch 51/120\n",
      "333/333 [==============================] - 23s 68ms/step - loss: 0.1450 - auc: 0.8364 - val_loss: 0.1622 - val_auc: 0.8294\n",
      "Epoch 52/120\n",
      "333/333 [==============================] - 22s 68ms/step - loss: 0.1438 - auc: 0.8420 - val_loss: 0.1584 - val_auc: 0.8328\n",
      "Epoch 53/120\n",
      "333/333 [==============================] - 23s 69ms/step - loss: 0.1449 - auc: 0.8392 - val_loss: 0.1499 - val_auc: 0.8316\n",
      "Epoch 54/120\n",
      "333/333 [==============================] - 22s 67ms/step - loss: 0.1423 - auc: 0.8489 - val_loss: 0.1649 - val_auc: 0.8412\n",
      "Epoch 55/120\n",
      "333/333 [==============================] - 22s 65ms/step - loss: 0.1411 - auc: 0.8510 - val_loss: 0.1571 - val_auc: 0.8323\n",
      "Epoch 56/120\n",
      "333/333 [==============================] - 21s 63ms/step - loss: 0.1428 - auc: 0.8441 - val_loss: 0.1616 - val_auc: 0.8411\n",
      "Epoch 57/120\n",
      "333/333 [==============================] - 20s 61ms/step - loss: 0.1433 - auc: 0.8463 - val_loss: 0.1493 - val_auc: 0.8585\n",
      "Epoch 58/120\n",
      "333/333 [==============================] - 22s 66ms/step - loss: 0.1402 - auc: 0.8528 - val_loss: 0.1680 - val_auc: 0.8184\n",
      "Epoch 59/120\n",
      "333/333 [==============================] - 22s 67ms/step - loss: 0.1426 - auc: 0.8466 - val_loss: 0.1526 - val_auc: 0.8328\n",
      "Epoch 60/120\n",
      "333/333 [==============================] - 22s 67ms/step - loss: 0.1401 - auc: 0.8554 - val_loss: 0.1552 - val_auc: 0.8358\n",
      "Epoch 61/120\n",
      "333/333 [==============================] - 21s 64ms/step - loss: 0.1414 - auc: 0.8473 - val_loss: 0.1559 - val_auc: 0.8410\n",
      "Epoch 62/120\n",
      "333/333 [==============================] - 21s 64ms/step - loss: 0.1403 - auc: 0.8521 - val_loss: 0.1501 - val_auc: 0.8385\n",
      "Epoch 63/120\n",
      "333/333 [==============================] - 22s 65ms/step - loss: 0.1397 - auc: 0.8513 - val_loss: 0.1606 - val_auc: 0.8415\n",
      "Epoch 64/120\n",
      "333/333 [==============================] - 22s 65ms/step - loss: 0.1382 - auc: 0.8581 - val_loss: 0.1601 - val_auc: 0.8334\n",
      "Epoch 65/120\n",
      "333/333 [==============================] - 21s 65ms/step - loss: 0.1386 - auc: 0.8551 - val_loss: 0.1528 - val_auc: 0.8360\n",
      "Epoch 66/120\n",
      "333/333 [==============================] - 22s 65ms/step - loss: 0.1389 - auc: 0.8544 - val_loss: 0.1602 - val_auc: 0.8321\n",
      "Epoch 67/120\n",
      "333/333 [==============================] - 21s 64ms/step - loss: 0.1374 - auc: 0.8597 - val_loss: 0.1534 - val_auc: 0.8397\n",
      "Epoch 68/120\n",
      "333/333 [==============================] - 22s 65ms/step - loss: 0.1391 - auc: 0.8581 - val_loss: 0.1458 - val_auc: 0.8392\n",
      "Epoch 69/120\n",
      "333/333 [==============================] - 22s 65ms/step - loss: 0.1377 - auc: 0.8595 - val_loss: 0.1584 - val_auc: 0.8401\n",
      "Epoch 70/120\n",
      "333/333 [==============================] - 22s 65ms/step - loss: 0.1376 - auc: 0.8584 - val_loss: 0.1556 - val_auc: 0.8279\n",
      "Epoch 71/120\n",
      "333/333 [==============================] - 22s 66ms/step - loss: 0.1367 - auc: 0.8623 - val_loss: 0.1485 - val_auc: 0.8299\n",
      "Epoch 72/120\n",
      "333/333 [==============================] - 22s 65ms/step - loss: 0.1369 - auc: 0.8620 - val_loss: 0.1515 - val_auc: 0.8481\n",
      "Epoch 73/120\n",
      "333/333 [==============================] - 21s 64ms/step - loss: 0.1359 - auc: 0.8620 - val_loss: 0.1532 - val_auc: 0.8349\n",
      "Epoch 74/120\n",
      "333/333 [==============================] - 25s 76ms/step - loss: 0.1340 - auc: 0.8694 - val_loss: 0.1458 - val_auc: 0.8482\n",
      "Epoch 75/120\n",
      "333/333 [==============================] - 21s 64ms/step - loss: 0.1338 - auc: 0.8699 - val_loss: 0.1605 - val_auc: 0.8284\n",
      "Epoch 76/120\n",
      "333/333 [==============================] - 25s 74ms/step - loss: 0.1334 - auc: 0.8670 - val_loss: 0.1439 - val_auc: 0.8617\n",
      "Epoch 77/120\n",
      "333/333 [==============================] - 25s 75ms/step - loss: 0.1352 - auc: 0.8683 - val_loss: 0.1580 - val_auc: 0.8309\n",
      "Epoch 78/120\n",
      "333/333 [==============================] - 20s 59ms/step - loss: 0.1333 - auc: 0.8708 - val_loss: 0.1371 - val_auc: 0.8762\n",
      "Epoch 79/120\n",
      "333/333 [==============================] - 20s 60ms/step - loss: 0.1339 - auc: 0.8688 - val_loss: 0.1573 - val_auc: 0.8221\n",
      "Epoch 80/120\n",
      "333/333 [==============================] - 23s 68ms/step - loss: 0.1354 - auc: 0.8644 - val_loss: 0.1543 - val_auc: 0.8439\n",
      "Epoch 81/120\n",
      "333/333 [==============================] - 22s 67ms/step - loss: 0.1327 - auc: 0.8762 - val_loss: 0.1464 - val_auc: 0.8417\n",
      "Epoch 82/120\n",
      "333/333 [==============================] - 22s 65ms/step - loss: 0.1337 - auc: 0.8709 - val_loss: 0.1533 - val_auc: 0.8505\n",
      "Epoch 83/120\n",
      "333/333 [==============================] - 25s 76ms/step - loss: 0.1330 - auc: 0.8715 - val_loss: 0.1443 - val_auc: 0.8638\n",
      "Epoch 84/120\n",
      "333/333 [==============================] - 21s 62ms/step - loss: 0.1331 - auc: 0.8734 - val_loss: 0.1525 - val_auc: 0.8464\n",
      "Epoch 85/120\n",
      "333/333 [==============================] - 22s 65ms/step - loss: 0.1311 - auc: 0.8776 - val_loss: 0.1533 - val_auc: 0.8456\n",
      "Epoch 86/120\n",
      "333/333 [==============================] - 26s 77ms/step - loss: 0.1306 - auc: 0.8776 - val_loss: 0.1489 - val_auc: 0.8509\n",
      "Epoch 87/120\n",
      "333/333 [==============================] - 21s 62ms/step - loss: 0.1315 - auc: 0.8728 - val_loss: 0.1539 - val_auc: 0.8514\n",
      "Epoch 88/120\n",
      "333/333 [==============================] - 21s 62ms/step - loss: 0.1291 - auc: 0.8818 - val_loss: 0.1477 - val_auc: 0.8485\n",
      "Epoch 89/120\n",
      "333/333 [==============================] - 22s 65ms/step - loss: 0.1315 - auc: 0.8752 - val_loss: 0.1497 - val_auc: 0.8475\n",
      "Epoch 90/120\n",
      "333/333 [==============================] - 22s 67ms/step - loss: 0.1288 - auc: 0.8831 - val_loss: 0.1484 - val_auc: 0.8588\n",
      "Epoch 91/120\n",
      "333/333 [==============================] - 23s 68ms/step - loss: 0.1306 - auc: 0.8791 - val_loss: 0.1485 - val_auc: 0.8530\n",
      "Epoch 92/120\n",
      "333/333 [==============================] - 23s 68ms/step - loss: 0.1300 - auc: 0.8753 - val_loss: 0.1524 - val_auc: 0.8483\n",
      "Epoch 93/120\n",
      "333/333 [==============================] - 22s 66ms/step - loss: 0.1270 - auc: 0.8877 - val_loss: 0.1513 - val_auc: 0.8479\n",
      "Epoch 94/120\n",
      "333/333 [==============================] - 23s 68ms/step - loss: 0.1283 - auc: 0.8813 - val_loss: 0.1570 - val_auc: 0.8330\n",
      "Epoch 95/120\n",
      "333/333 [==============================] - 22s 67ms/step - loss: 0.1297 - auc: 0.8771 - val_loss: 0.1590 - val_auc: 0.8498\n",
      "Epoch 96/120\n",
      "333/333 [==============================] - 22s 67ms/step - loss: 0.1258 - auc: 0.8849 - val_loss: 0.1519 - val_auc: 0.8181\n",
      "Epoch 97/120\n",
      "333/333 [==============================] - 23s 68ms/step - loss: 0.1271 - auc: 0.8857 - val_loss: 0.1547 - val_auc: 0.8484\n",
      "Epoch 98/120\n",
      "333/333 [==============================] - 22s 68ms/step - loss: 0.1278 - auc: 0.8831 - val_loss: 0.1485 - val_auc: 0.8487\n",
      "Epoch 99/120\n",
      "333/333 [==============================] - 22s 67ms/step - loss: 0.1248 - auc: 0.8927 - val_loss: 0.1503 - val_auc: 0.8418\n",
      "Epoch 100/120\n",
      "333/333 [==============================] - 23s 70ms/step - loss: 0.1240 - auc: 0.8915 - val_loss: 0.1442 - val_auc: 0.8666\n",
      "Epoch 101/120\n",
      "333/333 [==============================] - 45s 136ms/step - loss: 0.1240 - auc: 0.8919 - val_loss: 0.1590 - val_auc: 0.8411\n",
      "Epoch 102/120\n",
      "333/333 [==============================] - 4558s 14s/step - loss: 0.1245 - auc: 0.8902 - val_loss: 0.1495 - val_auc: 0.8289\n",
      "Epoch 103/120\n",
      "333/333 [==============================] - 22s 67ms/step - loss: 0.1243 - auc: 0.8902 - val_loss: 0.1614 - val_auc: 0.8378\n",
      "Epoch 104/120\n",
      "333/333 [==============================] - 22s 65ms/step - loss: 0.1246 - auc: 0.8910 - val_loss: 0.1414 - val_auc: 0.8576\n",
      "Epoch 105/120\n",
      "333/333 [==============================] - 24s 74ms/step - loss: 0.1229 - auc: 0.8923 - val_loss: 0.1709 - val_auc: 0.8404\n",
      "Epoch 106/120\n",
      "333/333 [==============================] - 24s 71ms/step - loss: 0.1238 - auc: 0.8902 - val_loss: 0.1476 - val_auc: 0.8255\n",
      "Epoch 107/120\n",
      "333/333 [==============================] - 27s 80ms/step - loss: 0.1241 - auc: 0.8943 - val_loss: 0.1487 - val_auc: 0.8608\n",
      "Epoch 108/120\n",
      "333/333 [==============================] - 23s 68ms/step - loss: 0.1239 - auc: 0.8924 - val_loss: 0.1577 - val_auc: 0.8294\n",
      "Epoch 109/120\n",
      "333/333 [==============================] - 22s 65ms/step - loss: 0.1247 - auc: 0.8902 - val_loss: 0.1470 - val_auc: 0.8694\n",
      "Epoch 110/120\n",
      "333/333 [==============================] - 23s 70ms/step - loss: 0.1210 - auc: 0.8976 - val_loss: 0.1572 - val_auc: 0.8361\n",
      "Epoch 111/120\n",
      "333/333 [==============================] - 23s 69ms/step - loss: 0.1204 - auc: 0.8971 - val_loss: 0.1460 - val_auc: 0.8586\n",
      "Epoch 112/120\n",
      "333/333 [==============================] - 22s 67ms/step - loss: 0.1214 - auc: 0.8983 - val_loss: 0.1349 - val_auc: 0.8723\n",
      "Epoch 113/120\n",
      "333/333 [==============================] - 23s 69ms/step - loss: 0.1189 - auc: 0.9003 - val_loss: 0.1580 - val_auc: 0.8323\n",
      "Epoch 114/120\n",
      "333/333 [==============================] - 23s 69ms/step - loss: 0.1197 - auc: 0.8988 - val_loss: 0.1582 - val_auc: 0.8320\n",
      "Epoch 115/120\n",
      "333/333 [==============================] - 25s 76ms/step - loss: 0.1201 - auc: 0.9024 - val_loss: 0.1453 - val_auc: 0.8530\n",
      "Epoch 116/120\n",
      "333/333 [==============================] - 25s 76ms/step - loss: 0.1204 - auc: 0.8984 - val_loss: 0.1549 - val_auc: 0.8396\n",
      "Epoch 117/120\n",
      "333/333 [==============================] - 24s 71ms/step - loss: 0.1198 - auc: 0.9023 - val_loss: 0.1425 - val_auc: 0.8603\n",
      "Epoch 118/120\n",
      "333/333 [==============================] - 25s 76ms/step - loss: 0.1189 - auc: 0.9052 - val_loss: 0.1526 - val_auc: 0.8463\n",
      "Epoch 119/120\n",
      "333/333 [==============================] - 23s 69ms/step - loss: 0.1194 - auc: 0.9010 - val_loss: 0.1494 - val_auc: 0.8498\n",
      "Epoch 120/120\n",
      "333/333 [==============================] - 23s 71ms/step - loss: 0.1185 - auc: 0.9056 - val_loss: 0.1464 - val_auc: 0.8540\n"
     ]
    }
   ],
   "source": [
    "# Define a function to build the model with RGAT aggregation mechanism\n",
    "def build_model_rgat():\n",
    "    # Model definition\n",
    "    data = Input(batch_shape=(None,))\n",
    "    edge = Input(batch_shape=(None, 2), dtype=tf.int32)\n",
    "    node2graph = Input(batch_shape=(None,), dtype=tf.int32)\n",
    "    embeded = Embedding(tokenizer.num_words, 20)(data)\n",
    "\n",
    "    num_graph = tf.reduce_max(node2graph) + 1\n",
    "\n",
    "    gnn_input = GNNInput(\n",
    "        node_features=embeded,\n",
    "        adjacency_lists=(edge,),\n",
    "        node_to_graph_map=node2graph,\n",
    "        num_graphs=num_graph,\n",
    "    )\n",
    "\n",
    "    params = GNN.get_default_hyperparameters()\n",
    "    params[\"hidden_dim\"] = 64\n",
    "    params[\"num_layers\"] = 3\n",
    "    params[\"message_calculation_class\"] = \"RGAT\"\n",
    "    params[\"num_heads\"] = 4  # Add num_heads parameter\n",
    "\n",
    "    gnn_layer = GNN(params)\n",
    "    gnn_out = gnn_layer(gnn_input)\n",
    "\n",
    "    avg = segment_mean(\n",
    "        data=gnn_out,\n",
    "        segment_ids=node2graph\n",
    "    )\n",
    "\n",
    "    pred = Dense(1, activation='sigmoid')(avg)\n",
    "\n",
    "    model = Model(\n",
    "        inputs={\n",
    "            'data': data,\n",
    "            'edges': edge,\n",
    "            'node2graph': node2graph,\n",
    "        },\n",
    "        outputs=pred\n",
    "    )\n",
    "\n",
    "    model.compile(\n",
    "        loss='binary_crossentropy',\n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        metrics=['AUC']\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "# Train the model for Trial 9\n",
    "model_rgat = build_model_rgat()\n",
    "history_rgat = model_rgat.fit(\n",
    "    gen_batch(training_set, tokenizer, batch_size=batch_size, repeat=True),\n",
    "    steps_per_epoch=num_batches,\n",
    "    epochs=120,\n",
    "    validation_data=gen_batch(validation_set, tokenizer, batch_size=64, repeat=True),\n",
    "    validation_steps=num_batches_validation\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC Score: 0.8405325412750244\n"
     ]
    }
   ],
   "source": [
    "auc_rgat = model_rgat.evaluate(\n",
    "    gen_batch(validation_set,tokenizer,batch_size=64, repeat=True),\n",
    "    steps=num_batches_validation,\n",
    "    verbose=0\n",
    ")[1]\n",
    "print(\"AUC Score:\", auc_rgat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "193/193 [==============================] - 4s 18ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred9 =model_rgat.predict(\n",
    "    gen_batch(testing_set,tokenizer, batch_size=64, shuffle=False)\n",
    ")\n",
    "y_pred9 = np.reshape(y_pred9, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({'label':y_pred9})\n",
    "submission.index.name = 'id'\n",
    "submission.to_csv(' 120epochmodel_rgat9.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trial 10: RGIN with Default Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Thoughts and Observations:**\n",
    "- RGIN (Relational Graph Isomorphism Network) is a type of graph neural network that learns node representations by aggregating information from neighboring nodes. It is designed to handle graph-structured data with multiple relations between nodes. RGIN incorporates the notion of graph isomorphism to learn node embeddings that are invariant under permutations of node indices while capturing the relational information present in the graph.    \n",
    "- RGIN message passing mechanism will be used with default hyperparameters.\n",
    "- The model will consist of GNN message passing layers.\n",
    "- The default hyperparameters will be used for other configurations.\n",
    "- The model will be trained for 100 epochs to evaluate its performance\n",
    "- **Expected Outcome:** \n",
    "    - RGIN (Relational Graph Isomorphism Network) is a powerful graph neural network architecture designed to handle graph-structured data with relational information.\n",
    "    - With default hyperparameters and training for 50 epochs, we expect the model to effectively capture the relational information present in the graph data.\n",
    "    - We anticipate that the model will achieve competitive performance in terms of AUC score compared to previous trials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rgin_out KerasTensor(type_spec=TensorSpec(shape=(None, 64), dtype=tf.float32, name=None), name='gnn_102/StatefulPartitionedCall:0', description=\"created by layer 'gnn_102'\")\n",
      "pred: KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='dense_91/Sigmoid:0', description=\"created by layer 'dense_91'\")\n",
      "Model: \"model_90\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_346 (InputLayer)      [(None,)]                    0         []                            \n",
      "                                                                                                  \n",
      " input_344 (InputLayer)      [(None,)]                    0         []                            \n",
      "                                                                                                  \n",
      " tf.math.reduce_max_113 (TF  ()                           0         ['input_346[0][0]']           \n",
      " OpLambda)                                                                                        \n",
      "                                                                                                  \n",
      " embedding_115 (Embedding)   (None, 20)                   10000     ['input_344[0][0]']           \n",
      "                                                                                                  \n",
      " input_345 (InputLayer)      [(None, 2)]                  0         []                            \n",
      "                                                                                                  \n",
      " tf.__operators__.add_113 (  ()                           0         ['tf.math.reduce_max_113[0][0]\n",
      " TFOpLambda)                                                        ']                            \n",
      "                                                                                                  \n",
      " gnn_102 (GNN)               (None, 64)                   1136512   ['embedding_115[0][0]',       \n",
      "                                                                     'input_345[0][0]',           \n",
      "                                                                     'input_346[0][0]',           \n",
      "                                                                     'tf.__operators__.add_113[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " tf.math.segment_mean_91 (T  (None, 64)                   0         ['gnn_102[0][0]',             \n",
      " FOpLambda)                                                          'input_346[0][0]']           \n",
      "                                                                                                  \n",
      " dense_91 (Dense)            (None, 1)                    65        ['tf.math.segment_mean_91[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1146577 (4.37 MB)\n",
      "Trainable params: 1146577 (4.37 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Define the input for the node features (a batch of sequences of tokens)\n",
    "data = Input(batch_shape=(None,))\n",
    "\n",
    "# the first dim is different to the previous one. it is the total number of edges in this batch\n",
    "edge_index = Input(batch_shape=(None, 2),  dtype=tf.int32)\n",
    "node2graph = Input(batch_shape=(None,), dtype=tf.int32)\n",
    "embeded = Embedding(tokenizer.num_words, 20)(data)\n",
    "\n",
    "# number of graphs (number of samples)\n",
    "num_graph = tf.reduce_max(node2graph)+1\n",
    "\n",
    "# Define the RGIN layer\n",
    "rgin_input = GNNInput( \n",
    "    node_features=embeded,\n",
    "    adjacency_lists=(edge_index,),\n",
    "    node_to_graph_map=node2graph,\n",
    "    num_graphs=num_graph, \n",
    ")\n",
    "\n",
    "#Get the default hyperparameters for the GNN model and set the hidden dimension to 64\n",
    "params10 = GNN.get_default_hyperparameters()\n",
    "params10[\"hidden_dim\"] = 64\n",
    "#Set the number of hidden layers for the aggregation MLP to 64\n",
    "params10[\"num_aggr_MLP_hidden_layers\"] = 64\n",
    "#Set the message calculation class to RGIN (Relational Graph Isomorphism Network)\n",
    "params10['message_calculation_class']= 'RGIN'\n",
    "params10[\"num_heads\"] = 4\n",
    "\n",
    "#Create a GNN layer with the given parameters\n",
    "rgin_layer = GNN(params10)\n",
    "\n",
    "#Apply the GNN layer to the input and get the output node representations\n",
    "rgin_out = rgin_layer(rgin_input)\n",
    "\n",
    "#Print the shape and values of rgin_out\n",
    "print('rgin_out', rgin_out)\n",
    "\n",
    "#Compute the mean of the node representations for each graph using segment_mean\n",
    "avg = segment_mean(\n",
    "    data=rgin_out,\n",
    "    segment_ids=node2graph\n",
    ")\n",
    "\n",
    "\n",
    "#Apply a dense layer with sigmoid activation to get the prediction for each graph\n",
    "pred10= Dense(1, activation='sigmoid')(avg)\n",
    "print('pred:', pred10)\n",
    "\n",
    "#Define the model with inputs and outputs\n",
    "model10 = Model(\n",
    "    inputs={\n",
    "        'data': data,\n",
    "        'edges': edge_index,\n",
    "        'node2grah': node2graph,\n",
    "    },\n",
    "    outputs=pred10\n",
    ")\n",
    "model10.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "metadata": {},
   "outputs": [],
   "source": [
    "model10.compile(\n",
    "    loss='BinaryCrossentropy',\n",
    "    metrics=['AUC']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 601,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "333/333 [==============================] - 142s 426ms/step - loss: 0.2603 - auc: 0.4923 - val_loss: 0.2508 - val_auc: 0.5000\n",
      "Epoch 2/5\n",
      "333/333 [==============================] - 157s 471ms/step - loss: 0.2291 - auc: 0.5000 - val_loss: 0.2277 - val_auc: 0.5000\n",
      "Epoch 3/5\n",
      "333/333 [==============================] - 148s 445ms/step - loss: 0.2101 - auc: 0.5069 - val_loss: 0.2168 - val_auc: 0.5000\n",
      "Epoch 4/5\n",
      "333/333 [==============================] - 152s 457ms/step - loss: 0.2000 - auc: 0.4977 - val_loss: 0.2003 - val_auc: 0.5000\n",
      "Epoch 5/5\n",
      "333/333 [==============================] - 148s 446ms/step - loss: 0.1956 - auc: 0.4955 - val_loss: 0.2047 - val_auc: 0.5000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x17e45c97f10>"
      ]
     },
     "execution_count": 601,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "batch_size = 64\n",
    "num_batchs = math.ceil(len(training_set) / batch_size)\n",
    "num_batchs_validation = math.ceil(len(validation_set) / batch_size)\n",
    "\n",
    "model10.fit(\n",
    "    gen_batch(\n",
    "        training_set,batch_size=batch_size, repeat=True\n",
    "    ),\n",
    "    steps_per_epoch=num_batchs,\n",
    "    epochs=5,  # Increased epochs for tuning\n",
    "    validation_data=gen_batch(\n",
    "        validation_set,batch_size=64, repeat=True\n",
    "    ),\n",
    "    validation_steps=num_batchs_validation,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upsampling\n",
    "- is a technique used to address class imbalance problems in machine learning datasets. When one class has significantly fewer samples than another class, the model may become biased towards the majority class, leading to poor performance in predicting the minority class.\n",
    "\n",
    "### Steps for Upsampling:\n",
    "\n",
    "1. **Identify the Minority Class**: Determine which class is the minority class that needs to be upsampled.\n",
    "\n",
    "2. **Calculate the Imbalance Ratio**: Calculate the ratio of the number of samples in the minority class to the number of samples in the majority class.\n",
    "\n",
    "3. **Upsample the Minority Class**: Generate additional samples for the minority class to match the number of samples in the majority class. This can be achieved by randomly duplicating existing samples or generating new samples based on existing ones.\n",
    "\n",
    "4. **Combine with the Majority Class**: Combine the upsampled minority class samples with the original majority class samples to create a balanced dataset.\n",
    "\n",
    "5. **Train the Model**: Train the machine learning model using the balanced dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upsample(training_set):\n",
    "    positive_samples = [sample for sample in training_set if sample[2] == 1]\n",
    "    negative_samples = [sample for sample in training_set if sample[2] == 0]\n",
    "\n",
    "    # Calculate the ratio of positive to negative samples\n",
    "    positive_ratio = len(positive_samples) / len(negative_samples)\n",
    "    \n",
    "    # Upsample positive samples to match the number of negative samples\n",
    "    upsampled_positive_samples = random.choices(positive_samples, k=len(negative_samples))\n",
    "\n",
    "    # Concatenate the upsampled positive samples with the original negative samples\n",
    "    upsampled_training_set = negative_samples + upsampled_positive_samples\n",
    "\n",
    "    return upsampled_training_set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Trial11 GGNN After (Upsampling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Upsampling Method:\n",
    "In Trial 11, the upsampling method involves randomly selecting samples from the positive (minority) class to match the number of samples in the negative (majority) class.\n",
    "\n",
    "#### Thoughts and Observations:\n",
    "- **Model Performance**: The model trained after upsampling achieves the highest AUC score among all previous trials, with an AUC Score of 0.8509.\n",
    "  \n",
    "- **Effectiveness of Upsampling**: The upsampling technique effectively addresses the class imbalance issue in the dataset, resulting in improved model performance.\n",
    "\n",
    "- **Training Duration**: The model is trained for 80 epochs to ensure convergence and evaluate its performance adequately.\n",
    "\n",
    "- **Further Improvements**: While the model's performance is currently the best among the trials, further optimization and tuning may still be possible to achieve even higher scores. Adjustments to hyperparameters and exploring other techniques may be considered in future iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/80\n",
      "333/333 [==============================] - 10s 19ms/step - loss: 0.6590 - auc: 0.6432 - val_loss: 0.6677 - val_auc: 0.6973\n",
      "Epoch 2/80\n",
      "333/333 [==============================] - 8s 24ms/step - loss: 0.6380 - auc: 0.6850 - val_loss: 0.6113 - val_auc: 0.7015\n",
      "Epoch 3/80\n",
      "333/333 [==============================] - 8s 24ms/step - loss: 0.6312 - auc: 0.6979 - val_loss: 0.5815 - val_auc: 0.7321\n",
      "Epoch 4/80\n",
      "333/333 [==============================] - 8s 23ms/step - loss: 0.6124 - auc: 0.7247 - val_loss: 0.7015 - val_auc: 0.7490\n",
      "Epoch 5/80\n",
      "333/333 [==============================] - 7s 20ms/step - loss: 0.5929 - auc: 0.7446 - val_loss: 0.6437 - val_auc: 0.7694\n",
      "Epoch 6/80\n",
      "333/333 [==============================] - 6s 20ms/step - loss: 0.5881 - auc: 0.7527 - val_loss: 0.5591 - val_auc: 0.7950\n",
      "Epoch 7/80\n",
      "333/333 [==============================] - 6s 18ms/step - loss: 0.5694 - auc: 0.7730 - val_loss: 0.5193 - val_auc: 0.7827\n",
      "Epoch 8/80\n",
      "333/333 [==============================] - 6s 19ms/step - loss: 0.5655 - auc: 0.7770 - val_loss: 0.5305 - val_auc: 0.8187\n",
      "Epoch 9/80\n",
      "333/333 [==============================] - 7s 21ms/step - loss: 0.5516 - auc: 0.7889 - val_loss: 0.4904 - val_auc: 0.7982\n",
      "Epoch 10/80\n",
      "333/333 [==============================] - 7s 20ms/step - loss: 0.5661 - auc: 0.7772 - val_loss: 0.5823 - val_auc: 0.7993\n",
      "Epoch 11/80\n",
      "333/333 [==============================] - 6s 18ms/step - loss: 0.5553 - auc: 0.7880 - val_loss: 0.6506 - val_auc: 0.8253\n",
      "Epoch 12/80\n",
      "333/333 [==============================] - 6s 18ms/step - loss: 0.5592 - auc: 0.7844 - val_loss: 0.6242 - val_auc: 0.7992\n",
      "Epoch 13/80\n",
      "333/333 [==============================] - 6s 18ms/step - loss: 0.5474 - auc: 0.7952 - val_loss: 0.5505 - val_auc: 0.8227\n",
      "Epoch 14/80\n",
      "333/333 [==============================] - 6s 18ms/step - loss: 0.5453 - auc: 0.7986 - val_loss: 0.5581 - val_auc: 0.8192\n",
      "Epoch 15/80\n",
      "333/333 [==============================] - 6s 18ms/step - loss: 0.5482 - auc: 0.7962 - val_loss: 0.4580 - val_auc: 0.8088\n",
      "Epoch 16/80\n",
      "333/333 [==============================] - 6s 17ms/step - loss: 0.5349 - auc: 0.8081 - val_loss: 0.5640 - val_auc: 0.8183\n",
      "Epoch 17/80\n",
      "333/333 [==============================] - 6s 18ms/step - loss: 0.5399 - auc: 0.8035 - val_loss: 0.6145 - val_auc: 0.8074\n",
      "Epoch 18/80\n",
      "333/333 [==============================] - 6s 18ms/step - loss: 0.5308 - auc: 0.8115 - val_loss: 0.5288 - val_auc: 0.8367\n",
      "Epoch 19/80\n",
      "333/333 [==============================] - 6s 18ms/step - loss: 0.5259 - auc: 0.8162 - val_loss: 0.5388 - val_auc: 0.8204\n",
      "Epoch 20/80\n",
      "333/333 [==============================] - 6s 17ms/step - loss: 0.5192 - auc: 0.8210 - val_loss: 0.5182 - val_auc: 0.8348\n",
      "Epoch 21/80\n",
      "333/333 [==============================] - 6s 18ms/step - loss: 0.5275 - auc: 0.8146 - val_loss: 0.5801 - val_auc: 0.8181\n",
      "Epoch 22/80\n",
      "333/333 [==============================] - 6s 18ms/step - loss: 0.5211 - auc: 0.8197 - val_loss: 0.5314 - val_auc: 0.8384\n",
      "Epoch 23/80\n",
      "333/333 [==============================] - 6s 18ms/step - loss: 0.5080 - auc: 0.8304 - val_loss: 0.5232 - val_auc: 0.8339\n",
      "Epoch 24/80\n",
      "333/333 [==============================] - 6s 18ms/step - loss: 0.5156 - auc: 0.8238 - val_loss: 0.5180 - val_auc: 0.8215\n",
      "Epoch 25/80\n",
      "333/333 [==============================] - 6s 18ms/step - loss: 0.5113 - auc: 0.8277 - val_loss: 0.5609 - val_auc: 0.8563\n",
      "Epoch 26/80\n",
      "333/333 [==============================] - 6s 18ms/step - loss: 0.5088 - auc: 0.8297 - val_loss: 0.5340 - val_auc: 0.8409\n",
      "Epoch 27/80\n",
      "333/333 [==============================] - 6s 17ms/step - loss: 0.5073 - auc: 0.8310 - val_loss: 0.5077 - val_auc: 0.8364\n",
      "Epoch 28/80\n",
      "333/333 [==============================] - 6s 18ms/step - loss: 0.4995 - auc: 0.8374 - val_loss: 0.5848 - val_auc: 0.8415\n",
      "Epoch 29/80\n",
      "333/333 [==============================] - 6s 18ms/step - loss: 0.4983 - auc: 0.8377 - val_loss: 0.5310 - val_auc: 0.8450\n",
      "Epoch 30/80\n",
      "333/333 [==============================] - 6s 18ms/step - loss: 0.4975 - auc: 0.8391 - val_loss: 0.5197 - val_auc: 0.8447\n",
      "Epoch 31/80\n",
      "333/333 [==============================] - 6s 18ms/step - loss: 0.4864 - auc: 0.8466 - val_loss: 0.5357 - val_auc: 0.8508\n",
      "Epoch 32/80\n",
      "333/333 [==============================] - 6s 18ms/step - loss: 0.4855 - auc: 0.8475 - val_loss: 0.4517 - val_auc: 0.8524\n",
      "Epoch 33/80\n",
      "333/333 [==============================] - 6s 18ms/step - loss: 0.4928 - auc: 0.8422 - val_loss: 0.4726 - val_auc: 0.8321\n",
      "Epoch 34/80\n",
      "333/333 [==============================] - 6s 17ms/step - loss: 0.4951 - auc: 0.8403 - val_loss: 0.4913 - val_auc: 0.8488\n",
      "Epoch 35/80\n",
      "333/333 [==============================] - 6s 18ms/step - loss: 0.4779 - auc: 0.8527 - val_loss: 0.5331 - val_auc: 0.8336\n",
      "Epoch 36/80\n",
      "333/333 [==============================] - 6s 17ms/step - loss: 0.4869 - auc: 0.8462 - val_loss: 0.5447 - val_auc: 0.8372\n",
      "Epoch 37/80\n",
      "333/333 [==============================] - 6s 18ms/step - loss: 0.4717 - auc: 0.8570 - val_loss: 0.4789 - val_auc: 0.8325\n",
      "Epoch 38/80\n",
      "333/333 [==============================] - 6s 18ms/step - loss: 0.4778 - auc: 0.8529 - val_loss: 0.5827 - val_auc: 0.8411\n",
      "Epoch 39/80\n",
      "333/333 [==============================] - 6s 18ms/step - loss: 0.4756 - auc: 0.8543 - val_loss: 0.4626 - val_auc: 0.8435\n",
      "Epoch 40/80\n",
      "333/333 [==============================] - 6s 18ms/step - loss: 0.4656 - auc: 0.8613 - val_loss: 0.4561 - val_auc: 0.8376\n",
      "Epoch 41/80\n",
      "333/333 [==============================] - 6s 19ms/step - loss: 0.4666 - auc: 0.8605 - val_loss: 0.4981 - val_auc: 0.8375\n",
      "Epoch 42/80\n",
      "333/333 [==============================] - 6s 18ms/step - loss: 0.4650 - auc: 0.8618 - val_loss: 0.4747 - val_auc: 0.8397\n",
      "Epoch 43/80\n",
      "333/333 [==============================] - 6s 18ms/step - loss: 0.4615 - auc: 0.8638 - val_loss: 0.5067 - val_auc: 0.8341\n",
      "Epoch 44/80\n",
      "333/333 [==============================] - 6s 18ms/step - loss: 0.4515 - auc: 0.8704 - val_loss: 0.5265 - val_auc: 0.8347\n",
      "Epoch 45/80\n",
      "333/333 [==============================] - 6s 17ms/step - loss: 0.4602 - auc: 0.8647 - val_loss: 0.4991 - val_auc: 0.8400\n",
      "Epoch 46/80\n",
      "333/333 [==============================] - 6s 18ms/step - loss: 0.4601 - auc: 0.8646 - val_loss: 0.4912 - val_auc: 0.8414\n",
      "Epoch 47/80\n",
      "333/333 [==============================] - 6s 18ms/step - loss: 0.4541 - auc: 0.8686 - val_loss: 0.4103 - val_auc: 0.8236\n",
      "Epoch 48/80\n",
      "333/333 [==============================] - 6s 18ms/step - loss: 0.4540 - auc: 0.8687 - val_loss: 0.5994 - val_auc: 0.8081\n",
      "Epoch 49/80\n",
      "333/333 [==============================] - 6s 18ms/step - loss: 0.4573 - auc: 0.8666 - val_loss: 0.5548 - val_auc: 0.8500\n",
      "Epoch 50/80\n",
      "333/333 [==============================] - 6s 19ms/step - loss: 0.4515 - auc: 0.8702 - val_loss: 0.4871 - val_auc: 0.8468\n",
      "Epoch 51/80\n",
      "333/333 [==============================] - 6s 19ms/step - loss: 0.4462 - auc: 0.8733 - val_loss: 0.5153 - val_auc: 0.8093\n",
      "Epoch 52/80\n",
      "333/333 [==============================] - 6s 18ms/step - loss: 0.4422 - auc: 0.8754 - val_loss: 0.6047 - val_auc: 0.8611\n",
      "Epoch 53/80\n",
      "333/333 [==============================] - 6s 17ms/step - loss: 0.4522 - auc: 0.8698 - val_loss: 0.5535 - val_auc: 0.8163\n",
      "Epoch 54/80\n",
      "333/333 [==============================] - 6s 17ms/step - loss: 0.4362 - auc: 0.8791 - val_loss: 0.5097 - val_auc: 0.8354\n",
      "Epoch 55/80\n",
      "333/333 [==============================] - 6s 17ms/step - loss: 0.4279 - auc: 0.8845 - val_loss: 0.4672 - val_auc: 0.8267\n",
      "Epoch 56/80\n",
      "333/333 [==============================] - 6s 17ms/step - loss: 0.4488 - auc: 0.8710 - val_loss: 0.4243 - val_auc: 0.8454\n",
      "Epoch 57/80\n",
      "333/333 [==============================] - 6s 17ms/step - loss: 0.4394 - auc: 0.8778 - val_loss: 0.4936 - val_auc: 0.8388\n",
      "Epoch 58/80\n",
      "333/333 [==============================] - 6s 17ms/step - loss: 0.4426 - auc: 0.8754 - val_loss: 0.5518 - val_auc: 0.8354\n",
      "Epoch 59/80\n",
      "333/333 [==============================] - 6s 17ms/step - loss: 0.4258 - auc: 0.8856 - val_loss: 0.5363 - val_auc: 0.8353\n",
      "Epoch 60/80\n",
      "333/333 [==============================] - 6s 19ms/step - loss: 0.4182 - auc: 0.8898 - val_loss: 0.5585 - val_auc: 0.8425\n",
      "Epoch 61/80\n",
      "333/333 [==============================] - 6s 18ms/step - loss: 0.4317 - auc: 0.8814 - val_loss: 0.4764 - val_auc: 0.8472\n",
      "Epoch 62/80\n",
      "333/333 [==============================] - 6s 17ms/step - loss: 0.4250 - auc: 0.8855 - val_loss: 0.5987 - val_auc: 0.8447\n",
      "Epoch 63/80\n",
      "333/333 [==============================] - 6s 18ms/step - loss: 0.4114 - auc: 0.8929 - val_loss: 0.4642 - val_auc: 0.8290\n",
      "Epoch 64/80\n",
      "333/333 [==============================] - 6s 17ms/step - loss: 0.4214 - auc: 0.8876 - val_loss: 0.4579 - val_auc: 0.8448\n",
      "Epoch 65/80\n",
      "333/333 [==============================] - 6s 17ms/step - loss: 0.4230 - auc: 0.8870 - val_loss: 0.4985 - val_auc: 0.8517\n",
      "Epoch 66/80\n",
      "333/333 [==============================] - 6s 17ms/step - loss: 0.4144 - auc: 0.8916 - val_loss: 0.5020 - val_auc: 0.8300\n",
      "Epoch 67/80\n",
      "333/333 [==============================] - 6s 18ms/step - loss: 0.4098 - auc: 0.8934 - val_loss: 0.5027 - val_auc: 0.8247\n",
      "Epoch 68/80\n",
      "333/333 [==============================] - 6s 17ms/step - loss: 0.4066 - auc: 0.8957 - val_loss: 0.4581 - val_auc: 0.8589\n",
      "Epoch 69/80\n",
      "333/333 [==============================] - 6s 17ms/step - loss: 0.4055 - auc: 0.8965 - val_loss: 0.4504 - val_auc: 0.8402\n",
      "Epoch 70/80\n",
      "333/333 [==============================] - 6s 18ms/step - loss: 0.4174 - auc: 0.8893 - val_loss: 0.3795 - val_auc: 0.8247\n",
      "Epoch 71/80\n",
      "333/333 [==============================] - 6s 17ms/step - loss: 0.4084 - auc: 0.8946 - val_loss: 0.4617 - val_auc: 0.8622\n",
      "Epoch 72/80\n",
      "333/333 [==============================] - 6s 18ms/step - loss: 0.4018 - auc: 0.8982 - val_loss: 0.4484 - val_auc: 0.8542\n",
      "Epoch 73/80\n",
      "333/333 [==============================] - 6s 18ms/step - loss: 0.4055 - auc: 0.8961 - val_loss: 0.5270 - val_auc: 0.8291\n",
      "Epoch 74/80\n",
      "333/333 [==============================] - 6s 17ms/step - loss: 0.3984 - auc: 0.9000 - val_loss: 0.4382 - val_auc: 0.8478\n",
      "Epoch 75/80\n",
      "333/333 [==============================] - 6s 18ms/step - loss: 0.3985 - auc: 0.8992 - val_loss: 0.4251 - val_auc: 0.8611\n",
      "Epoch 76/80\n",
      "333/333 [==============================] - 6s 17ms/step - loss: 0.3922 - auc: 0.9028 - val_loss: 0.5280 - val_auc: 0.8522\n",
      "Epoch 77/80\n",
      "333/333 [==============================] - 6s 18ms/step - loss: 0.3934 - auc: 0.9024 - val_loss: 0.4494 - val_auc: 0.8459\n",
      "Epoch 78/80\n",
      "333/333 [==============================] - 6s 17ms/step - loss: 0.3870 - auc: 0.9056 - val_loss: 0.4783 - val_auc: 0.8458\n",
      "Epoch 79/80\n",
      "333/333 [==============================] - 6s 17ms/step - loss: 0.3799 - auc: 0.9089 - val_loss: 0.5484 - val_auc: 0.8555\n",
      "Epoch 80/80\n",
      "333/333 [==============================] - 6s 17ms/step - loss: 0.3927 - auc: 0.9030 - val_loss: 0.4920 - val_auc: 0.8604\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x17e09bae710>"
      ]
     },
     "execution_count": 547,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Trial11 (Upsampling)\n",
    "upsampled_training_set = upsample(training_set)\n",
    "upsampled_ggnn_model_11= build_model_ggnn()\n",
    "upsampled_ggnn_model_11.fit(\n",
    "    gen_batch(upsampled_training_set, tokenizer, batch_size=batch_size, repeat=True),\n",
    "    steps_per_epoch=num_batches,\n",
    "    epochs=80,\n",
    "    validation_data=gen_batch(validation_set, tokenizer, batch_size=64, repeat=True),\n",
    "    validation_steps=num_batches_validation\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "193/193 [==============================] - 3s 12ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred11 =upsampled_ggnn_model_11.predict(\n",
    "    gen_batch(testing_set,tokenizer, batch_size=64, shuffle=False)\n",
    ")\n",
    "y_pred11 = np.reshape(y_pred11, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC Score: 0.85093092918396\n"
     ]
    }
   ],
   "source": [
    "# Calculate and print AUC score\n",
    "auc_score = upsampled_ggnn_model_11.evaluate(\n",
    "    gen_batch(validation_set,tokenizer,batch_size=64, repeat=True),\n",
    "    steps=num_batches_validation,\n",
    "    verbose=0\n",
    ")[1]\n",
    "print(\"AUC Score:\", auc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({'label':y_pred11})\n",
    "submission.index.name = 'id'\n",
    "submission.to_csv('upsampled_ggnn_model_11.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trial12 RGAT After (Upsampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/80\n",
      "333/333 [==============================] - 17s 38ms/step - loss: 0.6498 - auc: 0.6563 - val_loss: 0.5962 - val_auc: 0.7472\n",
      "Epoch 2/80\n",
      "333/333 [==============================] - 12s 36ms/step - loss: 0.6123 - auc: 0.7244 - val_loss: 0.6392 - val_auc: 0.7290\n",
      "Epoch 3/80\n",
      "333/333 [==============================] - 13s 39ms/step - loss: 0.6045 - auc: 0.7325 - val_loss: 0.6104 - val_auc: 0.7571\n",
      "Epoch 4/80\n",
      "333/333 [==============================] - 14s 41ms/step - loss: 0.6018 - auc: 0.7378 - val_loss: 0.5865 - val_auc: 0.7295\n",
      "Epoch 5/80\n",
      "333/333 [==============================] - 14s 42ms/step - loss: 0.6026 - auc: 0.7391 - val_loss: 0.5557 - val_auc: 0.7727\n",
      "Epoch 6/80\n",
      "333/333 [==============================] - 14s 43ms/step - loss: 0.5727 - auc: 0.7668 - val_loss: 0.7354 - val_auc: 0.7814\n",
      "Epoch 7/80\n",
      "333/333 [==============================] - 13s 39ms/step - loss: 0.5633 - auc: 0.7777 - val_loss: 0.5374 - val_auc: 0.7792\n",
      "Epoch 8/80\n",
      "333/333 [==============================] - 13s 39ms/step - loss: 0.5588 - auc: 0.7836 - val_loss: 0.5745 - val_auc: 0.8166\n",
      "Epoch 9/80\n",
      "333/333 [==============================] - 13s 39ms/step - loss: 0.5441 - auc: 0.7981 - val_loss: 0.5367 - val_auc: 0.8216\n",
      "Epoch 10/80\n",
      "333/333 [==============================] - 13s 39ms/step - loss: 0.5463 - auc: 0.7945 - val_loss: 0.6174 - val_auc: 0.7984\n",
      "Epoch 11/80\n",
      "333/333 [==============================] - 13s 39ms/step - loss: 0.5318 - auc: 0.8079 - val_loss: 0.5273 - val_auc: 0.8272\n",
      "Epoch 12/80\n",
      "333/333 [==============================] - 14s 42ms/step - loss: 0.5314 - auc: 0.8099 - val_loss: 0.6297 - val_auc: 0.7860\n",
      "Epoch 13/80\n",
      "333/333 [==============================] - 13s 38ms/step - loss: 0.5205 - auc: 0.8206 - val_loss: 0.5052 - val_auc: 0.8261\n",
      "Epoch 14/80\n",
      "333/333 [==============================] - 19s 56ms/step - loss: 0.5157 - auc: 0.8230 - val_loss: 0.5532 - val_auc: 0.8175\n",
      "Epoch 15/80\n",
      "333/333 [==============================] - 13s 40ms/step - loss: 0.5085 - auc: 0.8290 - val_loss: 0.4348 - val_auc: 0.8103\n",
      "Epoch 16/80\n",
      "333/333 [==============================] - 13s 40ms/step - loss: 0.4997 - auc: 0.8367 - val_loss: 0.5441 - val_auc: 0.8382\n",
      "Epoch 17/80\n",
      "333/333 [==============================] - 13s 40ms/step - loss: 0.4957 - auc: 0.8389 - val_loss: 0.5873 - val_auc: 0.8171\n",
      "Epoch 18/80\n",
      "333/333 [==============================] - 13s 40ms/step - loss: 0.4943 - auc: 0.8406 - val_loss: 0.5694 - val_auc: 0.8013\n",
      "Epoch 19/80\n",
      "333/333 [==============================] - 13s 40ms/step - loss: 0.4947 - auc: 0.8395 - val_loss: 0.4393 - val_auc: 0.8118\n",
      "Epoch 20/80\n",
      "333/333 [==============================] - 13s 39ms/step - loss: 0.4941 - auc: 0.8404 - val_loss: 0.7281 - val_auc: 0.8094\n",
      "Epoch 21/80\n",
      "333/333 [==============================] - 13s 40ms/step - loss: 0.4857 - auc: 0.8459 - val_loss: 0.5269 - val_auc: 0.8150\n",
      "Epoch 22/80\n",
      "333/333 [==============================] - 13s 39ms/step - loss: 0.4738 - auc: 0.8543 - val_loss: 0.5658 - val_auc: 0.7949\n",
      "Epoch 23/80\n",
      "333/333 [==============================] - 13s 39ms/step - loss: 0.4659 - auc: 0.8593 - val_loss: 0.5080 - val_auc: 0.8300\n",
      "Epoch 24/80\n",
      "333/333 [==============================] - 13s 40ms/step - loss: 0.4702 - auc: 0.8561 - val_loss: 0.4427 - val_auc: 0.8078\n",
      "Epoch 25/80\n",
      "333/333 [==============================] - 13s 39ms/step - loss: 0.4636 - auc: 0.8614 - val_loss: 0.5344 - val_auc: 0.8239\n",
      "Epoch 26/80\n",
      "333/333 [==============================] - 13s 39ms/step - loss: 0.4661 - auc: 0.8591 - val_loss: 0.6055 - val_auc: 0.8090\n",
      "Epoch 27/80\n",
      "333/333 [==============================] - 14s 41ms/step - loss: 0.4547 - auc: 0.8664 - val_loss: 0.5159 - val_auc: 0.8063\n",
      "Epoch 28/80\n",
      "333/333 [==============================] - 14s 42ms/step - loss: 0.4618 - auc: 0.8603 - val_loss: 0.5561 - val_auc: 0.8108\n",
      "Epoch 29/80\n",
      "333/333 [==============================] - 14s 41ms/step - loss: 0.4518 - auc: 0.8685 - val_loss: 0.4852 - val_auc: 0.7990\n",
      "Epoch 30/80\n",
      "333/333 [==============================] - 13s 40ms/step - loss: 0.4460 - auc: 0.8718 - val_loss: 0.4638 - val_auc: 0.8114\n",
      "Epoch 31/80\n",
      "333/333 [==============================] - 14s 41ms/step - loss: 0.4507 - auc: 0.8675 - val_loss: 0.5040 - val_auc: 0.8112\n",
      "Epoch 32/80\n",
      "333/333 [==============================] - 13s 39ms/step - loss: 0.4492 - auc: 0.8707 - val_loss: 0.4610 - val_auc: 0.8119\n",
      "Epoch 33/80\n",
      "333/333 [==============================] - 13s 39ms/step - loss: 0.4470 - auc: 0.8722 - val_loss: 0.4237 - val_auc: 0.8093\n",
      "Epoch 34/80\n",
      "333/333 [==============================] - 13s 39ms/step - loss: 0.4361 - auc: 0.8782 - val_loss: 0.4124 - val_auc: 0.8095\n",
      "Epoch 35/80\n",
      "333/333 [==============================] - 13s 39ms/step - loss: 0.4383 - auc: 0.8762 - val_loss: 0.5320 - val_auc: 0.8259\n",
      "Epoch 36/80\n",
      "333/333 [==============================] - 13s 40ms/step - loss: 0.4362 - auc: 0.8769 - val_loss: 0.3924 - val_auc: 0.8285\n",
      "Epoch 37/80\n",
      "333/333 [==============================] - 14s 41ms/step - loss: 0.4337 - auc: 0.8793 - val_loss: 0.4536 - val_auc: 0.7908\n",
      "Epoch 38/80\n",
      "333/333 [==============================] - 13s 39ms/step - loss: 0.4346 - auc: 0.8785 - val_loss: 0.4911 - val_auc: 0.8219\n",
      "Epoch 39/80\n",
      "333/333 [==============================] - 12s 37ms/step - loss: 0.4264 - auc: 0.8831 - val_loss: 0.5148 - val_auc: 0.8082\n",
      "Epoch 40/80\n",
      "333/333 [==============================] - 12s 37ms/step - loss: 0.4228 - auc: 0.8854 - val_loss: 0.5210 - val_auc: 0.8237\n",
      "Epoch 41/80\n",
      "333/333 [==============================] - 13s 39ms/step - loss: 0.4168 - auc: 0.8885 - val_loss: 0.4387 - val_auc: 0.8305\n",
      "Epoch 42/80\n",
      "333/333 [==============================] - 13s 38ms/step - loss: 0.4213 - auc: 0.8859 - val_loss: 0.5157 - val_auc: 0.8224\n",
      "Epoch 43/80\n",
      "333/333 [==============================] - 13s 39ms/step - loss: 0.4260 - auc: 0.8830 - val_loss: 0.4717 - val_auc: 0.8012\n",
      "Epoch 44/80\n",
      "333/333 [==============================] - 13s 39ms/step - loss: 0.4125 - auc: 0.8912 - val_loss: 0.4520 - val_auc: 0.8187\n",
      "Epoch 45/80\n",
      "333/333 [==============================] - 13s 39ms/step - loss: 0.4108 - auc: 0.8914 - val_loss: 0.4201 - val_auc: 0.8394\n",
      "Epoch 46/80\n",
      "333/333 [==============================] - 14s 41ms/step - loss: 0.4145 - auc: 0.8902 - val_loss: 0.5056 - val_auc: 0.8116\n",
      "Epoch 47/80\n",
      "333/333 [==============================] - 16s 49ms/step - loss: 0.4079 - auc: 0.8931 - val_loss: 0.5331 - val_auc: 0.8185\n",
      "Epoch 48/80\n",
      "333/333 [==============================] - 14s 41ms/step - loss: 0.4134 - auc: 0.8904 - val_loss: 0.5279 - val_auc: 0.8550\n",
      "Epoch 49/80\n",
      "333/333 [==============================] - 13s 40ms/step - loss: 0.3954 - auc: 0.8996 - val_loss: 0.5228 - val_auc: 0.8256\n",
      "Epoch 50/80\n",
      "333/333 [==============================] - 13s 39ms/step - loss: 0.4030 - auc: 0.8961 - val_loss: 0.5092 - val_auc: 0.8217\n",
      "Epoch 51/80\n",
      "333/333 [==============================] - 13s 39ms/step - loss: 0.3903 - auc: 0.9029 - val_loss: 0.4470 - val_auc: 0.8272\n",
      "Epoch 52/80\n",
      "333/333 [==============================] - 14s 41ms/step - loss: 0.3918 - auc: 0.9015 - val_loss: 0.4533 - val_auc: 0.8215\n",
      "Epoch 53/80\n",
      "333/333 [==============================] - 13s 38ms/step - loss: 0.3943 - auc: 0.9002 - val_loss: 0.3786 - val_auc: 0.8246\n",
      "Epoch 54/80\n",
      "333/333 [==============================] - 13s 38ms/step - loss: 0.3787 - auc: 0.9086 - val_loss: 0.5914 - val_auc: 0.8148\n",
      "Epoch 55/80\n",
      "333/333 [==============================] - 13s 39ms/step - loss: 0.3869 - auc: 0.9043 - val_loss: 0.4687 - val_auc: 0.8445\n",
      "Epoch 56/80\n",
      "333/333 [==============================] - 13s 38ms/step - loss: 0.3921 - auc: 0.9013 - val_loss: 0.5038 - val_auc: 0.8202\n",
      "Epoch 57/80\n",
      "333/333 [==============================] - 13s 38ms/step - loss: 0.3890 - auc: 0.9033 - val_loss: 0.4170 - val_auc: 0.8295\n",
      "Epoch 58/80\n",
      "333/333 [==============================] - 12s 38ms/step - loss: 0.3881 - auc: 0.9037 - val_loss: 0.5107 - val_auc: 0.8333\n",
      "Epoch 59/80\n",
      "333/333 [==============================] - 12s 37ms/step - loss: 0.3689 - auc: 0.9128 - val_loss: 0.5263 - val_auc: 0.8251\n",
      "Epoch 60/80\n",
      "333/333 [==============================] - 13s 38ms/step - loss: 0.3772 - auc: 0.9091 - val_loss: 0.5190 - val_auc: 0.8241\n",
      "Epoch 61/80\n",
      "333/333 [==============================] - 12s 37ms/step - loss: 0.3684 - auc: 0.9133 - val_loss: 0.4740 - val_auc: 0.8412\n",
      "Epoch 62/80\n",
      "333/333 [==============================] - 12s 36ms/step - loss: 0.3763 - auc: 0.9099 - val_loss: 0.5276 - val_auc: 0.8088\n",
      "Epoch 63/80\n",
      "333/333 [==============================] - 12s 36ms/step - loss: 0.3622 - auc: 0.9163 - val_loss: 0.4774 - val_auc: 0.8344\n",
      "Epoch 64/80\n",
      "333/333 [==============================] - 12s 36ms/step - loss: 0.3633 - auc: 0.9164 - val_loss: 0.3933 - val_auc: 0.8434\n",
      "Epoch 65/80\n",
      "333/333 [==============================] - 12s 36ms/step - loss: 0.3700 - auc: 0.9126 - val_loss: 0.3989 - val_auc: 0.8169\n",
      "Epoch 66/80\n",
      "333/333 [==============================] - 12s 36ms/step - loss: 0.3579 - auc: 0.9188 - val_loss: 0.4243 - val_auc: 0.8182\n",
      "Epoch 67/80\n",
      "333/333 [==============================] - 13s 39ms/step - loss: 0.3615 - auc: 0.9166 - val_loss: 0.4067 - val_auc: 0.8264\n",
      "Epoch 68/80\n",
      "333/333 [==============================] - 18s 53ms/step - loss: 0.3645 - auc: 0.9157 - val_loss: 0.4230 - val_auc: 0.8393\n",
      "Epoch 69/80\n",
      "333/333 [==============================] - 13s 39ms/step - loss: 0.3549 - auc: 0.9198 - val_loss: 0.5251 - val_auc: 0.8247\n",
      "Epoch 70/80\n",
      "333/333 [==============================] - 14s 41ms/step - loss: 0.3490 - auc: 0.9225 - val_loss: 0.4962 - val_auc: 0.8304\n",
      "Epoch 71/80\n",
      "333/333 [==============================] - 13s 39ms/step - loss: 0.3483 - auc: 0.9233 - val_loss: 0.4613 - val_auc: 0.8384\n",
      "Epoch 72/80\n",
      "333/333 [==============================] - 13s 39ms/step - loss: 0.3440 - auc: 0.9246 - val_loss: 0.4572 - val_auc: 0.8319\n",
      "Epoch 73/80\n",
      "333/333 [==============================] - 13s 39ms/step - loss: 0.3499 - auc: 0.9223 - val_loss: 0.5096 - val_auc: 0.8131\n",
      "Epoch 74/80\n",
      "333/333 [==============================] - 13s 38ms/step - loss: 0.3431 - auc: 0.9251 - val_loss: 0.4049 - val_auc: 0.8401\n",
      "Epoch 75/80\n",
      "333/333 [==============================] - 13s 39ms/step - loss: 0.3368 - auc: 0.9280 - val_loss: 0.4684 - val_auc: 0.8178\n",
      "Epoch 76/80\n",
      "333/333 [==============================] - 13s 38ms/step - loss: 0.3502 - auc: 0.9214 - val_loss: 0.4046 - val_auc: 0.8293\n",
      "Epoch 77/80\n",
      "333/333 [==============================] - 13s 40ms/step - loss: 0.3390 - auc: 0.9274 - val_loss: 0.5651 - val_auc: 0.8167\n",
      "Epoch 78/80\n",
      "333/333 [==============================] - 13s 39ms/step - loss: 0.3356 - auc: 0.9282 - val_loss: 0.3739 - val_auc: 0.8423\n",
      "Epoch 79/80\n",
      "333/333 [==============================] - 13s 40ms/step - loss: 0.3229 - auc: 0.9333 - val_loss: 0.4867 - val_auc: 0.8162\n",
      "Epoch 80/80\n",
      "333/333 [==============================] - 13s 39ms/step - loss: 0.3382 - auc: 0.9263 - val_loss: 0.4222 - val_auc: 0.8249\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x17e1183c990>"
      ]
     },
     "execution_count": 551,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "upsampled_training_set = upsample(training_set)\n",
    "upsampled_RGAT_12=  build_model_rgat()\n",
    "upsampled_RGAT_12.fit(\n",
    "    gen_batch(upsampled_training_set, tokenizer, batch_size=batch_size, repeat=True),\n",
    "    steps_per_epoch=num_batches,\n",
    "    epochs=80,\n",
    "    validation_data=gen_batch(validation_set, tokenizer, batch_size=64, repeat=True),\n",
    "    validation_steps=num_batches_validation\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "193/193 [==============================] - 4s 17ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred12 =upsampled_RGAT_12.predict(\n",
    "    gen_batch(testing_set,tokenizer, batch_size=64, shuffle=False)\n",
    ")\n",
    "y_pred12 = np.reshape(y_pred12, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC Score: 0.8186646103858948\n"
     ]
    }
   ],
   "source": [
    "# Calculate and print AUC score\n",
    "auc_score =upsampled_RGAT_12.evaluate(\n",
    "    gen_batch(validation_set,tokenizer,batch_size=64, repeat=True),\n",
    "    steps=num_batches_validation,\n",
    "    verbose=0\n",
    ")[1]\n",
    "print(\"AUC Score:\", auc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 559,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({'label':y_pred12})\n",
    "submission.index.name = 'id'\n",
    "submission.to_csv('upsampled_RGAT_12.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_ggnn_tuned():\n",
    "    # Model definition\n",
    "    data = Input(batch_shape=(None,))\n",
    "    edge = Input(batch_shape=(None, 2), dtype=tf.int32)\n",
    "    node2graph = Input(batch_shape=(None,), dtype=tf.int32)\n",
    "    embeded = Embedding(tokenizer.num_words, 20)(data)\n",
    "\n",
    "    num_graph = tf.reduce_max(node2graph) + 1\n",
    "\n",
    "    gnn_input = GNNInput(\n",
    "        node_features=embeded,\n",
    "        adjacency_lists=(edge,),\n",
    "        node_to_graph_map=node2graph,\n",
    "        num_graphs=num_graph,\n",
    "    )\n",
    "\n",
    "    params = GNN.get_default_hyperparameters()\n",
    "    params[\"message_calculation_class\"] = \"GGNN\"\n",
    "    params[\"hidden_dim\"] = 128  # Defining hidden dimension of the GNN layer\n",
    "    params[\"num_layers\"] = 8\n",
    "    params[\"dense_every_num_layers\"] = 8\n",
    "    params[\"num_heads\"] = 4\n",
    "    params[\"num_aggr_MLP_hidden_layers\"] = 1\n",
    "    params[\"film_parameter_MLP_hidden_layers\"] = 1\n",
    "\n",
    "    gnn_layer = GNN(params)\n",
    "    gnn_out = gnn_layer(gnn_input)\n",
    "\n",
    "    avg = segment_mean(\n",
    "        data=gnn_out,\n",
    "        segment_ids=node2graph\n",
    "    )\n",
    "\n",
    "    pred = Dense(1, activation='sigmoid')(avg)\n",
    "\n",
    "    model = Model(\n",
    "        inputs={\n",
    "            'data': data,\n",
    "            'edges': edge,\n",
    "            'node2graph': node2graph,\n",
    "        },\n",
    "        outputs=pred\n",
    "    )\n",
    "\n",
    "    model.compile(\n",
    "        loss='binary_crossentropy',\n",
    "        optimizer=Adam(),\n",
    "        metrics=['AUC']\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/80\n",
      "333/333 [==============================] - 115s 314ms/step - loss: 0.6581 - auc: 0.6444 - val_loss: 0.6879 - val_auc: 0.6988\n",
      "Epoch 2/80\n",
      "333/333 [==============================] - 108s 324ms/step - loss: 0.6353 - auc: 0.6929 - val_loss: 0.6296 - val_auc: 0.6999\n",
      "Epoch 3/80\n",
      "333/333 [==============================] - 113s 341ms/step - loss: 0.6179 - auc: 0.7182 - val_loss: 0.6607 - val_auc: 0.7453\n",
      "Epoch 4/80\n",
      "333/333 [==============================] - 109s 328ms/step - loss: 0.6031 - auc: 0.7374 - val_loss: 0.7528 - val_auc: 0.7157\n",
      "Epoch 5/80\n",
      "333/333 [==============================] - 109s 329ms/step - loss: 0.6062 - auc: 0.7328 - val_loss: 0.7351 - val_auc: 0.7577\n",
      "Epoch 6/80\n",
      "333/333 [==============================] - 109s 328ms/step - loss: 0.5830 - auc: 0.7584 - val_loss: 0.5793 - val_auc: 0.7109\n",
      "Epoch 7/80\n",
      "333/333 [==============================] - 112s 337ms/step - loss: 0.5977 - auc: 0.7371 - val_loss: 0.5957 - val_auc: 0.7412\n",
      "Epoch 8/80\n",
      "333/333 [==============================] - 103s 310ms/step - loss: 0.5541 - auc: 0.7876 - val_loss: 0.5858 - val_auc: 0.8085\n",
      "Epoch 9/80\n",
      "333/333 [==============================] - 110s 329ms/step - loss: 0.5527 - auc: 0.7915 - val_loss: 0.4660 - val_auc: 0.8106\n",
      "Epoch 10/80\n",
      "333/333 [==============================] - 115s 344ms/step - loss: 0.5244 - auc: 0.8163 - val_loss: 0.6393 - val_auc: 0.8346\n",
      "Epoch 11/80\n",
      "333/333 [==============================] - 113s 339ms/step - loss: 0.5368 - auc: 0.8078 - val_loss: 0.5850 - val_auc: 0.8065\n",
      "Epoch 12/80\n",
      "333/333 [==============================] - 108s 326ms/step - loss: 0.5041 - auc: 0.8334 - val_loss: 0.5524 - val_auc: 0.8441\n",
      "Epoch 13/80\n",
      "333/333 [==============================] - 107s 322ms/step - loss: 0.4981 - auc: 0.8386 - val_loss: 0.4817 - val_auc: 0.8343\n",
      "Epoch 14/80\n",
      "333/333 [==============================] - 108s 323ms/step - loss: 0.4897 - auc: 0.8447 - val_loss: 0.5049 - val_auc: 0.8284\n",
      "Epoch 15/80\n",
      "333/333 [==============================] - 108s 326ms/step - loss: 0.4580 - auc: 0.8658 - val_loss: 0.5269 - val_auc: 0.8549\n",
      "Epoch 16/80\n",
      "333/333 [==============================] - 107s 321ms/step - loss: 0.4671 - auc: 0.8593 - val_loss: 0.4395 - val_auc: 0.8311\n",
      "Epoch 17/80\n",
      "333/333 [==============================] - 106s 318ms/step - loss: 0.4472 - auc: 0.8728 - val_loss: 0.4903 - val_auc: 0.8875\n",
      "Epoch 18/80\n",
      "333/333 [==============================] - 107s 321ms/step - loss: 0.4364 - auc: 0.8802 - val_loss: 0.6033 - val_auc: 0.8736\n",
      "Epoch 19/80\n",
      "333/333 [==============================] - 104s 311ms/step - loss: 0.4349 - auc: 0.8808 - val_loss: 0.4496 - val_auc: 0.8643\n",
      "Epoch 20/80\n",
      "333/333 [==============================] - 103s 308ms/step - loss: 0.4037 - auc: 0.8984 - val_loss: 0.3758 - val_auc: 0.8831\n",
      "Epoch 21/80\n",
      "333/333 [==============================] - 103s 309ms/step - loss: 0.4002 - auc: 0.9001 - val_loss: 0.3249 - val_auc: 0.8831\n",
      "Epoch 22/80\n",
      "333/333 [==============================] - 105s 317ms/step - loss: 0.3884 - auc: 0.9061 - val_loss: 0.4439 - val_auc: 0.9127\n",
      "Epoch 23/80\n",
      "333/333 [==============================] - 102s 306ms/step - loss: 0.3761 - auc: 0.9117 - val_loss: 0.3285 - val_auc: 0.8980\n",
      "Epoch 24/80\n",
      "333/333 [==============================] - 106s 319ms/step - loss: 0.3502 - auc: 0.9237 - val_loss: 0.3144 - val_auc: 0.9191\n",
      "Epoch 25/80\n",
      "333/333 [==============================] - 108s 324ms/step - loss: 0.3344 - auc: 0.9298 - val_loss: 0.4578 - val_auc: 0.9143\n",
      "Epoch 26/80\n",
      "333/333 [==============================] - 108s 326ms/step - loss: 0.3298 - auc: 0.9319 - val_loss: 0.3524 - val_auc: 0.9263\n",
      "Epoch 27/80\n",
      "333/333 [==============================] - 107s 322ms/step - loss: 0.3088 - auc: 0.9407 - val_loss: 0.2843 - val_auc: 0.9253\n",
      "Epoch 28/80\n",
      "333/333 [==============================] - 107s 322ms/step - loss: 0.3088 - auc: 0.9408 - val_loss: 0.3451 - val_auc: 0.9133\n",
      "Epoch 29/80\n",
      "333/333 [==============================] - 105s 316ms/step - loss: 0.2997 - auc: 0.9438 - val_loss: 0.2924 - val_auc: 0.9384\n",
      "Epoch 30/80\n",
      "333/333 [==============================] - 106s 319ms/step - loss: 0.2984 - auc: 0.9439 - val_loss: 0.2950 - val_auc: 0.9375\n",
      "Epoch 31/80\n",
      "333/333 [==============================] - 106s 319ms/step - loss: 0.2691 - auc: 0.9537 - val_loss: 0.2873 - val_auc: 0.9439\n",
      "Epoch 32/80\n",
      "333/333 [==============================] - 106s 319ms/step - loss: 0.2574 - auc: 0.9574 - val_loss: 0.2760 - val_auc: 0.9538\n",
      "Epoch 33/80\n",
      "333/333 [==============================] - 107s 321ms/step - loss: 0.2512 - auc: 0.9595 - val_loss: 0.2944 - val_auc: 0.9343\n",
      "Epoch 34/80\n",
      "333/333 [==============================] - 107s 321ms/step - loss: 0.2656 - auc: 0.9551 - val_loss: 0.2658 - val_auc: 0.9475\n",
      "Epoch 35/80\n",
      "333/333 [==============================] - 104s 314ms/step - loss: 0.2391 - auc: 0.9624 - val_loss: 0.3019 - val_auc: 0.9453\n",
      "Epoch 36/80\n",
      "333/333 [==============================] - 106s 318ms/step - loss: 0.2399 - auc: 0.9623 - val_loss: 0.4658 - val_auc: 0.7971\n",
      "Epoch 37/80\n",
      "333/333 [==============================] - 105s 317ms/step - loss: 0.4587 - auc: 0.8671 - val_loss: 0.4047 - val_auc: 0.8884\n",
      "Epoch 38/80\n",
      "333/333 [==============================] - 108s 325ms/step - loss: 0.3912 - auc: 0.9038 - val_loss: 0.5742 - val_auc: 0.9102\n",
      "Epoch 39/80\n",
      "333/333 [==============================] - 107s 323ms/step - loss: 0.3468 - auc: 0.9254 - val_loss: 0.3998 - val_auc: 0.9247\n",
      "Epoch 40/80\n",
      "333/333 [==============================] - 108s 323ms/step - loss: 0.3412 - auc: 0.9258 - val_loss: 0.4205 - val_auc: 0.9258\n",
      "Epoch 41/80\n",
      "333/333 [==============================] - 105s 314ms/step - loss: 0.3133 - auc: 0.9379 - val_loss: 0.2906 - val_auc: 0.9318\n",
      "Epoch 42/80\n",
      "333/333 [==============================] - 106s 318ms/step - loss: 0.3146 - auc: 0.9368 - val_loss: 0.2663 - val_auc: 0.9516\n",
      "Epoch 43/80\n",
      "333/333 [==============================] - 105s 315ms/step - loss: 0.2732 - auc: 0.9530 - val_loss: 0.3167 - val_auc: 0.9447\n",
      "Epoch 44/80\n",
      "333/333 [==============================] - 104s 312ms/step - loss: 0.2941 - auc: 0.9452 - val_loss: 0.3650 - val_auc: 0.9520\n",
      "Epoch 45/80\n",
      "333/333 [==============================] - 102s 307ms/step - loss: 0.2794 - auc: 0.9501 - val_loss: 0.3309 - val_auc: 0.9451\n",
      "Epoch 46/80\n",
      "333/333 [==============================] - 106s 319ms/step - loss: 0.2584 - auc: 0.9562 - val_loss: 0.3775 - val_auc: 0.9579\n",
      "Epoch 47/80\n",
      "333/333 [==============================] - 102s 308ms/step - loss: 0.2498 - auc: 0.9599 - val_loss: 0.2746 - val_auc: 0.9472\n",
      "Epoch 48/80\n",
      "333/333 [==============================] - 105s 314ms/step - loss: 0.3002 - auc: 0.9440 - val_loss: 0.3678 - val_auc: 0.9327\n",
      "Epoch 49/80\n",
      "333/333 [==============================] - 109s 326ms/step - loss: 0.2975 - auc: 0.9440 - val_loss: 0.4172 - val_auc: 0.9315\n",
      "Epoch 50/80\n",
      "333/333 [==============================] - 107s 320ms/step - loss: 0.2683 - auc: 0.9545 - val_loss: 0.2767 - val_auc: 0.9391\n",
      "Epoch 51/80\n",
      "333/333 [==============================] - 107s 322ms/step - loss: 0.2424 - auc: 0.9628 - val_loss: 0.2617 - val_auc: 0.9514\n",
      "Epoch 52/80\n",
      "333/333 [==============================] - 106s 318ms/step - loss: 0.2515 - auc: 0.9585 - val_loss: 0.3317 - val_auc: 0.9544\n",
      "Epoch 53/80\n",
      "333/333 [==============================] - 109s 326ms/step - loss: 0.2567 - auc: 0.9581 - val_loss: 0.2714 - val_auc: 0.9537\n",
      "Epoch 54/80\n",
      "333/333 [==============================] - 107s 320ms/step - loss: 0.2216 - auc: 0.9684 - val_loss: 0.2925 - val_auc: 0.9545\n",
      "Epoch 55/80\n",
      "333/333 [==============================] - 103s 310ms/step - loss: 0.2126 - auc: 0.9699 - val_loss: 0.2721 - val_auc: 0.9530\n",
      "Epoch 56/80\n",
      "333/333 [==============================] - 104s 311ms/step - loss: 0.2145 - auc: 0.9691 - val_loss: 0.2429 - val_auc: 0.9465\n",
      "Epoch 57/80\n",
      "333/333 [==============================] - 102s 306ms/step - loss: 0.2064 - auc: 0.9710 - val_loss: 0.2461 - val_auc: 0.9620\n",
      "Epoch 58/80\n",
      "333/333 [==============================] - 102s 305ms/step - loss: 0.1773 - auc: 0.9782 - val_loss: 0.2211 - val_auc: 0.9595\n",
      "Epoch 59/80\n",
      "333/333 [==============================] - 105s 314ms/step - loss: 0.1882 - auc: 0.9761 - val_loss: 0.2202 - val_auc: 0.9595\n",
      "Epoch 60/80\n",
      "333/333 [==============================] - 110s 331ms/step - loss: 0.1800 - auc: 0.9776 - val_loss: 0.2135 - val_auc: 0.9672\n",
      "Epoch 61/80\n",
      "333/333 [==============================] - 106s 319ms/step - loss: 0.1977 - auc: 0.9736 - val_loss: 0.2217 - val_auc: 0.9430\n",
      "Epoch 62/80\n",
      "333/333 [==============================] - 106s 318ms/step - loss: 0.2317 - auc: 0.9650 - val_loss: 0.2266 - val_auc: 0.9558\n",
      "Epoch 63/80\n",
      "333/333 [==============================] - 106s 319ms/step - loss: 0.2186 - auc: 0.9686 - val_loss: 0.2851 - val_auc: 0.9581\n",
      "Epoch 64/80\n",
      "333/333 [==============================] - 110s 331ms/step - loss: 0.1898 - auc: 0.9758 - val_loss: 0.2127 - val_auc: 0.9637\n",
      "Epoch 65/80\n",
      "333/333 [==============================] - 113s 339ms/step - loss: 0.1859 - auc: 0.9768 - val_loss: 0.3142 - val_auc: 0.9638\n",
      "Epoch 66/80\n",
      "333/333 [==============================] - 110s 330ms/step - loss: 0.1792 - auc: 0.9778 - val_loss: 0.2226 - val_auc: 0.9622\n",
      "Epoch 67/80\n",
      "333/333 [==============================] - 111s 334ms/step - loss: 0.1609 - auc: 0.9817 - val_loss: 0.2346 - val_auc: 0.9564\n",
      "Epoch 68/80\n",
      "333/333 [==============================] - 107s 321ms/step - loss: 0.1909 - auc: 0.9753 - val_loss: 0.2092 - val_auc: 0.9586\n",
      "Epoch 69/80\n",
      "333/333 [==============================] - 105s 317ms/step - loss: 0.1544 - auc: 0.9823 - val_loss: 0.2488 - val_auc: 0.9788\n",
      "Epoch 70/80\n",
      "333/333 [==============================] - 104s 313ms/step - loss: 0.1466 - auc: 0.9844 - val_loss: 0.2054 - val_auc: 0.9623\n",
      "Epoch 71/80\n",
      "333/333 [==============================] - 104s 313ms/step - loss: 0.1323 - auc: 0.9872 - val_loss: 0.1910 - val_auc: 0.9477\n",
      "Epoch 72/80\n",
      "333/333 [==============================] - 104s 313ms/step - loss: 0.1517 - auc: 0.9831 - val_loss: 0.2336 - val_auc: 0.9809\n",
      "Epoch 73/80\n",
      "333/333 [==============================] - 107s 322ms/step - loss: 0.1381 - auc: 0.9858 - val_loss: 0.1837 - val_auc: 0.9640\n",
      "Epoch 74/80\n",
      "333/333 [==============================] - 106s 319ms/step - loss: 0.1429 - auc: 0.9852 - val_loss: 0.2118 - val_auc: 0.9660\n",
      "Epoch 75/80\n",
      "333/333 [==============================] - 105s 316ms/step - loss: 0.1346 - auc: 0.9863 - val_loss: 0.1648 - val_auc: 0.9636\n",
      "Epoch 76/80\n",
      "333/333 [==============================] - 106s 319ms/step - loss: 0.1342 - auc: 0.9864 - val_loss: 0.2362 - val_auc: 0.9639\n",
      "Epoch 77/80\n",
      "333/333 [==============================] - 106s 318ms/step - loss: 0.1961 - auc: 0.9748 - val_loss: 0.3870 - val_auc: 0.9252\n",
      "Epoch 78/80\n",
      "333/333 [==============================] - 108s 326ms/step - loss: 0.2597 - auc: 0.9585 - val_loss: 0.2875 - val_auc: 0.9490\n",
      "Epoch 79/80\n",
      "333/333 [==============================] - 108s 325ms/step - loss: 0.2190 - auc: 0.9688 - val_loss: 0.2795 - val_auc: 0.9457\n",
      "Epoch 80/80\n",
      "333/333 [==============================] - 105s 315ms/step - loss: 0.2018 - auc: 0.9732 - val_loss: 0.2117 - val_auc: 0.9561\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "ggnn_model_tuned = build_model_ggnn_tuned()\n",
    "history_tuned = ggnn_model_tuned.fit(\n",
    "    gen_batch(upsampled_training_set, tokenizer, batch_size=batch_size, repeat=True),\n",
    "    steps_per_epoch=num_batches,\n",
    "    epochs=80,\n",
    "    validation_data=gen_batch(validation_set, tokenizer, batch_size=64, repeat=True),\n",
    "    validation_steps=num_batches_validation\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "193/193 [==============================] - 34s 174ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred13 =ggnn_model_tuned.predict(\n",
    "    gen_batch(testing_set,tokenizer, batch_size=64, shuffle=False)\n",
    ")\n",
    "y_pred13 = np.reshape(y_pred13, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 569,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC Score: 0.9529203772544861\n"
     ]
    }
   ],
   "source": [
    "# Calculate and print AUC score\n",
    "auc_score =ggnn_model_tuned.evaluate(\n",
    "    gen_batch(validation_set,tokenizer,batch_size=64, repeat=True),\n",
    "    steps=num_batches_validation,\n",
    "    verbose=0\n",
    ")[1]\n",
    "print(\"AUC Score:\", auc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({'label':y_pred13})\n",
    "submission.index.name = 'id'\n",
    "submission.to_csv('upsampled_ggnn_tuned.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
